{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Title: FatSense - Body Fat Estimation Using Body Circumference Measurements: A Predictive AI Model**"
      ],
      "metadata": {
        "id": "yLbPhyWCY-IK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot 2023-gbgbg-11 115439.jpg](data:image/jpeg;base64,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)"
      ],
      "metadata": {
        "id": "ewJEbcwFZdKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview/Introduction\n",
        "\n",
        "In the field of health and fitness, accurate measurement of body fat is essential for assessing overall well-being and determining potential health risks. However, traditional methods such as underwater weighing can be inconvenient and costly. To address this issue, the Body Fat Prediction Dataset provides a valuable resource for developing alternative methods to estimate body fat accurately and efficiently.\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "The primary challenge in body fat estimation is finding simple and cost-effective techniques that can replace complex and expensive procedures like underwater weighing. The Body Fat Prediction Dataset offers an opportunity to explore the relationship between body circumference measurements and the percentage of body fat determined through underwater weighing. By leveraging machine learning algorithms, we can develop a predictive model that estimates body fat accurately based on easily obtainable measurements.\n",
        "\n",
        "## Objectives\n",
        "\n",
        "The main objectives of this AI project are as follows:\n",
        "\n",
        "1. Develop a machine learning model to predict the percentage of body fat using body circumference measurements as input variables.\n",
        "2. Create a user-friendly interface that allows individuals to input their own body circumference measurements and obtain an estimate of their body fat percentage.\n",
        "3. Compare the performance of the developed model with existing equations and methods for body fat estimation.\n",
        "4. Investigate the importance and contribution of each input variable in predicting body fat percentage, providing insights into the relationship between body measurements and body composition.\n",
        "\n",
        "## Dataset Description\n",
        "\n",
        "The Body Fat Prediction Dataset contains measurements and estimates of the percentage of body fat for 252 men. The variables included in the dataset are as follows:\n",
        "\n",
        "1. Density determined from underwater weighing.\n",
        "2. Percent body fat calculated using Siri's equation from 1956.\n",
        "3. Age (in years).\n",
        "4. Weight (in pounds).\n",
        "5. Height (in inches).\n",
        "6. Neck circumference (in centimeters).\n",
        "7. Chest circumference (in centimeters).\n",
        "8. Abdomen 2 circumference (in centimeters).\n",
        "9. Hip circumference (in centimeters).\n",
        "10. Thigh circumference (in centimeters).\n",
        "11. Knee circumference (in centimeters).\n",
        "12. Ankle circumference (in centimeters).\n",
        "13. Biceps (extended) circumference (in centimeters).\n",
        "14. Forearm circumference (in centimeters).\n",
        "15. Wrist circumference (in centimeters).\n",
        "\n",
        "These measurements adhere to standardized measurement standards provided in Benhke and Wilmore's work. Notably, the abdomen 2 circumference is measured laterally at the level of the iliac crests and anteriorly at the umbilicus.\n",
        "\n",
        "The dataset is derived from the research conducted by K.W. Penrose, A.G. Nelson, and A.G. Fisher from Brigham Young University, Provo, Utah, in 1985. Their work, titled \"Generalized body composition prediction equation for men using simple measurement techniques,\" provides predictive equations for lean body weight. The dataset comprises the first 143 cases from their research.\n",
        "\n",
        "**Dataset URL:** https://www.kaggle.com/datasets/fedesoriano/body-fat-prediction-dataset"
      ],
      "metadata": {
        "id": "GGCkpN4Q6Tj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Technologies Used\n",
        "\n",
        "The AI project for Body Fat Prediction Dataset utilizes the following technologies:\n",
        "\n",
        "1. Python: The primary programming language used for data preprocessing, model development, and evaluation.\n",
        "2. Pandas: A powerful library for data manipulation and analysis, used to read the dataset and perform various data transformations.\n",
        "3. Scikit-learn: A comprehensive machine learning library providing tools for preprocessing, model building, and evaluation. The `make_column_transformer` function is used to create a column transformer for data normalization and preprocessing.\n",
        "4. TensorFlow: An open-source deep learning framework used to build and train the predictive model for body fat prediction. The `Sequential` API is employed to define a sequential model architecture.\n",
        "5. Matplotlib: A plotting library used to create visualizations such as line plots and scatter plots to analyze and present the model's performance.\n",
        "\n",
        "## Methodology\n",
        "\n",
        "The AI project follows the following methodology:\n",
        "\n",
        "1. Data Loading and Preprocessing: The Body Fat Prediction Dataset is loaded using Pandas, and necessary data preprocessing steps are performed, including splitting the dataset into input features (`X`) and target variable (`y`).\n",
        "\n",
        "2. Data Transformation: The input features are normalized using the `MinMaxScaler` from Scikit-learn, and the `make_column_transformer` function is used to create a column transformer for data normalization and preprocessing. The normalized features are transformed using the column transformer.\n",
        "\n",
        "3. Model Development: A sequential model architecture is defined using the TensorFlow framework. The model consists of three dense layers with 100, 10, and 1 units, respectively. The model is compiled with the mean absolute error (MAE) loss function and Adam optimizer.\n",
        "\n",
        "4. Model Training: The model is trained on the training data for a specified number of epochs. The training progress is monitored using the validation data. A model checkpoint callback is included to save the best model based on the validation loss.\n",
        "\n",
        "5. Evaluation Metrics: Custom evaluation metrics such as mean absolute scaled error (MASE) and R-squared score are implemented. These metrics are calculated based on the model's predictions and the actual target values.\n",
        "\n",
        "6. Analysis and Visualization: The predicted and actual values are formatted into DataFrames for further analysis and comparison. Line plots and scatter plots are generated to visualize the model's performance and assess the linearity between the predicted and actual values.\n",
        "\n",
        "# Results\n",
        "\n",
        "The AI project's results for the Body Fat Prediction Dataset are as follows:\n",
        "\n",
        "- Mean Absolute Error (MAE): 0.101748876\n",
        "- Mean Squared Error (MSE): 0.022398995\n",
        "- Root Mean Squared Error (RMSE): 0.14966294\n",
        "- Mean Absolute Percentage Error (MAPE): 0.8678263%\n",
        "- Mean Absolute Scaled Error (MASE): 0.013991868\n",
        "- R-squared Score (R2): 0.9995185\n",
        "\n",
        "These metrics provide an indication of the model's accuracy and performance in predicting body fat percentage based on the given dataset.\n",
        "\n",
        "## Summary\n",
        "\n",
        "The developed AI model for body fat prediction based on the Body Fat Prediction Dataset demonstrates outstanding performance. It accurately estimates body fat percentages using easily obtainable measurements like body circumference. The model achieves low errors in its predictions, as indicated by the small mean absolute error (MAE), mean squared error (MSE), and root mean squared error (RMSE) values. The high R-squared score (R2) further confirms its strong correlation with the target variable.\n",
        "\n",
        "By providing a more convenient and cost-effective alternative to traditional methods such as underwater weighing, this AI project addresses significant issues in body fat estimation. The model's accurate predictions and reliable performance make it a valuable tool for assessing and monitoring body composition. It has the potential to benefit individuals, fitness professionals, and healthcare practitioners by providing them with an efficient means of estimating body fat percentage.\n"
      ],
      "metadata": {
        "id": "vw-BM6ke68Ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "aI67ZSdpHU2W",
        "outputId": "38c026a6-7422-4214-f3bf-aaaaef78db62",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:07:03.830712Z",
          "iopub.execute_input": "2023-07-10T10:07:03.831154Z",
          "iopub.status.idle": "2023-07-10T10:07:03.844611Z",
          "shell.execute_reply.started": "2023-07-10T10:07:03.831111Z",
          "shell.execute_reply": "2023-07-10T10:07:03.843381Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/body-fat-prediction-dataset/bodyfat.csv\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "LQkej7X2HU2a",
        "outputId": "f4f166aa-35f8-4a6b-9be8-05f210cd851e",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:07:03.846071Z",
          "iopub.execute_input": "2023-07-10T10:07:03.846729Z",
          "iopub.status.idle": "2023-07-10T10:07:07.103057Z",
          "shell.execute_reply.started": "2023-07-10T10:07:03.846691Z",
          "shell.execute_reply": "2023-07-10T10:07:07.101955Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The following code shows reading a CSV file, \"bodyfat.csv,\" using pandas and storing it in the variable df. This allows us to access and analyze the dataset."
      ],
      "metadata": {
        "id": "bzvBj28oJRbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FILE_DIR=\"/kaggle/input/body-fat-prediction-dataset/bodyfat.csv\""
      ],
      "metadata": {
        "id": "FuUsK7k-HU2a",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:07:07.104678Z",
          "iopub.execute_input": "2023-07-10T10:07:07.105575Z",
          "iopub.status.idle": "2023-07-10T10:07:07.110767Z",
          "shell.execute_reply.started": "2023-07-10T10:07:07.105525Z",
          "shell.execute_reply": "2023-07-10T10:07:07.109693Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the insurance dataset\n",
        "df = pd.read_csv(FILE_DIR)\n",
        "\n",
        "# Check out the insurance dataset\n",
        "df.head()"
      ],
      "metadata": {
        "id": "1LFOHwn7HU2b",
        "outputId": "2ab9101f-6029-4df2-a5be-9387760d9f9f",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:07:07.114411Z",
          "iopub.execute_input": "2023-07-10T10:07:07.114947Z",
          "iopub.status.idle": "2023-07-10T10:07:07.150884Z",
          "shell.execute_reply.started": "2023-07-10T10:07:07.114916Z",
          "shell.execute_reply": "2023-07-10T10:07:07.149934Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 4,
          "output_type": "execute_result",
          "data": {
            "text/plain": "   Density  BodyFat  Age  Weight  Height  Neck  Chest  Abdomen    Hip  Thigh  \\\n0   1.0708     12.3   23  154.25   67.75  36.2   93.1     85.2   94.5   59.0   \n1   1.0853      6.1   22  173.25   72.25  38.5   93.6     83.0   98.7   58.7   \n2   1.0414     25.3   22  154.00   66.25  34.0   95.8     87.9   99.2   59.6   \n3   1.0751     10.4   26  184.75   72.25  37.4  101.8     86.4  101.2   60.1   \n4   1.0340     28.7   24  184.25   71.25  34.4   97.3    100.0  101.9   63.2   \n\n   Knee  Ankle  Biceps  Forearm  Wrist  \n0  37.3   21.9    32.0     27.4   17.1  \n1  37.3   23.4    30.5     28.9   18.2  \n2  38.9   24.0    28.8     25.2   16.6  \n3  37.3   22.8    32.4     29.4   18.2  \n4  42.2   24.0    32.2     27.7   17.7  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Density</th>\n      <th>BodyFat</th>\n      <th>Age</th>\n      <th>Weight</th>\n      <th>Height</th>\n      <th>Neck</th>\n      <th>Chest</th>\n      <th>Abdomen</th>\n      <th>Hip</th>\n      <th>Thigh</th>\n      <th>Knee</th>\n      <th>Ankle</th>\n      <th>Biceps</th>\n      <th>Forearm</th>\n      <th>Wrist</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0708</td>\n      <td>12.3</td>\n      <td>23</td>\n      <td>154.25</td>\n      <td>67.75</td>\n      <td>36.2</td>\n      <td>93.1</td>\n      <td>85.2</td>\n      <td>94.5</td>\n      <td>59.0</td>\n      <td>37.3</td>\n      <td>21.9</td>\n      <td>32.0</td>\n      <td>27.4</td>\n      <td>17.1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0853</td>\n      <td>6.1</td>\n      <td>22</td>\n      <td>173.25</td>\n      <td>72.25</td>\n      <td>38.5</td>\n      <td>93.6</td>\n      <td>83.0</td>\n      <td>98.7</td>\n      <td>58.7</td>\n      <td>37.3</td>\n      <td>23.4</td>\n      <td>30.5</td>\n      <td>28.9</td>\n      <td>18.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0414</td>\n      <td>25.3</td>\n      <td>22</td>\n      <td>154.00</td>\n      <td>66.25</td>\n      <td>34.0</td>\n      <td>95.8</td>\n      <td>87.9</td>\n      <td>99.2</td>\n      <td>59.6</td>\n      <td>38.9</td>\n      <td>24.0</td>\n      <td>28.8</td>\n      <td>25.2</td>\n      <td>16.6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0751</td>\n      <td>10.4</td>\n      <td>26</td>\n      <td>184.75</td>\n      <td>72.25</td>\n      <td>37.4</td>\n      <td>101.8</td>\n      <td>86.4</td>\n      <td>101.2</td>\n      <td>60.1</td>\n      <td>37.3</td>\n      <td>22.8</td>\n      <td>32.4</td>\n      <td>29.4</td>\n      <td>18.2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0340</td>\n      <td>28.7</td>\n      <td>24</td>\n      <td>184.25</td>\n      <td>71.25</td>\n      <td>34.4</td>\n      <td>97.3</td>\n      <td>100.0</td>\n      <td>101.9</td>\n      <td>63.2</td>\n      <td>42.2</td>\n      <td>24.0</td>\n      <td>32.2</td>\n      <td>27.7</td>\n      <td>17.7</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "ptLDOvJUHU2c",
        "outputId": "5cb57e90-f3ac-41a0-b20e-d116216c03ee",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:07:07.151899Z",
          "iopub.execute_input": "2023-07-10T10:07:07.152161Z",
          "iopub.status.idle": "2023-07-10T10:07:07.157493Z",
          "shell.execute_reply.started": "2023-07-10T10:07:07.152138Z",
          "shell.execute_reply": "2023-07-10T10:07:07.156517Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(252, 15)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "HIIvWMSUHU2c",
        "outputId": "deff40f4-c768-4e9a-fc42-b9384b07d2c2",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:07:07.158610Z",
          "iopub.execute_input": "2023-07-10T10:07:07.158890Z",
          "iopub.status.idle": "2023-07-10T10:07:07.180371Z",
          "shell.execute_reply.started": "2023-07-10T10:07:07.158866Z",
          "shell.execute_reply": "2023-07-10T10:07:07.179519Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 252 entries, 0 to 251\nData columns (total 15 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Density  252 non-null    float64\n 1   BodyFat  252 non-null    float64\n 2   Age      252 non-null    int64  \n 3   Weight   252 non-null    float64\n 4   Height   252 non-null    float64\n 5   Neck     252 non-null    float64\n 6   Chest    252 non-null    float64\n 7   Abdomen  252 non-null    float64\n 8   Hip      252 non-null    float64\n 9   Thigh    252 non-null    float64\n 10  Knee     252 non-null    float64\n 11  Ankle    252 non-null    float64\n 12  Biceps   252 non-null    float64\n 13  Forearm  252 non-null    float64\n 14  Wrist    252 non-null    float64\ndtypes: float64(14), int64(1)\nmemory usage: 29.7 KB\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the target"
      ],
      "metadata": {
        "id": "CeYXawSJJZ3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target=\"BodyFat\""
      ],
      "metadata": {
        "id": "jutZDWzpHU2d",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:07:07.181308Z",
          "iopub.execute_input": "2023-07-10T10:07:07.181633Z",
          "iopub.status.idle": "2023-07-10T10:07:07.195943Z",
          "shell.execute_reply.started": "2023-07-10T10:07:07.181605Z",
          "shell.execute_reply": "2023-07-10T10:07:07.194593Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make_Column_Transformer**"
      ],
      "metadata": {
        "id": "E3VwjWhjHU2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next code block extracts the numerical features from the dataset and stores them in the numerical_features list. The target variable is excluded from the list. This code is done to separate the numerical features for further processing."
      ],
      "metadata": {
        "id": "XQZPgX9IJWck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features = df._get_numeric_data().columns.to_list()\n",
        "numerical_features.remove(target)\n",
        "numerical_features"
      ],
      "metadata": {
        "id": "8CrHmXEPHU2k",
        "outputId": "19296014-e470-4a86-ee43-6eda6cf7fa24",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:07:07.197411Z",
          "iopub.execute_input": "2023-07-10T10:07:07.197725Z",
          "iopub.status.idle": "2023-07-10T10:07:07.216452Z",
          "shell.execute_reply.started": "2023-07-10T10:07:07.197699Z",
          "shell.execute_reply": "2023-07-10T10:07:07.215374Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['Density',\n 'Age',\n 'Weight',\n 'Height',\n 'Neck',\n 'Chest',\n 'Abdomen',\n 'Hip',\n 'Thigh',\n 'Knee',\n 'Ankle',\n 'Biceps',\n 'Forearm',\n 'Wrist']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the subsequent code, the categorical features are extracted from the dataset and stored in the categorical_features list. This is done to separate the categorical features for preprocessing."
      ],
      "metadata": {
        "id": "r-oQduAAJeud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = df.select_dtypes(include=['object']).columns.to_list()\n",
        "categorical_features"
      ],
      "metadata": {
        "id": "CzffAiSYHU2k",
        "outputId": "320ab031-73a4-4f02-bb65-d0079364d874",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:07:07.217848Z",
          "iopub.execute_input": "2023-07-10T10:07:07.219771Z",
          "iopub.status.idle": "2023-07-10T10:07:07.230353Z",
          "shell.execute_reply.started": "2023-07-10T10:07:07.219729Z",
          "shell.execute_reply": "2023-07-10T10:07:07.229271Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next code block imports necessary libraries and initializes a column transformer, ct, which will be used for data normalization and preprocessing.\n",
        "\n",
        "After that, the code creates input features X and target variable y by splitting the dataset. The training and test sets (X_train, X_test, y_train, y_test) are generated using the train_test_split function.\n",
        "\n",
        "The following code fits the column transformer ct on the training data and transforms both the training and test data using the fitted transformer. This is important to normalize and preprocess the data consistently across the training and test sets."
      ],
      "metadata": {
        "id": "xi9wv0OuJjFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create column transformer (this will help us normalize/preprocess our data)\n",
        "ct = make_column_transformer(\n",
        "    (MinMaxScaler(), numerical_features))\n",
        "\n",
        "# Create X & y\n",
        "X = df.drop(target, axis=1)\n",
        "y = df[target]\n",
        "\n",
        "# Build our train and test sets (use random state to ensure same split as before)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit column transformer on the training data only (doing so on test data would result in data leakage)\n",
        "ct.fit(X_train)\n",
        "\n",
        "# Transform training and test data with normalization (MinMaxScalar) and one hot encoding (OneHotEncoder)\n",
        "X_train_normal = ct.transform(X_train)\n",
        "X_test_normal = ct.transform(X_test)"
      ],
      "metadata": {
        "id": "36oZsuKLHU2l",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:07:07.232018Z",
          "iopub.execute_input": "2023-07-10T10:07:07.232343Z",
          "iopub.status.idle": "2023-07-10T10:07:07.573166Z",
          "shell.execute_reply.started": "2023-07-10T10:07:07.232307Z",
          "shell.execute_reply": "2023-07-10T10:07:07.572236Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-normalized and non-one-hot encoded data example\n",
        "X_train.loc[0]"
      ],
      "metadata": {
        "id": "eWwQAquYHU2l",
        "outputId": "e902c14f-7833-458f-be6b-dfe80570a549",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:07:07.574695Z",
          "iopub.execute_input": "2023-07-10T10:07:07.575017Z",
          "iopub.status.idle": "2023-07-10T10:07:07.582864Z",
          "shell.execute_reply.started": "2023-07-10T10:07:07.574989Z",
          "shell.execute_reply": "2023-07-10T10:07:07.581762Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Density      1.0708\nAge         23.0000\nWeight     154.2500\nHeight      67.7500\nNeck        36.2000\nChest       93.1000\nAbdomen     85.2000\nHip         94.5000\nThigh       59.0000\nKnee        37.3000\nAnkle       21.9000\nBiceps      32.0000\nForearm     27.4000\nWrist       17.1000\nName: 0, dtype: float64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalized and one-hot encoded example\n",
        "X_train_normal[0]"
      ],
      "metadata": {
        "id": "g0sTf0nXHU2l",
        "outputId": "16f3c27f-32af-46f9-8955-316b0f5228a6",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:07:07.584763Z",
          "iopub.execute_input": "2023-07-10T10:07:07.585189Z",
          "iopub.status.idle": "2023-07-10T10:07:07.597008Z",
          "shell.execute_reply.started": "2023-07-10T10:07:07.585141Z",
          "shell.execute_reply": "2023-07-10T10:07:07.596177Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "array([0.47497805, 0.28813559, 0.33448873, 0.85492228, 0.4609375 ,\n       0.27755102, 0.29401408, 0.26108374, 0.42647059, 0.23846154,\n       0.2260274 , 0.55244755, 0.52517986, 0.23214286])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice the normalized/one-hot encoded shape is larger because of the extra columns\n",
        "X_train_normal.shape, X_train.shape"
      ],
      "metadata": {
        "id": "wUwWT3N_HU2l",
        "outputId": "cdfc987a-c12c-43fb-890a-ed7ac508e772",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:07:07.602056Z",
          "iopub.execute_input": "2023-07-10T10:07:07.602507Z",
          "iopub.status.idle": "2023-07-10T10:07:07.612969Z",
          "shell.execute_reply.started": "2023-07-10T10:07:07.602474Z",
          "shell.execute_reply": "2023-07-10T10:07:07.611796Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "((201, 14), (201, 14))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, the code block sets up a model checkpoint, cc, which will save the best model during training based on the validation loss."
      ],
      "metadata": {
        "id": "JWyj2aBTJtDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "cc = ModelCheckpoint('best_model.h5',\n",
        "                             monitor='val_loss',\n",
        "                             mode='min',\n",
        "                             save_best_only=True,\n",
        "                             verbose=1)\n"
      ],
      "metadata": {
        "id": "nC3AL2ItHU2l",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:07:07.614359Z",
          "iopub.execute_input": "2023-07-10T10:07:07.614699Z",
          "iopub.status.idle": "2023-07-10T10:07:07.625909Z",
          "shell.execute_reply.started": "2023-07-10T10:07:07.614671Z",
          "shell.execute_reply": "2023-07-10T10:07:07.624690Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code sets a random seed for reproducibility and builds a sequential model, model_dn, with three dense layers. The model is compiled with a mean absolute error (MAE) loss function, Adam optimizer, and MAE metric.\n",
        "\n",
        "The model is then trained for 1500 epochs using the training data, and the validation data is used to monitor the model's performance. The model checkpoint callback is included to save the best model based on the validation loss."
      ],
      "metadata": {
        "id": "UtgGivREJvTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Build the model (3 layers, 100, 10, 1 units)\n",
        "model_dn = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(100),\n",
        "  tf.keras.layers.Dense(10),\n",
        "  tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_dn.compile(loss=tf.keras.losses.mae,\n",
        "                          optimizer=tf.keras.optimizers.Adam(),\n",
        "                          metrics=['mae'])\n",
        "\n",
        "# Fit the model for 1000 epochs\n",
        "model_dn.fit(X_train_normal, y_train, epochs=1500, validation_data=(X_test_normal, y_test), verbose=1, callbacks=[cc])"
      ],
      "metadata": {
        "id": "5iqwUtEMHU2m",
        "outputId": "e577c7fe-7142-4963-d346-03b4edf04d8b",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:07:07.627243Z",
          "iopub.execute_input": "2023-07-10T10:07:07.627622Z",
          "iopub.status.idle": "2023-07-10T10:08:25.151078Z",
          "shell.execute_reply.started": "2023-07-10T10:07:07.627590Z",
          "shell.execute_reply": "2023-07-10T10:08:25.149953Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/1500\n1/7 [===>..........................] - ETA: 4s - loss: 19.4761 - mae: 19.4761\nEpoch 1: val_loss improved from inf to 16.32416, saving model to best_model.h5\n7/7 [==============================] - 1s 41ms/step - loss: 18.5501 - mae: 18.5501 - val_loss: 16.3242 - val_mae: 16.3242\nEpoch 2/1500\n1/7 [===>..........................] - ETA: 0s - loss: 17.7567 - mae: 17.7567\nEpoch 2: val_loss improved from 16.32416 to 14.97658, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 17.3227 - mae: 17.3227 - val_loss: 14.9766 - val_mae: 14.9766\nEpoch 3/1500\n1/7 [===>..........................] - ETA: 0s - loss: 16.0676 - mae: 16.0676\nEpoch 3: val_loss improved from 14.97658 to 13.43205, saving model to best_model.h5\n7/7 [==============================] - 0s 12ms/step - loss: 15.9650 - mae: 15.9650 - val_loss: 13.4321 - val_mae: 13.4321\nEpoch 4/1500\n1/7 [===>..........................] - ETA: 0s - loss: 13.9045 - mae: 13.9045\nEpoch 4: val_loss improved from 13.43205 to 11.64800, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 14.4097 - mae: 14.4097 - val_loss: 11.6480 - val_mae: 11.6480\nEpoch 5/1500\n1/7 [===>..........................] - ETA: 0s - loss: 12.7942 - mae: 12.7942\nEpoch 5: val_loss improved from 11.64800 to 9.62377, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 12.6215 - mae: 12.6215 - val_loss: 9.6238 - val_mae: 9.6238\nEpoch 6/1500\n1/7 [===>..........................] - ETA: 0s - loss: 11.3136 - mae: 11.3136\nEpoch 6: val_loss improved from 9.62377 to 7.47100, saving model to best_model.h5\n7/7 [==============================] - 0s 13ms/step - loss: 10.6100 - mae: 10.6100 - val_loss: 7.4710 - val_mae: 7.4710\nEpoch 7/1500\n1/7 [===>..........................] - ETA: 0s - loss: 8.6533 - mae: 8.6533\nEpoch 7: val_loss improved from 7.47100 to 5.44934, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 8.5360 - mae: 8.5360 - val_loss: 5.4493 - val_mae: 5.4493\nEpoch 8/1500\n1/7 [===>..........................] - ETA: 0s - loss: 6.2416 - mae: 6.2416\nEpoch 8: val_loss improved from 5.44934 to 4.85775, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 6.8581 - mae: 6.8581 - val_loss: 4.8577 - val_mae: 4.8577\nEpoch 9/1500\n1/7 [===>..........................] - ETA: 0s - loss: 5.7337 - mae: 5.7337\nEpoch 9: val_loss did not improve from 4.85775\n7/7 [==============================] - 0s 7ms/step - loss: 5.7856 - mae: 5.7856 - val_loss: 5.2803 - val_mae: 5.2803\nEpoch 10/1500\n1/7 [===>..........................] - ETA: 0s - loss: 5.4572 - mae: 5.4572\nEpoch 10: val_loss did not improve from 4.85775\n7/7 [==============================] - 0s 7ms/step - loss: 5.6260 - mae: 5.6260 - val_loss: 5.7722 - val_mae: 5.7722\nEpoch 11/1500\n1/7 [===>..........................] - ETA: 0s - loss: 5.7375 - mae: 5.7375\nEpoch 11: val_loss did not improve from 4.85775\n7/7 [==============================] - 0s 7ms/step - loss: 5.7634 - mae: 5.7634 - val_loss: 5.8854 - val_mae: 5.8854\nEpoch 12/1500\n1/7 [===>..........................] - ETA: 0s - loss: 6.0543 - mae: 6.0543\nEpoch 12: val_loss did not improve from 4.85775\n7/7 [==============================] - 0s 7ms/step - loss: 5.6545 - mae: 5.6545 - val_loss: 5.4339 - val_mae: 5.4339\nEpoch 13/1500\n1/7 [===>..........................] - ETA: 0s - loss: 6.0823 - mae: 6.0823\nEpoch 13: val_loss did not improve from 4.85775\n7/7 [==============================] - 0s 7ms/step - loss: 5.5350 - mae: 5.5350 - val_loss: 5.0778 - val_mae: 5.0778\nEpoch 14/1500\n1/7 [===>..........................] - ETA: 0s - loss: 6.5132 - mae: 6.5132\nEpoch 14: val_loss did not improve from 4.85775\n7/7 [==============================] - 0s 7ms/step - loss: 5.5244 - mae: 5.5244 - val_loss: 4.9892 - val_mae: 4.9892\nEpoch 15/1500\n1/7 [===>..........................] - ETA: 0s - loss: 5.7343 - mae: 5.7343\nEpoch 15: val_loss did not improve from 4.85775\n7/7 [==============================] - 0s 7ms/step - loss: 5.4567 - mae: 5.4567 - val_loss: 5.0490 - val_mae: 5.0490\nEpoch 16/1500\n1/7 [===>..........................] - ETA: 0s - loss: 4.9051 - mae: 4.9051\nEpoch 16: val_loss did not improve from 4.85775\n7/7 [==============================] - 0s 7ms/step - loss: 5.3887 - mae: 5.3887 - val_loss: 5.1300 - val_mae: 5.1300\nEpoch 17/1500\n1/7 [===>..........................] - ETA: 0s - loss: 5.0504 - mae: 5.0504\nEpoch 17: val_loss did not improve from 4.85775\n7/7 [==============================] - 0s 10ms/step - loss: 5.3368 - mae: 5.3368 - val_loss: 5.1241 - val_mae: 5.1241\nEpoch 18/1500\n1/7 [===>..........................] - ETA: 0s - loss: 5.7637 - mae: 5.7637\nEpoch 18: val_loss did not improve from 4.85775\n7/7 [==============================] - 0s 8ms/step - loss: 5.2853 - mae: 5.2853 - val_loss: 5.0847 - val_mae: 5.0847\nEpoch 19/1500\n1/7 [===>..........................] - ETA: 0s - loss: 5.9449 - mae: 5.9449\nEpoch 19: val_loss did not improve from 4.85775\n7/7 [==============================] - 0s 7ms/step - loss: 5.2385 - mae: 5.2385 - val_loss: 5.0800 - val_mae: 5.0800\nEpoch 20/1500\n1/7 [===>..........................] - ETA: 0s - loss: 5.9860 - mae: 5.9860\nEpoch 20: val_loss did not improve from 4.85775\n7/7 [==============================] - 0s 7ms/step - loss: 5.1772 - mae: 5.1772 - val_loss: 4.9868 - val_mae: 4.9868\nEpoch 21/1500\n1/7 [===>..........................] - ETA: 0s - loss: 5.7862 - mae: 5.7862\nEpoch 21: val_loss did not improve from 4.85775\n7/7 [==============================] - 0s 6ms/step - loss: 5.1247 - mae: 5.1247 - val_loss: 4.9138 - val_mae: 4.9138\nEpoch 22/1500\n1/7 [===>..........................] - ETA: 0s - loss: 4.6800 - mae: 4.6800\nEpoch 22: val_loss did not improve from 4.85775\n7/7 [==============================] - 0s 7ms/step - loss: 5.0836 - mae: 5.0836 - val_loss: 4.8919 - val_mae: 4.8919\nEpoch 23/1500\n1/7 [===>..........................] - ETA: 0s - loss: 4.6040 - mae: 4.6040\nEpoch 23: val_loss did not improve from 4.85775\n7/7 [==============================] - 0s 6ms/step - loss: 5.0488 - mae: 5.0488 - val_loss: 4.9795 - val_mae: 4.9795\nEpoch 24/1500\n1/7 [===>..........................] - ETA: 0s - loss: 5.4863 - mae: 5.4863\nEpoch 24: val_loss did not improve from 4.85775\n7/7 [==============================] - 0s 7ms/step - loss: 4.9706 - mae: 4.9706 - val_loss: 4.9300 - val_mae: 4.9300\nEpoch 25/1500\n1/7 [===>..........................] - ETA: 0s - loss: 4.9022 - mae: 4.9022\nEpoch 25: val_loss did not improve from 4.85775\n7/7 [==============================] - 0s 7ms/step - loss: 4.9214 - mae: 4.9214 - val_loss: 4.9069 - val_mae: 4.9069\nEpoch 26/1500\n1/7 [===>..........................] - ETA: 0s - loss: 5.2358 - mae: 5.2358\nEpoch 26: val_loss improved from 4.85775 to 4.80358, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 4.8914 - mae: 4.8914 - val_loss: 4.8036 - val_mae: 4.8036\nEpoch 27/1500\n1/7 [===>..........................] - ETA: 0s - loss: 5.3174 - mae: 5.3174\nEpoch 27: val_loss did not improve from 4.80358\n7/7 [==============================] - 0s 7ms/step - loss: 4.8347 - mae: 4.8347 - val_loss: 4.9366 - val_mae: 4.9366\nEpoch 28/1500\n1/7 [===>..........................] - ETA: 0s - loss: 4.7437 - mae: 4.7437\nEpoch 28: val_loss did not improve from 4.80358\n7/7 [==============================] - 0s 6ms/step - loss: 4.7877 - mae: 4.7877 - val_loss: 4.8497 - val_mae: 4.8497\nEpoch 29/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.9289 - mae: 3.9289\nEpoch 29: val_loss improved from 4.80358 to 4.70456, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 4.7342 - mae: 4.7342 - val_loss: 4.7046 - val_mae: 4.7046\nEpoch 30/1500\n1/7 [===>..........................] - ETA: 0s - loss: 4.3965 - mae: 4.3965\nEpoch 30: val_loss did not improve from 4.70456\n7/7 [==============================] - 0s 7ms/step - loss: 4.6851 - mae: 4.6851 - val_loss: 4.7544 - val_mae: 4.7544\nEpoch 31/1500\n1/7 [===>..........................] - ETA: 0s - loss: 4.9880 - mae: 4.9880\nEpoch 31: val_loss did not improve from 4.70456\n7/7 [==============================] - 0s 7ms/step - loss: 4.6599 - mae: 4.6599 - val_loss: 4.8269 - val_mae: 4.8269\nEpoch 32/1500\n1/7 [===>..........................] - ETA: 0s - loss: 5.6984 - mae: 5.6984\nEpoch 32: val_loss improved from 4.70456 to 4.69194, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 4.6081 - mae: 4.6081 - val_loss: 4.6919 - val_mae: 4.6919\nEpoch 33/1500\n1/7 [===>..........................] - ETA: 0s - loss: 4.3913 - mae: 4.3913\nEpoch 33: val_loss improved from 4.69194 to 4.60092, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 4.5645 - mae: 4.5645 - val_loss: 4.6009 - val_mae: 4.6009\nEpoch 34/1500\n1/7 [===>..........................] - ETA: 0s - loss: 4.0057 - mae: 4.0057\nEpoch 34: val_loss improved from 4.60092 to 4.55324, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 4.5204 - mae: 4.5204 - val_loss: 4.5532 - val_mae: 4.5532\nEpoch 35/1500\n1/7 [===>..........................] - ETA: 0s - loss: 5.0172 - mae: 5.0172\nEpoch 35: val_loss did not improve from 4.55324\n7/7 [==============================] - 0s 7ms/step - loss: 4.4836 - mae: 4.4836 - val_loss: 4.5941 - val_mae: 4.5941\nEpoch 36/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.8655 - mae: 3.8655\nEpoch 36: val_loss improved from 4.55324 to 4.43095, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 4.4301 - mae: 4.4301 - val_loss: 4.4309 - val_mae: 4.4309\nEpoch 37/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.9052 - mae: 3.9052\nEpoch 37: val_loss did not improve from 4.43095\n7/7 [==============================] - 0s 7ms/step - loss: 4.4010 - mae: 4.4010 - val_loss: 4.4494 - val_mae: 4.4494\nEpoch 38/1500\n1/7 [===>..........................] - ETA: 0s - loss: 4.6323 - mae: 4.6323\nEpoch 38: val_loss did not improve from 4.43095\n7/7 [==============================] - 0s 7ms/step - loss: 4.3494 - mae: 4.3494 - val_loss: 4.5229 - val_mae: 4.5229\nEpoch 39/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.8285 - mae: 3.8285\nEpoch 39: val_loss improved from 4.43095 to 4.41874, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 4.3123 - mae: 4.3123 - val_loss: 4.4187 - val_mae: 4.4187\nEpoch 40/1500\n1/7 [===>..........................] - ETA: 0s - loss: 4.0958 - mae: 4.0958\nEpoch 40: val_loss improved from 4.41874 to 4.39905, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 4.2797 - mae: 4.2797 - val_loss: 4.3991 - val_mae: 4.3991\nEpoch 41/1500\n1/7 [===>..........................] - ETA: 0s - loss: 4.2883 - mae: 4.2883\nEpoch 41: val_loss did not improve from 4.39905\n7/7 [==============================] - 0s 7ms/step - loss: 4.2589 - mae: 4.2589 - val_loss: 4.4871 - val_mae: 4.4871\nEpoch 42/1500\n1/7 [===>..........................] - ETA: 0s - loss: 4.3440 - mae: 4.3440\nEpoch 42: val_loss improved from 4.39905 to 4.23337, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 4.2048 - mae: 4.2048 - val_loss: 4.2334 - val_mae: 4.2334\nEpoch 43/1500\n1/7 [===>..........................] - ETA: 0s - loss: 4.3720 - mae: 4.3720\nEpoch 43: val_loss did not improve from 4.23337\n7/7 [==============================] - 0s 7ms/step - loss: 4.2006 - mae: 4.2006 - val_loss: 4.2693 - val_mae: 4.2693\nEpoch 44/1500\n1/7 [===>..........................] - ETA: 0s - loss: 4.1341 - mae: 4.1341\nEpoch 44: val_loss did not improve from 4.23337\n7/7 [==============================] - 0s 7ms/step - loss: 4.1274 - mae: 4.1274 - val_loss: 4.5055 - val_mae: 4.5055\nEpoch 45/1500\n1/7 [===>..........................] - ETA: 0s - loss: 4.4979 - mae: 4.4979\nEpoch 45: val_loss did not improve from 4.23337\n7/7 [==============================] - 0s 7ms/step - loss: 4.1269 - mae: 4.1269 - val_loss: 4.2994 - val_mae: 4.2994\nEpoch 46/1500\n1/7 [===>..........................] - ETA: 0s - loss: 4.1067 - mae: 4.1067\nEpoch 46: val_loss did not improve from 4.23337\n7/7 [==============================] - 0s 7ms/step - loss: 4.0534 - mae: 4.0534 - val_loss: 4.2483 - val_mae: 4.2483\nEpoch 47/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.2636 - mae: 3.2636\nEpoch 47: val_loss did not improve from 4.23337\n7/7 [==============================] - 0s 7ms/step - loss: 4.0146 - mae: 4.0146 - val_loss: 4.3852 - val_mae: 4.3852\nEpoch 48/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.7748 - mae: 2.7748\nEpoch 48: val_loss did not improve from 4.23337\n7/7 [==============================] - 0s 7ms/step - loss: 3.9950 - mae: 3.9950 - val_loss: 4.2481 - val_mae: 4.2481\nEpoch 49/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.7995 - mae: 2.7995\nEpoch 49: val_loss improved from 4.23337 to 4.23114, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 3.9407 - mae: 3.9407 - val_loss: 4.2311 - val_mae: 4.2311\nEpoch 50/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.9716 - mae: 3.9716\nEpoch 50: val_loss improved from 4.23114 to 4.18334, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 3.9091 - mae: 3.9091 - val_loss: 4.1833 - val_mae: 4.1833\nEpoch 51/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.8172 - mae: 3.8172\nEpoch 51: val_loss did not improve from 4.18334\n7/7 [==============================] - 0s 7ms/step - loss: 3.8876 - mae: 3.8876 - val_loss: 4.2540 - val_mae: 4.2540\nEpoch 52/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.1003 - mae: 3.1003\nEpoch 52: val_loss improved from 4.18334 to 4.14625, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 3.8472 - mae: 3.8472 - val_loss: 4.1462 - val_mae: 4.1462\nEpoch 53/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.5171 - mae: 3.5171\nEpoch 53: val_loss improved from 4.14625 to 4.06040, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 3.8173 - mae: 3.8173 - val_loss: 4.0604 - val_mae: 4.0604\nEpoch 54/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.9820 - mae: 3.9820\nEpoch 54: val_loss did not improve from 4.06040\n7/7 [==============================] - 0s 7ms/step - loss: 3.7782 - mae: 3.7782 - val_loss: 4.0816 - val_mae: 4.0816\nEpoch 55/1500\n1/7 [===>..........................] - ETA: 0s - loss: 4.0171 - mae: 4.0171\nEpoch 55: val_loss did not improve from 4.06040\n7/7 [==============================] - 0s 7ms/step - loss: 3.7797 - mae: 3.7797 - val_loss: 4.1610 - val_mae: 4.1610\nEpoch 56/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.7484 - mae: 3.7484\nEpoch 56: val_loss improved from 4.06040 to 3.92924, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 3.7459 - mae: 3.7459 - val_loss: 3.9292 - val_mae: 3.9292\nEpoch 57/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.4842 - mae: 3.4842\nEpoch 57: val_loss did not improve from 3.92924\n7/7 [==============================] - 0s 7ms/step - loss: 3.7013 - mae: 3.7013 - val_loss: 4.0338 - val_mae: 4.0338\nEpoch 58/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.4313 - mae: 3.4313\nEpoch 58: val_loss did not improve from 3.92924\n7/7 [==============================] - 0s 6ms/step - loss: 3.6602 - mae: 3.6602 - val_loss: 3.9585 - val_mae: 3.9585\nEpoch 59/1500\n1/7 [===>..........................] - ETA: 0s - loss: 4.0196 - mae: 4.0196\nEpoch 59: val_loss improved from 3.92924 to 3.88523, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 3.6481 - mae: 3.6481 - val_loss: 3.8852 - val_mae: 3.8852\nEpoch 60/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.6384 - mae: 3.6384\nEpoch 60: val_loss did not improve from 3.88523\n7/7 [==============================] - 0s 7ms/step - loss: 3.6050 - mae: 3.6050 - val_loss: 3.9752 - val_mae: 3.9752\nEpoch 61/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.4686 - mae: 3.4686\nEpoch 61: val_loss improved from 3.88523 to 3.80558, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 3.5778 - mae: 3.5778 - val_loss: 3.8056 - val_mae: 3.8056\nEpoch 62/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.7921 - mae: 3.7921\nEpoch 62: val_loss improved from 3.80558 to 3.76830, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 3.5674 - mae: 3.5674 - val_loss: 3.7683 - val_mae: 3.7683\nEpoch 63/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.5639 - mae: 3.5639\nEpoch 63: val_loss did not improve from 3.76830\n7/7 [==============================] - 0s 7ms/step - loss: 3.5163 - mae: 3.5163 - val_loss: 3.7966 - val_mae: 3.7966\nEpoch 64/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.9250 - mae: 2.9250\nEpoch 64: val_loss did not improve from 3.76830\n7/7 [==============================] - 0s 7ms/step - loss: 3.4767 - mae: 3.4767 - val_loss: 3.8137 - val_mae: 3.8137\nEpoch 65/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.0136 - mae: 3.0136\nEpoch 65: val_loss did not improve from 3.76830\n7/7 [==============================] - 0s 7ms/step - loss: 3.4709 - mae: 3.4709 - val_loss: 3.8044 - val_mae: 3.8044\nEpoch 66/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.8415 - mae: 3.8415\nEpoch 66: val_loss improved from 3.76830 to 3.69777, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 3.4317 - mae: 3.4317 - val_loss: 3.6978 - val_mae: 3.6978\nEpoch 67/1500\n1/7 [===>..........................] - ETA: 0s - loss: 4.2269 - mae: 4.2269\nEpoch 67: val_loss improved from 3.69777 to 3.58518, saving model to best_model.h5\n7/7 [==============================] - 0s 14ms/step - loss: 3.4078 - mae: 3.4078 - val_loss: 3.5852 - val_mae: 3.5852\nEpoch 68/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.1252 - mae: 3.1252\nEpoch 68: val_loss did not improve from 3.58518\n7/7 [==============================] - 0s 10ms/step - loss: 3.3890 - mae: 3.3890 - val_loss: 3.7712 - val_mae: 3.7712\nEpoch 69/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.7127 - mae: 3.7127\nEpoch 69: val_loss improved from 3.58518 to 3.55664, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 3.3543 - mae: 3.3543 - val_loss: 3.5566 - val_mae: 3.5566\nEpoch 70/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.9557 - mae: 3.9557\nEpoch 70: val_loss improved from 3.55664 to 3.38703, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 3.2965 - mae: 3.2965 - val_loss: 3.3870 - val_mae: 3.3870\nEpoch 71/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.5491 - mae: 3.5491\nEpoch 71: val_loss did not improve from 3.38703\n7/7 [==============================] - 0s 7ms/step - loss: 3.2948 - mae: 3.2948 - val_loss: 3.4770 - val_mae: 3.4770\nEpoch 72/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.3842 - mae: 3.3842\nEpoch 72: val_loss did not improve from 3.38703\n7/7 [==============================] - 0s 7ms/step - loss: 3.2360 - mae: 3.2360 - val_loss: 3.4443 - val_mae: 3.4443\nEpoch 73/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.3999 - mae: 3.3999\nEpoch 73: val_loss improved from 3.38703 to 3.38624, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 3.2305 - mae: 3.2305 - val_loss: 3.3862 - val_mae: 3.3862\nEpoch 74/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.5812 - mae: 3.5812\nEpoch 74: val_loss did not improve from 3.38624\n7/7 [==============================] - 0s 7ms/step - loss: 3.1920 - mae: 3.1920 - val_loss: 3.5484 - val_mae: 3.5484\nEpoch 75/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.0773 - mae: 3.0773\nEpoch 75: val_loss improved from 3.38624 to 3.31193, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 3.1582 - mae: 3.1582 - val_loss: 3.3119 - val_mae: 3.3119\nEpoch 76/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.7474 - mae: 2.7474\nEpoch 76: val_loss improved from 3.31193 to 3.16140, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 3.1595 - mae: 3.1595 - val_loss: 3.1614 - val_mae: 3.1614\nEpoch 77/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.3629 - mae: 3.3629\nEpoch 77: val_loss did not improve from 3.16140\n7/7 [==============================] - 0s 7ms/step - loss: 3.0994 - mae: 3.0994 - val_loss: 3.4040 - val_mae: 3.4040\nEpoch 78/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.2010 - mae: 2.2010\nEpoch 78: val_loss did not improve from 3.16140\n7/7 [==============================] - 0s 7ms/step - loss: 3.0517 - mae: 3.0517 - val_loss: 3.1837 - val_mae: 3.1837\nEpoch 79/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.4924 - mae: 2.4924\nEpoch 79: val_loss improved from 3.16140 to 3.01296, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 3.0358 - mae: 3.0358 - val_loss: 3.0130 - val_mae: 3.0130\nEpoch 80/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.9112 - mae: 2.9112\nEpoch 80: val_loss did not improve from 3.01296\n7/7 [==============================] - 0s 10ms/step - loss: 3.0198 - mae: 3.0198 - val_loss: 3.1983 - val_mae: 3.1983\nEpoch 81/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.6537 - mae: 2.6537\nEpoch 81: val_loss did not improve from 3.01296\n7/7 [==============================] - 0s 7ms/step - loss: 2.9594 - mae: 2.9594 - val_loss: 3.1544 - val_mae: 3.1544\nEpoch 82/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.9589 - mae: 2.9589\nEpoch 82: val_loss did not improve from 3.01296\n7/7 [==============================] - 0s 7ms/step - loss: 2.9214 - mae: 2.9214 - val_loss: 3.0562 - val_mae: 3.0562\nEpoch 83/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.2870 - mae: 2.2870\nEpoch 83: val_loss improved from 3.01296 to 2.88412, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 2.8763 - mae: 2.8763 - val_loss: 2.8841 - val_mae: 2.8841\nEpoch 84/1500\n1/7 [===>..........................] - ETA: 0s - loss: 3.4761 - mae: 3.4761\nEpoch 84: val_loss did not improve from 2.88412\n7/7 [==============================] - 0s 10ms/step - loss: 2.8638 - mae: 2.8638 - val_loss: 2.9332 - val_mae: 2.9332\nEpoch 85/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.6963 - mae: 2.6963\nEpoch 85: val_loss did not improve from 2.88412\n7/7 [==============================] - 0s 7ms/step - loss: 2.8143 - mae: 2.8143 - val_loss: 3.0131 - val_mae: 3.0131\nEpoch 86/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.6655 - mae: 2.6655\nEpoch 86: val_loss improved from 2.88412 to 2.79969, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 2.7882 - mae: 2.7882 - val_loss: 2.7997 - val_mae: 2.7997\nEpoch 87/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.8216 - mae: 2.8216\nEpoch 87: val_loss improved from 2.79969 to 2.74130, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 2.7666 - mae: 2.7666 - val_loss: 2.7413 - val_mae: 2.7413\nEpoch 88/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.2181 - mae: 2.2181\nEpoch 88: val_loss did not improve from 2.74130\n7/7 [==============================] - 0s 7ms/step - loss: 2.7054 - mae: 2.7054 - val_loss: 2.9243 - val_mae: 2.9243\nEpoch 89/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.3949 - mae: 2.3949\nEpoch 89: val_loss improved from 2.74130 to 2.70073, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 2.7440 - mae: 2.7440 - val_loss: 2.7007 - val_mae: 2.7007\nEpoch 90/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.9064 - mae: 2.9064\nEpoch 90: val_loss did not improve from 2.70073\n7/7 [==============================] - 0s 7ms/step - loss: 2.6644 - mae: 2.6644 - val_loss: 2.8638 - val_mae: 2.8638\nEpoch 91/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.7162 - mae: 2.7162\nEpoch 91: val_loss did not improve from 2.70073\n7/7 [==============================] - 0s 7ms/step - loss: 2.6865 - mae: 2.6865 - val_loss: 2.7083 - val_mae: 2.7083\nEpoch 92/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.7540 - mae: 2.7540\nEpoch 92: val_loss improved from 2.70073 to 2.58177, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 2.6059 - mae: 2.6059 - val_loss: 2.5818 - val_mae: 2.5818\nEpoch 93/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.6122 - mae: 2.6122\nEpoch 93: val_loss did not improve from 2.58177\n7/7 [==============================] - 0s 7ms/step - loss: 2.5671 - mae: 2.5671 - val_loss: 2.6282 - val_mae: 2.6282\nEpoch 94/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.3598 - mae: 2.3598\nEpoch 94: val_loss did not improve from 2.58177\n7/7 [==============================] - 0s 6ms/step - loss: 2.5497 - mae: 2.5497 - val_loss: 2.5954 - val_mae: 2.5954\nEpoch 95/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.5979 - mae: 2.5979\nEpoch 95: val_loss improved from 2.58177 to 2.55793, saving model to best_model.h5\n7/7 [==============================] - 0s 12ms/step - loss: 2.5389 - mae: 2.5389 - val_loss: 2.5579 - val_mae: 2.5579\nEpoch 96/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.3877 - mae: 2.3877\nEpoch 96: val_loss improved from 2.55793 to 2.34966, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 2.5593 - mae: 2.5593 - val_loss: 2.3497 - val_mae: 2.3497\nEpoch 97/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.4838 - mae: 2.4838\nEpoch 97: val_loss did not improve from 2.34966\n7/7 [==============================] - 0s 7ms/step - loss: 2.4793 - mae: 2.4793 - val_loss: 2.5692 - val_mae: 2.5692\nEpoch 98/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.2057 - mae: 2.2057\nEpoch 98: val_loss did not improve from 2.34966\n7/7 [==============================] - 0s 7ms/step - loss: 2.4334 - mae: 2.4334 - val_loss: 2.3674 - val_mae: 2.3674\nEpoch 99/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.8876 - mae: 2.8876\nEpoch 99: val_loss improved from 2.34966 to 2.33323, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 2.4151 - mae: 2.4151 - val_loss: 2.3332 - val_mae: 2.3332\nEpoch 100/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.1056 - mae: 2.1056\nEpoch 100: val_loss did not improve from 2.33323\n7/7 [==============================] - 0s 7ms/step - loss: 2.3950 - mae: 2.3950 - val_loss: 2.4198 - val_mae: 2.4198\nEpoch 101/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.7558 - mae: 1.7558\nEpoch 101: val_loss improved from 2.33323 to 2.23210, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 2.3752 - mae: 2.3752 - val_loss: 2.2321 - val_mae: 2.2321\nEpoch 102/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.3457 - mae: 2.3457\nEpoch 102: val_loss did not improve from 2.23210\n7/7 [==============================] - 0s 7ms/step - loss: 2.3263 - mae: 2.3263 - val_loss: 2.3531 - val_mae: 2.3531\nEpoch 103/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.2292 - mae: 2.2292\nEpoch 103: val_loss improved from 2.23210 to 2.22482, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 2.3357 - mae: 2.3357 - val_loss: 2.2248 - val_mae: 2.2248\nEpoch 104/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.6894 - mae: 2.6894\nEpoch 104: val_loss did not improve from 2.22482\n7/7 [==============================] - 0s 7ms/step - loss: 2.2643 - mae: 2.2643 - val_loss: 2.3111 - val_mae: 2.3111\nEpoch 105/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.0323 - mae: 2.0323\nEpoch 105: val_loss improved from 2.22482 to 2.12882, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 2.2665 - mae: 2.2665 - val_loss: 2.1288 - val_mae: 2.1288\nEpoch 106/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.3409 - mae: 2.3409\nEpoch 106: val_loss improved from 2.12882 to 2.08170, saving model to best_model.h5\n7/7 [==============================] - 0s 12ms/step - loss: 2.2208 - mae: 2.2208 - val_loss: 2.0817 - val_mae: 2.0817\nEpoch 107/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.6055 - mae: 2.6055\nEpoch 107: val_loss did not improve from 2.08170\n7/7 [==============================] - 0s 7ms/step - loss: 2.2104 - mae: 2.2104 - val_loss: 2.1024 - val_mae: 2.1024\nEpoch 108/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.0120 - mae: 2.0120\nEpoch 108: val_loss did not improve from 2.08170\n7/7 [==============================] - 0s 7ms/step - loss: 2.1775 - mae: 2.1775 - val_loss: 2.0837 - val_mae: 2.0837\nEpoch 109/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.4152 - mae: 2.4152\nEpoch 109: val_loss improved from 2.08170 to 1.99781, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 2.1710 - mae: 2.1710 - val_loss: 1.9978 - val_mae: 1.9978\nEpoch 110/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.4731 - mae: 2.4731\nEpoch 110: val_loss did not improve from 1.99781\n7/7 [==============================] - 0s 7ms/step - loss: 2.1156 - mae: 2.1156 - val_loss: 2.0335 - val_mae: 2.0335\nEpoch 111/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.8869 - mae: 1.8869\nEpoch 111: val_loss improved from 1.99781 to 1.92214, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 2.1021 - mae: 2.1021 - val_loss: 1.9221 - val_mae: 1.9221\nEpoch 112/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.0050 - mae: 2.0050\nEpoch 112: val_loss improved from 1.92214 to 1.90381, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 2.0808 - mae: 2.0808 - val_loss: 1.9038 - val_mae: 1.9038\nEpoch 113/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.1271 - mae: 2.1271\nEpoch 113: val_loss did not improve from 1.90381\n7/7 [==============================] - 0s 7ms/step - loss: 2.0290 - mae: 2.0290 - val_loss: 1.9130 - val_mae: 1.9130\nEpoch 114/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.9364 - mae: 1.9364\nEpoch 114: val_loss improved from 1.90381 to 1.83972, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 2.0477 - mae: 2.0477 - val_loss: 1.8397 - val_mae: 1.8397\nEpoch 115/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.0191 - mae: 2.0191\nEpoch 115: val_loss improved from 1.83972 to 1.80734, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 2.0062 - mae: 2.0062 - val_loss: 1.8073 - val_mae: 1.8073\nEpoch 116/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.2577 - mae: 2.2577\nEpoch 116: val_loss did not improve from 1.80734\n7/7 [==============================] - 0s 7ms/step - loss: 2.0574 - mae: 2.0574 - val_loss: 1.8204 - val_mae: 1.8204\nEpoch 117/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.8016 - mae: 1.8016\nEpoch 117: val_loss improved from 1.80734 to 1.72776, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 1.9288 - mae: 1.9288 - val_loss: 1.7278 - val_mae: 1.7278\nEpoch 118/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.8949 - mae: 1.8949\nEpoch 118: val_loss did not improve from 1.72776\n7/7 [==============================] - 0s 7ms/step - loss: 1.9429 - mae: 1.9429 - val_loss: 1.8176 - val_mae: 1.8176\nEpoch 119/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.0259 - mae: 2.0259\nEpoch 119: val_loss improved from 1.72776 to 1.67606, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 1.9313 - mae: 1.9313 - val_loss: 1.6761 - val_mae: 1.6761\nEpoch 120/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.7013 - mae: 1.7013\nEpoch 120: val_loss did not improve from 1.67606\n7/7 [==============================] - 0s 7ms/step - loss: 1.8795 - mae: 1.8795 - val_loss: 1.7996 - val_mae: 1.7996\nEpoch 121/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.1145 - mae: 2.1145\nEpoch 121: val_loss improved from 1.67606 to 1.62774, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 1.8880 - mae: 1.8880 - val_loss: 1.6277 - val_mae: 1.6277\nEpoch 122/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.0580 - mae: 2.0580\nEpoch 122: val_loss did not improve from 1.62774\n7/7 [==============================] - 0s 7ms/step - loss: 1.8448 - mae: 1.8448 - val_loss: 1.7196 - val_mae: 1.7196\nEpoch 123/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.0258 - mae: 2.0258\nEpoch 123: val_loss improved from 1.62774 to 1.59464, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 1.8295 - mae: 1.8295 - val_loss: 1.5946 - val_mae: 1.5946\nEpoch 124/1500\n1/7 [===>..........................] - ETA: 0s - loss: 2.0817 - mae: 2.0817\nEpoch 124: val_loss improved from 1.59464 to 1.56713, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 1.7711 - mae: 1.7711 - val_loss: 1.5671 - val_mae: 1.5671\nEpoch 125/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.7063 - mae: 1.7063\nEpoch 125: val_loss improved from 1.56713 to 1.54516, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 1.8115 - mae: 1.8115 - val_loss: 1.5452 - val_mae: 1.5452\nEpoch 126/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.8845 - mae: 1.8845\nEpoch 126: val_loss improved from 1.54516 to 1.53294, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 1.7964 - mae: 1.7964 - val_loss: 1.5329 - val_mae: 1.5329\nEpoch 127/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.6201 - mae: 1.6201\nEpoch 127: val_loss improved from 1.53294 to 1.48941, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 1.7167 - mae: 1.7167 - val_loss: 1.4894 - val_mae: 1.4894\nEpoch 128/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.8505 - mae: 1.8505\nEpoch 128: val_loss improved from 1.48941 to 1.45280, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 1.6868 - mae: 1.6868 - val_loss: 1.4528 - val_mae: 1.4528\nEpoch 129/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.3918 - mae: 1.3918\nEpoch 129: val_loss did not improve from 1.45280\n7/7 [==============================] - 0s 7ms/step - loss: 1.6926 - mae: 1.6926 - val_loss: 1.4790 - val_mae: 1.4790\nEpoch 130/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.4116 - mae: 1.4116\nEpoch 130: val_loss improved from 1.45280 to 1.39325, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 1.7121 - mae: 1.7121 - val_loss: 1.3933 - val_mae: 1.3933\nEpoch 131/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.4526 - mae: 1.4526\nEpoch 131: val_loss improved from 1.39325 to 1.38604, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 1.6605 - mae: 1.6605 - val_loss: 1.3860 - val_mae: 1.3860\nEpoch 132/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.9375 - mae: 1.9375\nEpoch 132: val_loss improved from 1.38604 to 1.38503, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 1.7006 - mae: 1.7006 - val_loss: 1.3850 - val_mae: 1.3850\nEpoch 133/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1884 - mae: 1.1884\nEpoch 133: val_loss improved from 1.38503 to 1.33421, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 1.6284 - mae: 1.6284 - val_loss: 1.3342 - val_mae: 1.3342\nEpoch 134/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1811 - mae: 1.1811\nEpoch 134: val_loss did not improve from 1.33421\n7/7 [==============================] - 0s 7ms/step - loss: 1.6135 - mae: 1.6135 - val_loss: 1.3480 - val_mae: 1.3480\nEpoch 135/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.7277 - mae: 1.7277\nEpoch 135: val_loss improved from 1.33421 to 1.30710, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 1.6354 - mae: 1.6354 - val_loss: 1.3071 - val_mae: 1.3071\nEpoch 136/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.5968 - mae: 1.5968\nEpoch 136: val_loss did not improve from 1.30710\n7/7 [==============================] - 0s 7ms/step - loss: 1.7392 - mae: 1.7392 - val_loss: 1.4447 - val_mae: 1.4447\nEpoch 137/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.6940 - mae: 1.6940\nEpoch 137: val_loss improved from 1.30710 to 1.27623, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 1.5623 - mae: 1.5623 - val_loss: 1.2762 - val_mae: 1.2762\nEpoch 138/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.2004 - mae: 1.2004\nEpoch 138: val_loss did not improve from 1.27623\n7/7 [==============================] - 0s 7ms/step - loss: 1.5394 - mae: 1.5394 - val_loss: 1.3341 - val_mae: 1.3341\nEpoch 139/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.4222 - mae: 1.4222\nEpoch 139: val_loss improved from 1.27623 to 1.23826, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 1.5262 - mae: 1.5262 - val_loss: 1.2383 - val_mae: 1.2383\nEpoch 140/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.5310 - mae: 1.5310\nEpoch 140: val_loss did not improve from 1.23826\n7/7 [==============================] - 0s 7ms/step - loss: 1.4525 - mae: 1.4525 - val_loss: 1.2573 - val_mae: 1.2573\nEpoch 141/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1384 - mae: 1.1384\nEpoch 141: val_loss improved from 1.23826 to 1.22656, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 1.4829 - mae: 1.4829 - val_loss: 1.2266 - val_mae: 1.2266\nEpoch 142/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.5561 - mae: 1.5561\nEpoch 142: val_loss improved from 1.22656 to 1.19002, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 1.4626 - mae: 1.4626 - val_loss: 1.1900 - val_mae: 1.1900\nEpoch 143/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.2147 - mae: 1.2147\nEpoch 143: val_loss did not improve from 1.19002\n7/7 [==============================] - 0s 7ms/step - loss: 1.4317 - mae: 1.4317 - val_loss: 1.1908 - val_mae: 1.1908\nEpoch 144/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.5704 - mae: 1.5704\nEpoch 144: val_loss improved from 1.19002 to 1.15728, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 1.4072 - mae: 1.4072 - val_loss: 1.1573 - val_mae: 1.1573\nEpoch 145/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.3179 - mae: 1.3179\nEpoch 145: val_loss improved from 1.15728 to 1.11892, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 1.4054 - mae: 1.4054 - val_loss: 1.1189 - val_mae: 1.1189\nEpoch 146/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.4231 - mae: 1.4231\nEpoch 146: val_loss did not improve from 1.11892\n7/7 [==============================] - 0s 7ms/step - loss: 1.4003 - mae: 1.4003 - val_loss: 1.1791 - val_mae: 1.1791\nEpoch 147/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.2899 - mae: 1.2899\nEpoch 147: val_loss did not improve from 1.11892\n7/7 [==============================] - 0s 7ms/step - loss: 1.3945 - mae: 1.3945 - val_loss: 1.1260 - val_mae: 1.1260\nEpoch 148/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.4184 - mae: 1.4184\nEpoch 148: val_loss improved from 1.11892 to 1.08000, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 1.4112 - mae: 1.4112 - val_loss: 1.0800 - val_mae: 1.0800\nEpoch 149/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1288 - mae: 1.1288\nEpoch 149: val_loss improved from 1.08000 to 1.05254, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 1.3204 - mae: 1.3204 - val_loss: 1.0525 - val_mae: 1.0525\nEpoch 150/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.6271 - mae: 1.6271\nEpoch 150: val_loss did not improve from 1.05254\n7/7 [==============================] - 0s 7ms/step - loss: 1.3000 - mae: 1.3000 - val_loss: 1.0587 - val_mae: 1.0587\nEpoch 151/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.2901 - mae: 1.2901\nEpoch 151: val_loss improved from 1.05254 to 1.01278, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 1.2870 - mae: 1.2870 - val_loss: 1.0128 - val_mae: 1.0128\nEpoch 152/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.5278 - mae: 1.5278\nEpoch 152: val_loss improved from 1.01278 to 1.00306, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 1.2709 - mae: 1.2709 - val_loss: 1.0031 - val_mae: 1.0031\nEpoch 153/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.2987 - mae: 1.2987\nEpoch 153: val_loss improved from 1.00306 to 0.99767, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 1.2597 - mae: 1.2597 - val_loss: 0.9977 - val_mae: 0.9977\nEpoch 154/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0315 - mae: 1.0315\nEpoch 154: val_loss improved from 0.99767 to 0.97281, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 1.2422 - mae: 1.2422 - val_loss: 0.9728 - val_mae: 0.9728\nEpoch 155/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1604 - mae: 1.1604\nEpoch 155: val_loss improved from 0.97281 to 0.95290, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 1.2266 - mae: 1.2266 - val_loss: 0.9529 - val_mae: 0.9529\nEpoch 156/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7090 - mae: 0.7090\nEpoch 156: val_loss did not improve from 0.95290\n7/7 [==============================] - 0s 7ms/step - loss: 1.2167 - mae: 1.2167 - val_loss: 0.9754 - val_mae: 0.9754\nEpoch 157/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8942 - mae: 0.8942\nEpoch 157: val_loss did not improve from 0.95290\n7/7 [==============================] - 0s 7ms/step - loss: 1.2051 - mae: 1.2051 - val_loss: 0.9998 - val_mae: 0.9998\nEpoch 158/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.3281 - mae: 1.3281\nEpoch 158: val_loss did not improve from 0.95290\n7/7 [==============================] - 0s 6ms/step - loss: 1.2017 - mae: 1.2017 - val_loss: 1.0893 - val_mae: 1.0893\nEpoch 159/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.5918 - mae: 1.5918\nEpoch 159: val_loss improved from 0.95290 to 0.93239, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 1.2216 - mae: 1.2216 - val_loss: 0.9324 - val_mae: 0.9324\nEpoch 160/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8209 - mae: 0.8209\nEpoch 160: val_loss improved from 0.93239 to 0.89351, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 1.1511 - mae: 1.1511 - val_loss: 0.8935 - val_mae: 0.8935\nEpoch 161/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9152 - mae: 0.9152\nEpoch 161: val_loss did not improve from 0.89351\n7/7 [==============================] - 0s 7ms/step - loss: 1.1252 - mae: 1.1252 - val_loss: 0.9762 - val_mae: 0.9762\nEpoch 162/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.4265 - mae: 1.4265\nEpoch 162: val_loss improved from 0.89351 to 0.87647, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 1.1778 - mae: 1.1778 - val_loss: 0.8765 - val_mae: 0.8765\nEpoch 163/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8996 - mae: 0.8996\nEpoch 163: val_loss did not improve from 0.87647\n7/7 [==============================] - 0s 7ms/step - loss: 1.1453 - mae: 1.1453 - val_loss: 0.9254 - val_mae: 0.9254\nEpoch 164/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1674 - mae: 1.1674\nEpoch 164: val_loss did not improve from 0.87647\n7/7 [==============================] - 0s 7ms/step - loss: 1.1368 - mae: 1.1368 - val_loss: 1.1226 - val_mae: 1.1226\nEpoch 165/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.4788 - mae: 1.4788\nEpoch 165: val_loss improved from 0.87647 to 0.86960, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 1.1919 - mae: 1.1919 - val_loss: 0.8696 - val_mae: 0.8696\nEpoch 166/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9188 - mae: 0.9188\nEpoch 166: val_loss did not improve from 0.86960\n7/7 [==============================] - 0s 7ms/step - loss: 1.1209 - mae: 1.1209 - val_loss: 0.9539 - val_mae: 0.9539\nEpoch 167/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.7150 - mae: 1.7150\nEpoch 167: val_loss improved from 0.86960 to 0.83467, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 1.1027 - mae: 1.1027 - val_loss: 0.8347 - val_mae: 0.8347\nEpoch 168/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9949 - mae: 0.9949\nEpoch 168: val_loss did not improve from 0.83467\n7/7 [==============================] - 0s 7ms/step - loss: 1.0652 - mae: 1.0652 - val_loss: 0.9732 - val_mae: 0.9732\nEpoch 169/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.4632 - mae: 1.4632\nEpoch 169: val_loss improved from 0.83467 to 0.81167, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 1.1122 - mae: 1.1122 - val_loss: 0.8117 - val_mae: 0.8117\nEpoch 170/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8770 - mae: 0.8770\nEpoch 170: val_loss did not improve from 0.81167\n7/7 [==============================] - 0s 7ms/step - loss: 1.0377 - mae: 1.0377 - val_loss: 0.8123 - val_mae: 0.8123\nEpoch 171/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.3381 - mae: 1.3381\nEpoch 171: val_loss did not improve from 0.81167\n7/7 [==============================] - 0s 7ms/step - loss: 1.0336 - mae: 1.0336 - val_loss: 0.8135 - val_mae: 0.8135\nEpoch 172/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9740 - mae: 0.9740\nEpoch 172: val_loss improved from 0.81167 to 0.80780, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 1.0135 - mae: 1.0135 - val_loss: 0.8078 - val_mae: 0.8078\nEpoch 173/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9112 - mae: 0.9112\nEpoch 173: val_loss did not improve from 0.80780\n7/7 [==============================] - 0s 7ms/step - loss: 1.0138 - mae: 1.0138 - val_loss: 0.8517 - val_mae: 0.8517\nEpoch 174/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7220 - mae: 0.7220\nEpoch 174: val_loss did not improve from 0.80780\n7/7 [==============================] - 0s 7ms/step - loss: 0.9990 - mae: 0.9990 - val_loss: 0.8264 - val_mae: 0.8264\nEpoch 175/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6910 - mae: 0.6910\nEpoch 175: val_loss did not improve from 0.80780\n7/7 [==============================] - 0s 7ms/step - loss: 0.9821 - mae: 0.9821 - val_loss: 0.8306 - val_mae: 0.8306\nEpoch 176/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9836 - mae: 0.9836\nEpoch 176: val_loss did not improve from 0.80780\n7/7 [==============================] - 0s 9ms/step - loss: 1.0168 - mae: 1.0168 - val_loss: 0.9235 - val_mae: 0.9235\nEpoch 177/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8746 - mae: 0.8746\nEpoch 177: val_loss did not improve from 0.80780\n7/7 [==============================] - 0s 7ms/step - loss: 1.0364 - mae: 1.0364 - val_loss: 0.9422 - val_mae: 0.9422\nEpoch 178/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7393 - mae: 0.7393\nEpoch 178: val_loss did not improve from 0.80780\n7/7 [==============================] - 0s 8ms/step - loss: 1.0177 - mae: 1.0177 - val_loss: 0.9914 - val_mae: 0.9914\nEpoch 179/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8201 - mae: 0.8201\nEpoch 179: val_loss improved from 0.80780 to 0.76450, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 1.0468 - mae: 1.0468 - val_loss: 0.7645 - val_mae: 0.7645\nEpoch 180/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8464 - mae: 0.8464\nEpoch 180: val_loss did not improve from 0.76450\n7/7 [==============================] - 0s 7ms/step - loss: 0.9456 - mae: 0.9456 - val_loss: 0.9155 - val_mae: 0.9155\nEpoch 181/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9022 - mae: 0.9022\nEpoch 181: val_loss did not improve from 0.76450\n7/7 [==============================] - 0s 7ms/step - loss: 1.0028 - mae: 1.0028 - val_loss: 0.8842 - val_mae: 0.8842\nEpoch 182/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9741 - mae: 0.9741\nEpoch 182: val_loss improved from 0.76450 to 0.75471, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 1.0117 - mae: 1.0117 - val_loss: 0.7547 - val_mae: 0.7547\nEpoch 183/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7416 - mae: 0.7416\nEpoch 183: val_loss did not improve from 0.75471\n7/7 [==============================] - 0s 7ms/step - loss: 0.9992 - mae: 0.9992 - val_loss: 0.8101 - val_mae: 0.8101\nEpoch 184/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6901 - mae: 0.6901\nEpoch 184: val_loss did not improve from 0.75471\n7/7 [==============================] - 0s 7ms/step - loss: 0.9819 - mae: 0.9819 - val_loss: 0.7952 - val_mae: 0.7952\nEpoch 185/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9325 - mae: 0.9325\nEpoch 185: val_loss did not improve from 0.75471\n7/7 [==============================] - 0s 7ms/step - loss: 0.9976 - mae: 0.9976 - val_loss: 0.7609 - val_mae: 0.7609\nEpoch 186/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.3384 - mae: 1.3384\nEpoch 186: val_loss did not improve from 0.75471\n7/7 [==============================] - 0s 7ms/step - loss: 1.0035 - mae: 1.0035 - val_loss: 0.9225 - val_mae: 0.9225\nEpoch 187/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8932 - mae: 0.8932\nEpoch 187: val_loss improved from 0.75471 to 0.72566, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 0.9637 - mae: 0.9637 - val_loss: 0.7257 - val_mae: 0.7257\nEpoch 188/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7336 - mae: 0.7336\nEpoch 188: val_loss did not improve from 0.72566\n7/7 [==============================] - 0s 7ms/step - loss: 0.8960 - mae: 0.8960 - val_loss: 0.7486 - val_mae: 0.7486\nEpoch 189/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7776 - mae: 0.7776\nEpoch 189: val_loss improved from 0.72566 to 0.72261, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 0.9199 - mae: 0.9199 - val_loss: 0.7226 - val_mae: 0.7226\nEpoch 190/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7123 - mae: 0.7123\nEpoch 190: val_loss did not improve from 0.72261\n7/7 [==============================] - 0s 7ms/step - loss: 0.9018 - mae: 0.9018 - val_loss: 0.7410 - val_mae: 0.7410\nEpoch 191/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6486 - mae: 0.6486\nEpoch 191: val_loss did not improve from 0.72261\n7/7 [==============================] - 0s 7ms/step - loss: 0.9157 - mae: 0.9157 - val_loss: 0.8007 - val_mae: 0.8007\nEpoch 192/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8928 - mae: 0.8928\nEpoch 192: val_loss did not improve from 0.72261\n7/7 [==============================] - 0s 7ms/step - loss: 0.9989 - mae: 0.9989 - val_loss: 0.9392 - val_mae: 0.9392\nEpoch 193/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8050 - mae: 0.8050\nEpoch 193: val_loss did not improve from 0.72261\n7/7 [==============================] - 0s 7ms/step - loss: 0.8966 - mae: 0.8966 - val_loss: 0.7598 - val_mae: 0.7598\nEpoch 194/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7715 - mae: 0.7715\nEpoch 194: val_loss did not improve from 0.72261\n7/7 [==============================] - 0s 7ms/step - loss: 0.9291 - mae: 0.9291 - val_loss: 0.7294 - val_mae: 0.7294\nEpoch 195/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8031 - mae: 0.8031\nEpoch 195: val_loss improved from 0.72261 to 0.71920, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.8800 - mae: 0.8800 - val_loss: 0.7192 - val_mae: 0.7192\nEpoch 196/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7081 - mae: 0.7081\nEpoch 196: val_loss improved from 0.71920 to 0.70119, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.8813 - mae: 0.8813 - val_loss: 0.7012 - val_mae: 0.7012\nEpoch 197/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1183 - mae: 1.1183\nEpoch 197: val_loss improved from 0.70119 to 0.67634, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.8708 - mae: 0.8708 - val_loss: 0.6763 - val_mae: 0.6763\nEpoch 198/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9344 - mae: 0.9344\nEpoch 198: val_loss did not improve from 0.67634\n7/7 [==============================] - 0s 7ms/step - loss: 0.8621 - mae: 0.8621 - val_loss: 0.8115 - val_mae: 0.8115\nEpoch 199/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6204 - mae: 0.6204\nEpoch 199: val_loss did not improve from 0.67634\n7/7 [==============================] - 0s 7ms/step - loss: 0.8970 - mae: 0.8970 - val_loss: 0.6886 - val_mae: 0.6886\nEpoch 200/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.2549 - mae: 1.2549\nEpoch 200: val_loss did not improve from 0.67634\n7/7 [==============================] - 0s 7ms/step - loss: 0.8776 - mae: 0.8776 - val_loss: 0.6811 - val_mae: 0.6811\nEpoch 201/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5200 - mae: 0.5200\nEpoch 201: val_loss did not improve from 0.67634\n7/7 [==============================] - 0s 7ms/step - loss: 0.8984 - mae: 0.8984 - val_loss: 0.7712 - val_mae: 0.7712\nEpoch 202/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9582 - mae: 0.9582\nEpoch 202: val_loss improved from 0.67634 to 0.67513, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.8740 - mae: 0.8740 - val_loss: 0.6751 - val_mae: 0.6751\nEpoch 203/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1332 - mae: 1.1332\nEpoch 203: val_loss improved from 0.67513 to 0.65863, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.8282 - mae: 0.8282 - val_loss: 0.6586 - val_mae: 0.6586\nEpoch 204/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7409 - mae: 0.7409\nEpoch 204: val_loss did not improve from 0.65863\n7/7 [==============================] - 0s 7ms/step - loss: 0.8270 - mae: 0.8270 - val_loss: 0.6734 - val_mae: 0.6734\nEpoch 205/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6548 - mae: 0.6548\nEpoch 205: val_loss did not improve from 0.65863\n7/7 [==============================] - 0s 7ms/step - loss: 0.8879 - mae: 0.8879 - val_loss: 0.7245 - val_mae: 0.7245\nEpoch 206/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4614 - mae: 0.4614\nEpoch 206: val_loss did not improve from 0.65863\n7/7 [==============================] - 0s 6ms/step - loss: 0.8391 - mae: 0.8391 - val_loss: 0.6945 - val_mae: 0.6945\nEpoch 207/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0940 - mae: 1.0940\nEpoch 207: val_loss did not improve from 0.65863\n7/7 [==============================] - 0s 7ms/step - loss: 0.8239 - mae: 0.8239 - val_loss: 0.7788 - val_mae: 0.7788\nEpoch 208/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0070 - mae: 1.0070\nEpoch 208: val_loss did not improve from 0.65863\n7/7 [==============================] - 0s 10ms/step - loss: 0.8595 - mae: 0.8595 - val_loss: 0.6659 - val_mae: 0.6659\nEpoch 209/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6252 - mae: 0.6252\nEpoch 209: val_loss improved from 0.65863 to 0.64795, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.8106 - mae: 0.8106 - val_loss: 0.6480 - val_mae: 0.6480\nEpoch 210/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4997 - mae: 0.4997\nEpoch 210: val_loss improved from 0.64795 to 0.64441, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 0.8104 - mae: 0.8104 - val_loss: 0.6444 - val_mae: 0.6444\nEpoch 211/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4403 - mae: 0.4403\nEpoch 211: val_loss did not improve from 0.64441\n7/7 [==============================] - 0s 7ms/step - loss: 0.8313 - mae: 0.8313 - val_loss: 0.6961 - val_mae: 0.6961\nEpoch 212/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.4595 - mae: 1.4595\nEpoch 212: val_loss did not improve from 0.64441\n7/7 [==============================] - 0s 7ms/step - loss: 0.8830 - mae: 0.8830 - val_loss: 0.6457 - val_mae: 0.6457\nEpoch 213/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4989 - mae: 0.4989\nEpoch 213: val_loss did not improve from 0.64441\n7/7 [==============================] - 0s 7ms/step - loss: 0.8239 - mae: 0.8239 - val_loss: 0.7087 - val_mae: 0.7087\nEpoch 214/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7459 - mae: 0.7459\nEpoch 214: val_loss improved from 0.64441 to 0.62726, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.8469 - mae: 0.8469 - val_loss: 0.6273 - val_mae: 0.6273\nEpoch 215/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.2862 - mae: 1.2862\nEpoch 215: val_loss improved from 0.62726 to 0.61595, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.9018 - mae: 0.9018 - val_loss: 0.6159 - val_mae: 0.6159\nEpoch 216/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9068 - mae: 0.9068\nEpoch 216: val_loss did not improve from 0.61595\n7/7 [==============================] - 0s 7ms/step - loss: 0.8476 - mae: 0.8476 - val_loss: 0.7727 - val_mae: 0.7727\nEpoch 217/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1315 - mae: 1.1315\nEpoch 217: val_loss did not improve from 0.61595\n7/7 [==============================] - 0s 7ms/step - loss: 0.9176 - mae: 0.9176 - val_loss: 0.6241 - val_mae: 0.6241\nEpoch 218/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0417 - mae: 1.0417\nEpoch 218: val_loss did not improve from 0.61595\n7/7 [==============================] - 0s 7ms/step - loss: 0.8709 - mae: 0.8709 - val_loss: 0.8150 - val_mae: 0.8150\nEpoch 219/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0002 - mae: 1.0002\nEpoch 219: val_loss improved from 0.61595 to 0.60392, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.8699 - mae: 0.8699 - val_loss: 0.6039 - val_mae: 0.6039\nEpoch 220/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.2109 - mae: 1.2109\nEpoch 220: val_loss did not improve from 0.60392\n7/7 [==============================] - 0s 7ms/step - loss: 0.7933 - mae: 0.7933 - val_loss: 0.6551 - val_mae: 0.6551\nEpoch 221/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8357 - mae: 0.8357\nEpoch 221: val_loss did not improve from 0.60392\n7/7 [==============================] - 0s 7ms/step - loss: 0.8276 - mae: 0.8276 - val_loss: 0.6479 - val_mae: 0.6479\nEpoch 222/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0258 - mae: 1.0258\nEpoch 222: val_loss did not improve from 0.60392\n7/7 [==============================] - 0s 7ms/step - loss: 0.8119 - mae: 0.8119 - val_loss: 0.7242 - val_mae: 0.7242\nEpoch 223/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9011 - mae: 0.9011\nEpoch 223: val_loss improved from 0.60392 to 0.60372, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.9303 - mae: 0.9303 - val_loss: 0.6037 - val_mae: 0.6037\nEpoch 224/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1039 - mae: 1.1039\nEpoch 224: val_loss did not improve from 0.60372\n7/7 [==============================] - 0s 7ms/step - loss: 0.8000 - mae: 0.8000 - val_loss: 0.6182 - val_mae: 0.6182\nEpoch 225/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5018 - mae: 0.5018\nEpoch 225: val_loss did not improve from 0.60372\n7/7 [==============================] - 0s 7ms/step - loss: 0.8179 - mae: 0.8179 - val_loss: 0.7553 - val_mae: 0.7553\nEpoch 226/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9029 - mae: 0.9029\nEpoch 226: val_loss did not improve from 0.60372\n7/7 [==============================] - 0s 7ms/step - loss: 0.8265 - mae: 0.8265 - val_loss: 0.6063 - val_mae: 0.6063\nEpoch 227/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0802 - mae: 1.0802\nEpoch 227: val_loss improved from 0.60372 to 0.59752, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 0.7648 - mae: 0.7648 - val_loss: 0.5975 - val_mae: 0.5975\nEpoch 228/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9997 - mae: 0.9997\nEpoch 228: val_loss did not improve from 0.59752\n7/7 [==============================] - 0s 7ms/step - loss: 0.7564 - mae: 0.7564 - val_loss: 0.6002 - val_mae: 0.6002\nEpoch 229/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1822 - mae: 1.1822\nEpoch 229: val_loss did not improve from 0.59752\n7/7 [==============================] - 0s 7ms/step - loss: 0.7854 - mae: 0.7854 - val_loss: 0.7492 - val_mae: 0.7492\nEpoch 230/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.2489 - mae: 1.2489\nEpoch 230: val_loss improved from 0.59752 to 0.58064, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.8875 - mae: 0.8875 - val_loss: 0.5806 - val_mae: 0.5806\nEpoch 231/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7907 - mae: 0.7907\nEpoch 231: val_loss did not improve from 0.58064\n7/7 [==============================] - 0s 7ms/step - loss: 0.8109 - mae: 0.8109 - val_loss: 0.5974 - val_mae: 0.5974\nEpoch 232/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8443 - mae: 0.8443\nEpoch 232: val_loss did not improve from 0.58064\n7/7 [==============================] - 0s 7ms/step - loss: 0.7609 - mae: 0.7609 - val_loss: 0.5812 - val_mae: 0.5812\nEpoch 233/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4608 - mae: 0.4608\nEpoch 233: val_loss improved from 0.58064 to 0.56628, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.7484 - mae: 0.7484 - val_loss: 0.5663 - val_mae: 0.5663\nEpoch 234/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8338 - mae: 0.8338\nEpoch 234: val_loss did not improve from 0.56628\n7/7 [==============================] - 0s 7ms/step - loss: 0.7504 - mae: 0.7504 - val_loss: 0.5680 - val_mae: 0.5680\nEpoch 235/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4337 - mae: 0.4337\nEpoch 235: val_loss did not improve from 0.56628\n7/7 [==============================] - 0s 7ms/step - loss: 0.7705 - mae: 0.7705 - val_loss: 0.6314 - val_mae: 0.6314\nEpoch 236/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7026 - mae: 0.7026\nEpoch 236: val_loss did not improve from 0.56628\n7/7 [==============================] - 0s 7ms/step - loss: 0.7970 - mae: 0.7970 - val_loss: 0.6598 - val_mae: 0.6598\nEpoch 237/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5630 - mae: 0.5630\nEpoch 237: val_loss improved from 0.56628 to 0.55365, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.7874 - mae: 0.7874 - val_loss: 0.5537 - val_mae: 0.5537\nEpoch 238/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.4973 - mae: 1.4973\nEpoch 238: val_loss did not improve from 0.55365\n7/7 [==============================] - 0s 7ms/step - loss: 0.8032 - mae: 0.8032 - val_loss: 0.5799 - val_mae: 0.5799\nEpoch 239/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4615 - mae: 0.4615\nEpoch 239: val_loss did not improve from 0.55365\n7/7 [==============================] - 0s 7ms/step - loss: 0.7468 - mae: 0.7468 - val_loss: 0.5901 - val_mae: 0.5901\nEpoch 240/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5653 - mae: 0.5653\nEpoch 240: val_loss did not improve from 0.55365\n7/7 [==============================] - 0s 9ms/step - loss: 0.7796 - mae: 0.7796 - val_loss: 0.5705 - val_mae: 0.5705\nEpoch 241/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8793 - mae: 0.8793\nEpoch 241: val_loss did not improve from 0.55365\n7/7 [==============================] - 0s 7ms/step - loss: 0.7843 - mae: 0.7843 - val_loss: 0.5716 - val_mae: 0.5716\nEpoch 242/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8144 - mae: 0.8144\nEpoch 242: val_loss did not improve from 0.55365\n7/7 [==============================] - 0s 7ms/step - loss: 0.7311 - mae: 0.7311 - val_loss: 0.5613 - val_mae: 0.5613\nEpoch 243/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0419 - mae: 1.0419\nEpoch 243: val_loss did not improve from 0.55365\n7/7 [==============================] - 0s 6ms/step - loss: 0.7297 - mae: 0.7297 - val_loss: 0.6196 - val_mae: 0.6196\nEpoch 244/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8961 - mae: 0.8961\nEpoch 244: val_loss did not improve from 0.55365\n7/7 [==============================] - 0s 7ms/step - loss: 0.7642 - mae: 0.7642 - val_loss: 0.6783 - val_mae: 0.6783\nEpoch 245/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8981 - mae: 0.8981\nEpoch 245: val_loss did not improve from 0.55365\n7/7 [==============================] - 0s 7ms/step - loss: 0.8082 - mae: 0.8082 - val_loss: 0.5748 - val_mae: 0.5748\nEpoch 246/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7075 - mae: 0.7075\nEpoch 246: val_loss did not improve from 0.55365\n7/7 [==============================] - 0s 6ms/step - loss: 0.8134 - mae: 0.8134 - val_loss: 0.7050 - val_mae: 0.7050\nEpoch 247/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1674 - mae: 1.1674\nEpoch 247: val_loss improved from 0.55365 to 0.54225, saving model to best_model.h5\n7/7 [==============================] - 0s 11ms/step - loss: 0.7997 - mae: 0.7997 - val_loss: 0.5422 - val_mae: 0.5422\nEpoch 248/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9209 - mae: 0.9209\nEpoch 248: val_loss did not improve from 0.54225\n7/7 [==============================] - 0s 9ms/step - loss: 0.7198 - mae: 0.7198 - val_loss: 0.5829 - val_mae: 0.5829\nEpoch 249/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8325 - mae: 0.8325\nEpoch 249: val_loss did not improve from 0.54225\n7/7 [==============================] - 0s 7ms/step - loss: 0.7345 - mae: 0.7345 - val_loss: 0.5755 - val_mae: 0.5755\nEpoch 250/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6367 - mae: 0.6367\nEpoch 250: val_loss improved from 0.54225 to 0.53578, saving model to best_model.h5\n7/7 [==============================] - 0s 12ms/step - loss: 0.7076 - mae: 0.7076 - val_loss: 0.5358 - val_mae: 0.5358\nEpoch 251/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9308 - mae: 0.9308\nEpoch 251: val_loss improved from 0.53578 to 0.52824, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.7676 - mae: 0.7676 - val_loss: 0.5282 - val_mae: 0.5282\nEpoch 252/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0981 - mae: 1.0981\nEpoch 252: val_loss did not improve from 0.52824\n7/7 [==============================] - 0s 7ms/step - loss: 0.7660 - mae: 0.7660 - val_loss: 0.7007 - val_mae: 0.7007\nEpoch 253/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9767 - mae: 0.9767\nEpoch 253: val_loss did not improve from 0.52824\n7/7 [==============================] - 0s 7ms/step - loss: 0.7636 - mae: 0.7636 - val_loss: 0.5795 - val_mae: 0.5795\nEpoch 254/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6660 - mae: 0.6660\nEpoch 254: val_loss did not improve from 0.52824\n7/7 [==============================] - 0s 7ms/step - loss: 0.7797 - mae: 0.7797 - val_loss: 0.5942 - val_mae: 0.5942\nEpoch 255/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6266 - mae: 0.6266\nEpoch 255: val_loss did not improve from 0.52824\n7/7 [==============================] - 0s 6ms/step - loss: 0.7211 - mae: 0.7211 - val_loss: 0.5336 - val_mae: 0.5336\nEpoch 256/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9676 - mae: 0.9676\nEpoch 256: val_loss did not improve from 0.52824\n7/7 [==============================] - 0s 7ms/step - loss: 0.7180 - mae: 0.7180 - val_loss: 0.5795 - val_mae: 0.5795\nEpoch 257/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8921 - mae: 0.8921\nEpoch 257: val_loss did not improve from 0.52824\n7/7 [==============================] - 0s 7ms/step - loss: 0.7172 - mae: 0.7172 - val_loss: 0.5557 - val_mae: 0.5557\nEpoch 258/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6048 - mae: 0.6048\nEpoch 258: val_loss did not improve from 0.52824\n7/7 [==============================] - 0s 6ms/step - loss: 0.7527 - mae: 0.7527 - val_loss: 0.5745 - val_mae: 0.5745\nEpoch 259/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5658 - mae: 0.5658\nEpoch 259: val_loss improved from 0.52824 to 0.49873, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 0.7216 - mae: 0.7216 - val_loss: 0.4987 - val_mae: 0.4987\nEpoch 260/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3636 - mae: 0.3636\nEpoch 260: val_loss did not improve from 0.49873\n7/7 [==============================] - 0s 7ms/step - loss: 0.6823 - mae: 0.6823 - val_loss: 0.5120 - val_mae: 0.5120\nEpoch 261/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4046 - mae: 0.4046\nEpoch 261: val_loss did not improve from 0.49873\n7/7 [==============================] - 0s 7ms/step - loss: 0.7128 - mae: 0.7128 - val_loss: 0.5081 - val_mae: 0.5081\nEpoch 262/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5539 - mae: 0.5539\nEpoch 262: val_loss did not improve from 0.49873\n7/7 [==============================] - 0s 7ms/step - loss: 0.7188 - mae: 0.7188 - val_loss: 0.5361 - val_mae: 0.5361\nEpoch 263/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8996 - mae: 0.8996\nEpoch 263: val_loss did not improve from 0.49873\n7/7 [==============================] - 0s 7ms/step - loss: 0.7185 - mae: 0.7185 - val_loss: 0.6331 - val_mae: 0.6331\nEpoch 264/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1808 - mae: 1.1808\nEpoch 264: val_loss did not improve from 0.49873\n7/7 [==============================] - 0s 7ms/step - loss: 0.7333 - mae: 0.7333 - val_loss: 0.5437 - val_mae: 0.5437\nEpoch 265/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5312 - mae: 0.5312\nEpoch 265: val_loss did not improve from 0.49873\n7/7 [==============================] - 0s 7ms/step - loss: 0.7531 - mae: 0.7531 - val_loss: 0.5137 - val_mae: 0.5137\nEpoch 266/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8711 - mae: 0.8711\nEpoch 266: val_loss did not improve from 0.49873\n7/7 [==============================] - 0s 7ms/step - loss: 0.7526 - mae: 0.7526 - val_loss: 0.6080 - val_mae: 0.6080\nEpoch 267/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7929 - mae: 0.7929\nEpoch 267: val_loss did not improve from 0.49873\n7/7 [==============================] - 0s 7ms/step - loss: 0.6845 - mae: 0.6845 - val_loss: 0.5111 - val_mae: 0.5111\nEpoch 268/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9415 - mae: 0.9415\nEpoch 268: val_loss did not improve from 0.49873\n7/7 [==============================] - 0s 7ms/step - loss: 0.6790 - mae: 0.6790 - val_loss: 0.5102 - val_mae: 0.5102\nEpoch 269/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9676 - mae: 0.9676\nEpoch 269: val_loss did not improve from 0.49873\n7/7 [==============================] - 0s 7ms/step - loss: 0.7029 - mae: 0.7029 - val_loss: 0.5187 - val_mae: 0.5187\nEpoch 270/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6252 - mae: 0.6252\nEpoch 270: val_loss improved from 0.49873 to 0.48333, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.7183 - mae: 0.7183 - val_loss: 0.4833 - val_mae: 0.4833\nEpoch 271/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4341 - mae: 0.4341\nEpoch 271: val_loss did not improve from 0.48333\n7/7 [==============================] - 0s 7ms/step - loss: 0.6780 - mae: 0.6780 - val_loss: 0.4837 - val_mae: 0.4837\nEpoch 272/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5549 - mae: 0.5549\nEpoch 272: val_loss did not improve from 0.48333\n7/7 [==============================] - 0s 9ms/step - loss: 0.6939 - mae: 0.6939 - val_loss: 0.4895 - val_mae: 0.4895\nEpoch 273/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8839 - mae: 0.8839\nEpoch 273: val_loss did not improve from 0.48333\n7/7 [==============================] - 0s 7ms/step - loss: 0.6785 - mae: 0.6785 - val_loss: 0.4944 - val_mae: 0.4944\nEpoch 274/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6010 - mae: 0.6010\nEpoch 274: val_loss did not improve from 0.48333\n7/7 [==============================] - 0s 7ms/step - loss: 0.6862 - mae: 0.6862 - val_loss: 0.5751 - val_mae: 0.5751\nEpoch 275/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0863 - mae: 1.0863\nEpoch 275: val_loss did not improve from 0.48333\n7/7 [==============================] - 0s 7ms/step - loss: 0.7513 - mae: 0.7513 - val_loss: 0.5218 - val_mae: 0.5218\nEpoch 276/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5744 - mae: 0.5744\nEpoch 276: val_loss did not improve from 0.48333\n7/7 [==============================] - 0s 6ms/step - loss: 0.6932 - mae: 0.6932 - val_loss: 0.5121 - val_mae: 0.5121\nEpoch 277/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9143 - mae: 0.9143\nEpoch 277: val_loss improved from 0.48333 to 0.46964, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 0.6763 - mae: 0.6763 - val_loss: 0.4696 - val_mae: 0.4696\nEpoch 278/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4733 - mae: 0.4733\nEpoch 278: val_loss did not improve from 0.46964\n7/7 [==============================] - 0s 7ms/step - loss: 0.6613 - mae: 0.6613 - val_loss: 0.4880 - val_mae: 0.4880\nEpoch 279/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4609 - mae: 0.4609\nEpoch 279: val_loss did not improve from 0.46964\n7/7 [==============================] - 0s 9ms/step - loss: 0.6630 - mae: 0.6630 - val_loss: 0.5075 - val_mae: 0.5075\nEpoch 280/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4933 - mae: 0.4933\nEpoch 280: val_loss did not improve from 0.46964\n7/7 [==============================] - 0s 7ms/step - loss: 0.6729 - mae: 0.6729 - val_loss: 0.4753 - val_mae: 0.4753\nEpoch 281/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4742 - mae: 0.4742\nEpoch 281: val_loss did not improve from 0.46964\n7/7 [==============================] - 0s 7ms/step - loss: 0.6494 - mae: 0.6494 - val_loss: 0.4852 - val_mae: 0.4852\nEpoch 282/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8745 - mae: 0.8745\nEpoch 282: val_loss did not improve from 0.46964\n7/7 [==============================] - 0s 9ms/step - loss: 0.6492 - mae: 0.6492 - val_loss: 0.4810 - val_mae: 0.4810\nEpoch 283/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6876 - mae: 0.6876\nEpoch 283: val_loss did not improve from 0.46964\n7/7 [==============================] - 0s 7ms/step - loss: 0.6436 - mae: 0.6436 - val_loss: 0.4877 - val_mae: 0.4877\nEpoch 284/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3924 - mae: 0.3924\nEpoch 284: val_loss did not improve from 0.46964\n7/7 [==============================] - 0s 7ms/step - loss: 0.6593 - mae: 0.6593 - val_loss: 0.5050 - val_mae: 0.5050\nEpoch 285/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1712 - mae: 1.1712\nEpoch 285: val_loss did not improve from 0.46964\n7/7 [==============================] - 0s 7ms/step - loss: 0.6630 - mae: 0.6630 - val_loss: 0.5499 - val_mae: 0.5499\nEpoch 286/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4571 - mae: 0.4571\nEpoch 286: val_loss did not improve from 0.46964\n7/7 [==============================] - 0s 7ms/step - loss: 0.6838 - mae: 0.6838 - val_loss: 0.5289 - val_mae: 0.5289\nEpoch 287/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3903 - mae: 0.3903\nEpoch 287: val_loss did not improve from 0.46964\n7/7 [==============================] - 0s 10ms/step - loss: 0.7415 - mae: 0.7415 - val_loss: 0.4697 - val_mae: 0.4697\nEpoch 288/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8986 - mae: 0.8986\nEpoch 288: val_loss did not improve from 0.46964\n7/7 [==============================] - 0s 8ms/step - loss: 0.7011 - mae: 0.7011 - val_loss: 0.6752 - val_mae: 0.6752\nEpoch 289/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7397 - mae: 0.7397\nEpoch 289: val_loss did not improve from 0.46964\n7/7 [==============================] - 0s 7ms/step - loss: 0.7379 - mae: 0.7379 - val_loss: 0.5355 - val_mae: 0.5355\nEpoch 290/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6502 - mae: 0.6502\nEpoch 290: val_loss improved from 0.46964 to 0.46852, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.7089 - mae: 0.7089 - val_loss: 0.4685 - val_mae: 0.4685\nEpoch 291/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9540 - mae: 0.9540\nEpoch 291: val_loss did not improve from 0.46852\n7/7 [==============================] - 0s 7ms/step - loss: 0.6822 - mae: 0.6822 - val_loss: 0.5715 - val_mae: 0.5715\nEpoch 292/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6694 - mae: 0.6694\nEpoch 292: val_loss did not improve from 0.46852\n7/7 [==============================] - 0s 7ms/step - loss: 0.7226 - mae: 0.7226 - val_loss: 0.5379 - val_mae: 0.5379\nEpoch 293/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0892 - mae: 1.0892\nEpoch 293: val_loss did not improve from 0.46852\n7/7 [==============================] - 0s 7ms/step - loss: 0.6818 - mae: 0.6818 - val_loss: 0.5454 - val_mae: 0.5454\nEpoch 294/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5437 - mae: 0.5437\nEpoch 294: val_loss did not improve from 0.46852\n7/7 [==============================] - 0s 7ms/step - loss: 0.6817 - mae: 0.6817 - val_loss: 0.4726 - val_mae: 0.4726\nEpoch 295/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3492 - mae: 0.3492\nEpoch 295: val_loss did not improve from 0.46852\n7/7 [==============================] - 0s 7ms/step - loss: 0.6465 - mae: 0.6465 - val_loss: 0.4687 - val_mae: 0.4687\nEpoch 296/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7154 - mae: 0.7154\nEpoch 296: val_loss improved from 0.46852 to 0.46470, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 0.6525 - mae: 0.6525 - val_loss: 0.4647 - val_mae: 0.4647\nEpoch 297/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4169 - mae: 0.4169\nEpoch 297: val_loss did not improve from 0.46470\n7/7 [==============================] - 0s 6ms/step - loss: 0.6474 - mae: 0.6474 - val_loss: 0.4855 - val_mae: 0.4855\nEpoch 298/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6957 - mae: 0.6957\nEpoch 298: val_loss improved from 0.46470 to 0.44696, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.6202 - mae: 0.6202 - val_loss: 0.4470 - val_mae: 0.4470\nEpoch 299/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4203 - mae: 0.4203\nEpoch 299: val_loss did not improve from 0.44696\n7/7 [==============================] - 0s 7ms/step - loss: 0.6397 - mae: 0.6397 - val_loss: 0.4625 - val_mae: 0.4625\nEpoch 300/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4950 - mae: 0.4950\nEpoch 300: val_loss improved from 0.44696 to 0.43355, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.6391 - mae: 0.6391 - val_loss: 0.4335 - val_mae: 0.4335\nEpoch 301/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6525 - mae: 0.6525\nEpoch 301: val_loss improved from 0.43355 to 0.43336, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.6293 - mae: 0.6293 - val_loss: 0.4334 - val_mae: 0.4334\nEpoch 302/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0775 - mae: 1.0775\nEpoch 302: val_loss did not improve from 0.43336\n7/7 [==============================] - 0s 7ms/step - loss: 0.6414 - mae: 0.6414 - val_loss: 0.4552 - val_mae: 0.4552\nEpoch 303/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9237 - mae: 0.9237\nEpoch 303: val_loss did not improve from 0.43336\n7/7 [==============================] - 0s 7ms/step - loss: 0.6268 - mae: 0.6268 - val_loss: 0.4399 - val_mae: 0.4399\nEpoch 304/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9476 - mae: 0.9476\nEpoch 304: val_loss did not improve from 0.43336\n7/7 [==============================] - 0s 7ms/step - loss: 0.6155 - mae: 0.6155 - val_loss: 0.4461 - val_mae: 0.4461\nEpoch 305/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6804 - mae: 0.6804\nEpoch 305: val_loss did not improve from 0.43336\n7/7 [==============================] - 0s 7ms/step - loss: 0.6218 - mae: 0.6218 - val_loss: 0.4660 - val_mae: 0.4660\nEpoch 306/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8486 - mae: 0.8486\nEpoch 306: val_loss improved from 0.43336 to 0.42983, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.6109 - mae: 0.6109 - val_loss: 0.4298 - val_mae: 0.4298\nEpoch 307/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8812 - mae: 0.8812\nEpoch 307: val_loss improved from 0.42983 to 0.42560, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 0.6139 - mae: 0.6139 - val_loss: 0.4256 - val_mae: 0.4256\nEpoch 308/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5940 - mae: 0.5940\nEpoch 308: val_loss did not improve from 0.42560\n7/7 [==============================] - 0s 7ms/step - loss: 0.6114 - mae: 0.6114 - val_loss: 0.4293 - val_mae: 0.4293\nEpoch 309/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3514 - mae: 0.3514\nEpoch 309: val_loss improved from 0.42560 to 0.42123, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.6143 - mae: 0.6143 - val_loss: 0.4212 - val_mae: 0.4212\nEpoch 310/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4703 - mae: 0.4703\nEpoch 310: val_loss did not improve from 0.42123\n7/7 [==============================] - 0s 7ms/step - loss: 0.6104 - mae: 0.6104 - val_loss: 0.5661 - val_mae: 0.5661\nEpoch 311/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4635 - mae: 0.4635\nEpoch 311: val_loss did not improve from 0.42123\n7/7 [==============================] - 0s 7ms/step - loss: 0.6542 - mae: 0.6542 - val_loss: 0.4437 - val_mae: 0.4437\nEpoch 312/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.2948 - mae: 1.2948\nEpoch 312: val_loss did not improve from 0.42123\n7/7 [==============================] - 0s 7ms/step - loss: 0.6060 - mae: 0.6060 - val_loss: 0.4489 - val_mae: 0.4489\nEpoch 313/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6473 - mae: 0.6473\nEpoch 313: val_loss did not improve from 0.42123\n7/7 [==============================] - 0s 7ms/step - loss: 0.6170 - mae: 0.6170 - val_loss: 0.4245 - val_mae: 0.4245\nEpoch 314/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4986 - mae: 0.4986\nEpoch 314: val_loss did not improve from 0.42123\n7/7 [==============================] - 0s 6ms/step - loss: 0.6816 - mae: 0.6816 - val_loss: 0.4411 - val_mae: 0.4411\nEpoch 315/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2827 - mae: 0.2827\nEpoch 315: val_loss did not improve from 0.42123\n7/7 [==============================] - 0s 7ms/step - loss: 0.6882 - mae: 0.6882 - val_loss: 0.4507 - val_mae: 0.4507\nEpoch 316/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0550 - mae: 1.0550\nEpoch 316: val_loss did not improve from 0.42123\n7/7 [==============================] - 0s 7ms/step - loss: 0.6191 - mae: 0.6191 - val_loss: 0.5419 - val_mae: 0.5419\nEpoch 317/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9954 - mae: 0.9954\nEpoch 317: val_loss did not improve from 0.42123\n7/7 [==============================] - 0s 7ms/step - loss: 0.6232 - mae: 0.6232 - val_loss: 0.4502 - val_mae: 0.4502\nEpoch 318/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4360 - mae: 0.4360\nEpoch 318: val_loss did not improve from 0.42123\n7/7 [==============================] - 0s 7ms/step - loss: 0.6062 - mae: 0.6062 - val_loss: 0.4255 - val_mae: 0.4255\nEpoch 319/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7944 - mae: 0.7944\nEpoch 319: val_loss did not improve from 0.42123\n7/7 [==============================] - 0s 7ms/step - loss: 0.6148 - mae: 0.6148 - val_loss: 0.4754 - val_mae: 0.4754\nEpoch 320/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.4043 - mae: 1.4043\nEpoch 320: val_loss improved from 0.42123 to 0.41918, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.6473 - mae: 0.6473 - val_loss: 0.4192 - val_mae: 0.4192\nEpoch 321/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3760 - mae: 0.3760\nEpoch 321: val_loss improved from 0.41918 to 0.40982, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.6095 - mae: 0.6095 - val_loss: 0.4098 - val_mae: 0.4098\nEpoch 322/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4360 - mae: 0.4360\nEpoch 322: val_loss did not improve from 0.40982\n7/7 [==============================] - 0s 7ms/step - loss: 0.5870 - mae: 0.5870 - val_loss: 0.4527 - val_mae: 0.4527\nEpoch 323/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6182 - mae: 0.6182\nEpoch 323: val_loss did not improve from 0.40982\n7/7 [==============================] - 0s 7ms/step - loss: 0.6142 - mae: 0.6142 - val_loss: 0.4190 - val_mae: 0.4190\nEpoch 324/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8950 - mae: 0.8950\nEpoch 324: val_loss did not improve from 0.40982\n7/7 [==============================] - 0s 10ms/step - loss: 0.6008 - mae: 0.6008 - val_loss: 0.4433 - val_mae: 0.4433\nEpoch 325/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5892 - mae: 0.5892\nEpoch 325: val_loss did not improve from 0.40982\n7/7 [==============================] - 0s 7ms/step - loss: 0.6201 - mae: 0.6201 - val_loss: 0.4199 - val_mae: 0.4199\nEpoch 326/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3627 - mae: 0.3627\nEpoch 326: val_loss improved from 0.40982 to 0.39422, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 0.6493 - mae: 0.6493 - val_loss: 0.3942 - val_mae: 0.3942\nEpoch 327/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4620 - mae: 0.4620\nEpoch 327: val_loss did not improve from 0.39422\n7/7 [==============================] - 0s 7ms/step - loss: 0.5868 - mae: 0.5868 - val_loss: 0.4471 - val_mae: 0.4471\nEpoch 328/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8350 - mae: 0.8350\nEpoch 328: val_loss did not improve from 0.39422\n7/7 [==============================] - 0s 7ms/step - loss: 0.6091 - mae: 0.6091 - val_loss: 0.4071 - val_mae: 0.4071\nEpoch 329/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3934 - mae: 0.3934\nEpoch 329: val_loss did not improve from 0.39422\n7/7 [==============================] - 0s 7ms/step - loss: 0.6270 - mae: 0.6270 - val_loss: 0.4268 - val_mae: 0.4268\nEpoch 330/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1807 - mae: 1.1807\nEpoch 330: val_loss did not improve from 0.39422\n7/7 [==============================] - 0s 6ms/step - loss: 0.5936 - mae: 0.5936 - val_loss: 0.4647 - val_mae: 0.4647\nEpoch 331/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4917 - mae: 0.4917\nEpoch 331: val_loss did not improve from 0.39422\n7/7 [==============================] - 0s 7ms/step - loss: 0.6797 - mae: 0.6797 - val_loss: 0.4435 - val_mae: 0.4435\nEpoch 332/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4271 - mae: 0.4271\nEpoch 332: val_loss did not improve from 0.39422\n7/7 [==============================] - 0s 7ms/step - loss: 0.6148 - mae: 0.6148 - val_loss: 0.4629 - val_mae: 0.4629\nEpoch 333/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3933 - mae: 0.3933\nEpoch 333: val_loss did not improve from 0.39422\n7/7 [==============================] - 0s 7ms/step - loss: 0.5991 - mae: 0.5991 - val_loss: 0.4190 - val_mae: 0.4190\nEpoch 334/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6219 - mae: 0.6219\nEpoch 334: val_loss did not improve from 0.39422\n7/7 [==============================] - 0s 7ms/step - loss: 0.5932 - mae: 0.5932 - val_loss: 0.3974 - val_mae: 0.3974\nEpoch 335/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5468 - mae: 0.5468\nEpoch 335: val_loss did not improve from 0.39422\n7/7 [==============================] - 0s 7ms/step - loss: 0.6135 - mae: 0.6135 - val_loss: 0.4179 - val_mae: 0.4179\nEpoch 336/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4154 - mae: 0.4154\nEpoch 336: val_loss improved from 0.39422 to 0.38248, saving model to best_model.h5\n7/7 [==============================] - 0s 12ms/step - loss: 0.5950 - mae: 0.5950 - val_loss: 0.3825 - val_mae: 0.3825\nEpoch 337/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3734 - mae: 0.3734\nEpoch 337: val_loss improved from 0.38248 to 0.37809, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.5768 - mae: 0.5768 - val_loss: 0.3781 - val_mae: 0.3781\nEpoch 338/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3825 - mae: 0.3825\nEpoch 338: val_loss did not improve from 0.37809\n7/7 [==============================] - 0s 7ms/step - loss: 0.5849 - mae: 0.5849 - val_loss: 0.4178 - val_mae: 0.4178\nEpoch 339/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9771 - mae: 0.9771\nEpoch 339: val_loss did not improve from 0.37809\n7/7 [==============================] - 0s 7ms/step - loss: 0.5934 - mae: 0.5934 - val_loss: 0.4401 - val_mae: 0.4401\nEpoch 340/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7626 - mae: 0.7626\nEpoch 340: val_loss did not improve from 0.37809\n7/7 [==============================] - 0s 7ms/step - loss: 0.6654 - mae: 0.6654 - val_loss: 0.6081 - val_mae: 0.6081\nEpoch 341/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.2259 - mae: 1.2259\nEpoch 341: val_loss did not improve from 0.37809\n7/7 [==============================] - 0s 7ms/step - loss: 0.7175 - mae: 0.7175 - val_loss: 0.3855 - val_mae: 0.3855\nEpoch 342/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8274 - mae: 0.8274\nEpoch 342: val_loss did not improve from 0.37809\n7/7 [==============================] - 0s 7ms/step - loss: 0.6168 - mae: 0.6168 - val_loss: 0.3849 - val_mae: 0.3849\nEpoch 343/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3346 - mae: 0.3346\nEpoch 343: val_loss improved from 0.37809 to 0.37706, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.5697 - mae: 0.5697 - val_loss: 0.3771 - val_mae: 0.3771\nEpoch 344/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4131 - mae: 0.4131\nEpoch 344: val_loss did not improve from 0.37706\n7/7 [==============================] - 0s 10ms/step - loss: 0.5663 - mae: 0.5663 - val_loss: 0.4243 - val_mae: 0.4243\nEpoch 345/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5539 - mae: 0.5539\nEpoch 345: val_loss improved from 0.37706 to 0.37271, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 0.5992 - mae: 0.5992 - val_loss: 0.3727 - val_mae: 0.3727\nEpoch 346/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2804 - mae: 0.2804\nEpoch 346: val_loss improved from 0.37271 to 0.37141, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.5859 - mae: 0.5859 - val_loss: 0.3714 - val_mae: 0.3714\nEpoch 347/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0492 - mae: 1.0492\nEpoch 347: val_loss did not improve from 0.37141\n7/7 [==============================] - 0s 7ms/step - loss: 0.5734 - mae: 0.5734 - val_loss: 0.3831 - val_mae: 0.3831\nEpoch 348/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2948 - mae: 0.2948\nEpoch 348: val_loss improved from 0.37141 to 0.37050, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.5575 - mae: 0.5575 - val_loss: 0.3705 - val_mae: 0.3705\nEpoch 349/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3162 - mae: 0.3162\nEpoch 349: val_loss did not improve from 0.37050\n7/7 [==============================] - 0s 7ms/step - loss: 0.5907 - mae: 0.5907 - val_loss: 0.3727 - val_mae: 0.3727\nEpoch 350/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2858 - mae: 0.2858\nEpoch 350: val_loss did not improve from 0.37050\n7/7 [==============================] - 0s 7ms/step - loss: 0.5606 - mae: 0.5606 - val_loss: 0.4028 - val_mae: 0.4028\nEpoch 351/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4284 - mae: 0.4284\nEpoch 351: val_loss did not improve from 0.37050\n7/7 [==============================] - 0s 7ms/step - loss: 0.5736 - mae: 0.5736 - val_loss: 0.3941 - val_mae: 0.3941\nEpoch 352/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0513 - mae: 1.0513\nEpoch 352: val_loss improved from 0.37050 to 0.36953, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.5573 - mae: 0.5573 - val_loss: 0.3695 - val_mae: 0.3695\nEpoch 353/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6022 - mae: 0.6022\nEpoch 353: val_loss improved from 0.36953 to 0.36907, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.5637 - mae: 0.5637 - val_loss: 0.3691 - val_mae: 0.3691\nEpoch 354/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5195 - mae: 0.5195\nEpoch 354: val_loss did not improve from 0.36907\n7/7 [==============================] - 0s 7ms/step - loss: 0.5980 - mae: 0.5980 - val_loss: 0.4291 - val_mae: 0.4291\nEpoch 355/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5012 - mae: 0.5012\nEpoch 355: val_loss did not improve from 0.36907\n7/7 [==============================] - 0s 7ms/step - loss: 0.6412 - mae: 0.6412 - val_loss: 0.4094 - val_mae: 0.4094\nEpoch 356/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7357 - mae: 0.7357\nEpoch 356: val_loss did not improve from 0.36907\n7/7 [==============================] - 0s 7ms/step - loss: 0.7120 - mae: 0.7120 - val_loss: 0.6206 - val_mae: 0.6206\nEpoch 357/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6229 - mae: 0.6229\nEpoch 357: val_loss did not improve from 0.36907\n7/7 [==============================] - 0s 7ms/step - loss: 0.6416 - mae: 0.6416 - val_loss: 0.4196 - val_mae: 0.4196\nEpoch 358/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1835 - mae: 1.1835\nEpoch 358: val_loss did not improve from 0.36907\n7/7 [==============================] - 0s 7ms/step - loss: 0.5692 - mae: 0.5692 - val_loss: 0.3790 - val_mae: 0.3790\nEpoch 359/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7897 - mae: 0.7897\nEpoch 359: val_loss did not improve from 0.36907\n7/7 [==============================] - 0s 7ms/step - loss: 0.5526 - mae: 0.5526 - val_loss: 0.4013 - val_mae: 0.4013\nEpoch 360/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7275 - mae: 0.7275\nEpoch 360: val_loss did not improve from 0.36907\n7/7 [==============================] - 0s 7ms/step - loss: 0.5627 - mae: 0.5627 - val_loss: 0.4045 - val_mae: 0.4045\nEpoch 361/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7499 - mae: 0.7499\nEpoch 361: val_loss did not improve from 0.36907\n7/7 [==============================] - 0s 7ms/step - loss: 0.5962 - mae: 0.5962 - val_loss: 0.3829 - val_mae: 0.3829\nEpoch 362/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8472 - mae: 0.8472\nEpoch 362: val_loss did not improve from 0.36907\n7/7 [==============================] - 0s 7ms/step - loss: 0.5795 - mae: 0.5795 - val_loss: 0.4122 - val_mae: 0.4122\nEpoch 363/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4365 - mae: 0.4365\nEpoch 363: val_loss did not improve from 0.36907\n7/7 [==============================] - 0s 8ms/step - loss: 0.5685 - mae: 0.5685 - val_loss: 0.4258 - val_mae: 0.4258\nEpoch 364/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7339 - mae: 0.7339\nEpoch 364: val_loss improved from 0.36907 to 0.36407, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.5539 - mae: 0.5539 - val_loss: 0.3641 - val_mae: 0.3641\nEpoch 365/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3134 - mae: 0.3134\nEpoch 365: val_loss improved from 0.36407 to 0.36171, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.5497 - mae: 0.5497 - val_loss: 0.3617 - val_mae: 0.3617\nEpoch 366/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3924 - mae: 0.3924\nEpoch 366: val_loss improved from 0.36171 to 0.34312, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.5542 - mae: 0.5542 - val_loss: 0.3431 - val_mae: 0.3431\nEpoch 367/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4787 - mae: 0.4787\nEpoch 367: val_loss did not improve from 0.34312\n7/7 [==============================] - 0s 7ms/step - loss: 0.5538 - mae: 0.5538 - val_loss: 0.3698 - val_mae: 0.3698\nEpoch 368/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3086 - mae: 0.3086\nEpoch 368: val_loss did not improve from 0.34312\n7/7 [==============================] - 0s 7ms/step - loss: 0.5663 - mae: 0.5663 - val_loss: 0.3459 - val_mae: 0.3459\nEpoch 369/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4755 - mae: 0.4755\nEpoch 369: val_loss did not improve from 0.34312\n7/7 [==============================] - 0s 10ms/step - loss: 0.5787 - mae: 0.5787 - val_loss: 0.3998 - val_mae: 0.3998\nEpoch 370/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7807 - mae: 0.7807\nEpoch 370: val_loss did not improve from 0.34312\n7/7 [==============================] - 0s 7ms/step - loss: 0.5733 - mae: 0.5733 - val_loss: 0.3919 - val_mae: 0.3919\nEpoch 371/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3643 - mae: 0.3643\nEpoch 371: val_loss did not improve from 0.34312\n7/7 [==============================] - 0s 6ms/step - loss: 0.5963 - mae: 0.5963 - val_loss: 0.6209 - val_mae: 0.6209\nEpoch 372/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1473 - mae: 1.1473\nEpoch 372: val_loss did not improve from 0.34312\n7/7 [==============================] - 0s 7ms/step - loss: 0.6267 - mae: 0.6267 - val_loss: 0.3515 - val_mae: 0.3515\nEpoch 373/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4881 - mae: 0.4881\nEpoch 373: val_loss did not improve from 0.34312\n7/7 [==============================] - 0s 9ms/step - loss: 0.6182 - mae: 0.6182 - val_loss: 0.4013 - val_mae: 0.4013\nEpoch 374/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4451 - mae: 0.4451\nEpoch 374: val_loss did not improve from 0.34312\n7/7 [==============================] - 0s 6ms/step - loss: 0.5535 - mae: 0.5535 - val_loss: 0.5147 - val_mae: 0.5147\nEpoch 375/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4878 - mae: 0.4878\nEpoch 375: val_loss did not improve from 0.34312\n7/7 [==============================] - 0s 6ms/step - loss: 0.5820 - mae: 0.5820 - val_loss: 0.3936 - val_mae: 0.3936\nEpoch 376/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6592 - mae: 0.6592\nEpoch 376: val_loss did not improve from 0.34312\n7/7 [==============================] - 0s 7ms/step - loss: 0.5611 - mae: 0.5611 - val_loss: 0.3610 - val_mae: 0.3610\nEpoch 377/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6056 - mae: 0.6056\nEpoch 377: val_loss did not improve from 0.34312\n7/7 [==============================] - 0s 7ms/step - loss: 0.5312 - mae: 0.5312 - val_loss: 0.4707 - val_mae: 0.4707\nEpoch 378/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3699 - mae: 0.3699\nEpoch 378: val_loss did not improve from 0.34312\n7/7 [==============================] - 0s 7ms/step - loss: 0.5816 - mae: 0.5816 - val_loss: 0.3611 - val_mae: 0.3611\nEpoch 379/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3533 - mae: 0.3533\nEpoch 379: val_loss improved from 0.34312 to 0.33574, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 0.5419 - mae: 0.5419 - val_loss: 0.3357 - val_mae: 0.3357\nEpoch 380/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3034 - mae: 0.3034\nEpoch 380: val_loss did not improve from 0.33574\n7/7 [==============================] - 0s 6ms/step - loss: 0.5407 - mae: 0.5407 - val_loss: 0.3955 - val_mae: 0.3955\nEpoch 381/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5546 - mae: 0.5546\nEpoch 381: val_loss did not improve from 0.33574\n7/7 [==============================] - 0s 9ms/step - loss: 0.5949 - mae: 0.5949 - val_loss: 0.4757 - val_mae: 0.4757\nEpoch 382/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4373 - mae: 0.4373\nEpoch 382: val_loss did not improve from 0.33574\n7/7 [==============================] - 0s 7ms/step - loss: 0.5739 - mae: 0.5739 - val_loss: 0.3489 - val_mae: 0.3489\nEpoch 383/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9373 - mae: 0.9373\nEpoch 383: val_loss did not improve from 0.33574\n7/7 [==============================] - 0s 7ms/step - loss: 0.5309 - mae: 0.5309 - val_loss: 0.3509 - val_mae: 0.3509\nEpoch 384/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3319 - mae: 0.3319\nEpoch 384: val_loss improved from 0.33574 to 0.32725, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.5503 - mae: 0.5503 - val_loss: 0.3273 - val_mae: 0.3273\nEpoch 385/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2328 - mae: 0.2328\nEpoch 385: val_loss did not improve from 0.32725\n7/7 [==============================] - 0s 7ms/step - loss: 0.5326 - mae: 0.5326 - val_loss: 0.3332 - val_mae: 0.3332\nEpoch 386/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3496 - mae: 0.3496\nEpoch 386: val_loss did not improve from 0.32725\n7/7 [==============================] - 0s 7ms/step - loss: 0.5260 - mae: 0.5260 - val_loss: 0.3919 - val_mae: 0.3919\nEpoch 387/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0033 - mae: 1.0033\nEpoch 387: val_loss did not improve from 0.32725\n7/7 [==============================] - 0s 7ms/step - loss: 0.5786 - mae: 0.5786 - val_loss: 0.3368 - val_mae: 0.3368\nEpoch 388/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1098 - mae: 1.1098\nEpoch 388: val_loss did not improve from 0.32725\n7/7 [==============================] - 0s 7ms/step - loss: 0.5326 - mae: 0.5326 - val_loss: 0.3319 - val_mae: 0.3319\nEpoch 389/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3750 - mae: 0.3750\nEpoch 389: val_loss did not improve from 0.32725\n7/7 [==============================] - 0s 7ms/step - loss: 0.5474 - mae: 0.5474 - val_loss: 0.3495 - val_mae: 0.3495\nEpoch 390/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9280 - mae: 0.9280\nEpoch 390: val_loss did not improve from 0.32725\n7/7 [==============================] - 0s 7ms/step - loss: 0.5471 - mae: 0.5471 - val_loss: 0.4927 - val_mae: 0.4927\nEpoch 391/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9300 - mae: 0.9300\nEpoch 391: val_loss did not improve from 0.32725\n7/7 [==============================] - 0s 7ms/step - loss: 0.6095 - mae: 0.6095 - val_loss: 0.4846 - val_mae: 0.4846\nEpoch 392/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4304 - mae: 0.4304\nEpoch 392: val_loss did not improve from 0.32725\n7/7 [==============================] - 0s 7ms/step - loss: 0.6027 - mae: 0.6027 - val_loss: 0.3966 - val_mae: 0.3966\nEpoch 393/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5961 - mae: 0.5961\nEpoch 393: val_loss did not improve from 0.32725\n7/7 [==============================] - 0s 7ms/step - loss: 0.6369 - mae: 0.6369 - val_loss: 0.3575 - val_mae: 0.3575\nEpoch 394/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3543 - mae: 0.3543\nEpoch 394: val_loss did not improve from 0.32725\n7/7 [==============================] - 0s 7ms/step - loss: 0.6269 - mae: 0.6269 - val_loss: 0.3599 - val_mae: 0.3599\nEpoch 395/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4521 - mae: 0.4521\nEpoch 395: val_loss did not improve from 0.32725\n7/7 [==============================] - 0s 7ms/step - loss: 0.5626 - mae: 0.5626 - val_loss: 0.3427 - val_mae: 0.3427\nEpoch 396/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3397 - mae: 0.3397\nEpoch 396: val_loss did not improve from 0.32725\n7/7 [==============================] - 0s 10ms/step - loss: 0.5308 - mae: 0.5308 - val_loss: 0.4686 - val_mae: 0.4686\nEpoch 397/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3666 - mae: 0.3666\nEpoch 397: val_loss did not improve from 0.32725\n7/7 [==============================] - 0s 7ms/step - loss: 0.5710 - mae: 0.5710 - val_loss: 0.3723 - val_mae: 0.3723\nEpoch 398/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3920 - mae: 0.3920\nEpoch 398: val_loss did not improve from 0.32725\n7/7 [==============================] - 0s 7ms/step - loss: 0.5589 - mae: 0.5589 - val_loss: 0.3834 - val_mae: 0.3834\nEpoch 399/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4156 - mae: 0.4156\nEpoch 399: val_loss improved from 0.32725 to 0.32372, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.5407 - mae: 0.5407 - val_loss: 0.3237 - val_mae: 0.3237\nEpoch 400/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8891 - mae: 0.8891\nEpoch 400: val_loss did not improve from 0.32372\n7/7 [==============================] - 0s 7ms/step - loss: 0.5114 - mae: 0.5114 - val_loss: 0.3285 - val_mae: 0.3285\nEpoch 401/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2803 - mae: 0.2803\nEpoch 401: val_loss improved from 0.32372 to 0.31123, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.5163 - mae: 0.5163 - val_loss: 0.3112 - val_mae: 0.3112\nEpoch 402/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2429 - mae: 0.2429\nEpoch 402: val_loss did not improve from 0.31123\n7/7 [==============================] - 0s 7ms/step - loss: 0.5132 - mae: 0.5132 - val_loss: 0.3628 - val_mae: 0.3628\nEpoch 403/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7164 - mae: 0.7164\nEpoch 403: val_loss did not improve from 0.31123\n7/7 [==============================] - 0s 7ms/step - loss: 0.5350 - mae: 0.5350 - val_loss: 0.6248 - val_mae: 0.6248\nEpoch 404/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5781 - mae: 0.5781\nEpoch 404: val_loss did not improve from 0.31123\n7/7 [==============================] - 0s 7ms/step - loss: 0.7229 - mae: 0.7229 - val_loss: 0.3262 - val_mae: 0.3262\nEpoch 405/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6737 - mae: 0.6737\nEpoch 405: val_loss did not improve from 0.31123\n7/7 [==============================] - 0s 7ms/step - loss: 0.6294 - mae: 0.6294 - val_loss: 0.5441 - val_mae: 0.5441\nEpoch 406/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5553 - mae: 0.5553\nEpoch 406: val_loss did not improve from 0.31123\n7/7 [==============================] - 0s 6ms/step - loss: 0.5782 - mae: 0.5782 - val_loss: 0.3153 - val_mae: 0.3153\nEpoch 407/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3403 - mae: 0.3403\nEpoch 407: val_loss did not improve from 0.31123\n7/7 [==============================] - 0s 7ms/step - loss: 0.5233 - mae: 0.5233 - val_loss: 0.3205 - val_mae: 0.3205\nEpoch 408/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5492 - mae: 0.5492\nEpoch 408: val_loss did not improve from 0.31123\n7/7 [==============================] - 0s 10ms/step - loss: 0.5314 - mae: 0.5314 - val_loss: 0.3223 - val_mae: 0.3223\nEpoch 409/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5728 - mae: 0.5728\nEpoch 409: val_loss improved from 0.31123 to 0.31112, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.5360 - mae: 0.5360 - val_loss: 0.3111 - val_mae: 0.3111\nEpoch 410/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6624 - mae: 0.6624\nEpoch 410: val_loss improved from 0.31112 to 0.30869, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.5097 - mae: 0.5097 - val_loss: 0.3087 - val_mae: 0.3087\nEpoch 411/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5235 - mae: 0.5235\nEpoch 411: val_loss improved from 0.30869 to 0.30862, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.5114 - mae: 0.5114 - val_loss: 0.3086 - val_mae: 0.3086\nEpoch 412/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9726 - mae: 0.9726\nEpoch 412: val_loss did not improve from 0.30862\n7/7 [==============================] - 0s 7ms/step - loss: 0.5008 - mae: 0.5008 - val_loss: 0.3947 - val_mae: 0.3947\nEpoch 413/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2913 - mae: 0.2913\nEpoch 413: val_loss did not improve from 0.30862\n7/7 [==============================] - 0s 7ms/step - loss: 0.5302 - mae: 0.5302 - val_loss: 0.4442 - val_mae: 0.4442\nEpoch 414/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1867 - mae: 1.1867\nEpoch 414: val_loss did not improve from 0.30862\n7/7 [==============================] - 0s 7ms/step - loss: 0.6143 - mae: 0.6143 - val_loss: 0.3977 - val_mae: 0.3977\nEpoch 415/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9058 - mae: 0.9058\nEpoch 415: val_loss did not improve from 0.30862\n7/7 [==============================] - 0s 7ms/step - loss: 0.5810 - mae: 0.5810 - val_loss: 0.3250 - val_mae: 0.3250\nEpoch 416/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5637 - mae: 0.5637\nEpoch 416: val_loss did not improve from 0.30862\n7/7 [==============================] - 0s 7ms/step - loss: 0.5321 - mae: 0.5321 - val_loss: 0.3784 - val_mae: 0.3784\nEpoch 417/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3929 - mae: 0.3929\nEpoch 417: val_loss did not improve from 0.30862\n7/7 [==============================] - 0s 7ms/step - loss: 0.5452 - mae: 0.5452 - val_loss: 0.5095 - val_mae: 0.5095\nEpoch 418/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3926 - mae: 0.3926\nEpoch 418: val_loss did not improve from 0.30862\n7/7 [==============================] - 0s 7ms/step - loss: 0.5726 - mae: 0.5726 - val_loss: 0.3920 - val_mae: 0.3920\nEpoch 419/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3765 - mae: 0.3765\nEpoch 419: val_loss did not improve from 0.30862\n7/7 [==============================] - 0s 7ms/step - loss: 0.5145 - mae: 0.5145 - val_loss: 0.4864 - val_mae: 0.4864\nEpoch 420/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4867 - mae: 0.4867\nEpoch 420: val_loss improved from 0.30862 to 0.30515, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.6139 - mae: 0.6139 - val_loss: 0.3052 - val_mae: 0.3052\nEpoch 421/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1720 - mae: 1.1720\nEpoch 421: val_loss improved from 0.30515 to 0.29648, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 0.5577 - mae: 0.5577 - val_loss: 0.2965 - val_mae: 0.2965\nEpoch 422/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0287 - mae: 1.0287\nEpoch 422: val_loss did not improve from 0.29648\n7/7 [==============================] - 0s 7ms/step - loss: 0.5488 - mae: 0.5488 - val_loss: 0.3261 - val_mae: 0.3261\nEpoch 423/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3996 - mae: 0.3996\nEpoch 423: val_loss did not improve from 0.29648\n7/7 [==============================] - 0s 7ms/step - loss: 0.5675 - mae: 0.5675 - val_loss: 0.3171 - val_mae: 0.3171\nEpoch 424/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3098 - mae: 0.3098\nEpoch 424: val_loss did not improve from 0.29648\n7/7 [==============================] - 0s 7ms/step - loss: 0.5345 - mae: 0.5345 - val_loss: 0.3173 - val_mae: 0.3173\nEpoch 425/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3510 - mae: 0.3510\nEpoch 425: val_loss did not improve from 0.29648\n7/7 [==============================] - 0s 7ms/step - loss: 0.5319 - mae: 0.5319 - val_loss: 0.3243 - val_mae: 0.3243\nEpoch 426/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0867 - mae: 1.0867\nEpoch 426: val_loss did not improve from 0.29648\n7/7 [==============================] - 0s 7ms/step - loss: 0.5582 - mae: 0.5582 - val_loss: 0.3133 - val_mae: 0.3133\nEpoch 427/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5866 - mae: 0.5866\nEpoch 427: val_loss did not improve from 0.29648\n7/7 [==============================] - 0s 7ms/step - loss: 0.5479 - mae: 0.5479 - val_loss: 0.3355 - val_mae: 0.3355\nEpoch 428/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2756 - mae: 0.2756\nEpoch 428: val_loss did not improve from 0.29648\n7/7 [==============================] - 0s 7ms/step - loss: 0.5138 - mae: 0.5138 - val_loss: 0.3326 - val_mae: 0.3326\nEpoch 429/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.3440 - mae: 1.3440\nEpoch 429: val_loss did not improve from 0.29648\n7/7 [==============================] - 0s 7ms/step - loss: 0.4990 - mae: 0.4990 - val_loss: 0.3918 - val_mae: 0.3918\nEpoch 430/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7709 - mae: 0.7709\nEpoch 430: val_loss did not improve from 0.29648\n7/7 [==============================] - 0s 7ms/step - loss: 0.5368 - mae: 0.5368 - val_loss: 0.3527 - val_mae: 0.3527\nEpoch 431/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5128 - mae: 0.5128\nEpoch 431: val_loss improved from 0.29648 to 0.29576, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 0.5354 - mae: 0.5354 - val_loss: 0.2958 - val_mae: 0.2958\nEpoch 432/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4601 - mae: 0.4601\nEpoch 432: val_loss did not improve from 0.29576\n7/7 [==============================] - 0s 7ms/step - loss: 0.5054 - mae: 0.5054 - val_loss: 0.5024 - val_mae: 0.5024\nEpoch 433/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6067 - mae: 0.6067\nEpoch 433: val_loss did not improve from 0.29576\n7/7 [==============================] - 0s 7ms/step - loss: 0.5874 - mae: 0.5874 - val_loss: 0.3243 - val_mae: 0.3243\nEpoch 434/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5252 - mae: 0.5252\nEpoch 434: val_loss did not improve from 0.29576\n7/7 [==============================] - 0s 6ms/step - loss: 0.5087 - mae: 0.5087 - val_loss: 0.3139 - val_mae: 0.3139\nEpoch 435/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4679 - mae: 0.4679\nEpoch 435: val_loss did not improve from 0.29576\n7/7 [==============================] - 0s 7ms/step - loss: 0.5219 - mae: 0.5219 - val_loss: 0.3076 - val_mae: 0.3076\nEpoch 436/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2617 - mae: 0.2617\nEpoch 436: val_loss did not improve from 0.29576\n7/7 [==============================] - 0s 7ms/step - loss: 0.4867 - mae: 0.4867 - val_loss: 0.3066 - val_mae: 0.3066\nEpoch 437/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8067 - mae: 0.8067\nEpoch 437: val_loss did not improve from 0.29576\n7/7 [==============================] - 0s 7ms/step - loss: 0.4908 - mae: 0.4908 - val_loss: 0.3583 - val_mae: 0.3583\nEpoch 438/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9611 - mae: 0.9611\nEpoch 438: val_loss did not improve from 0.29576\n7/7 [==============================] - 0s 7ms/step - loss: 0.5488 - mae: 0.5488 - val_loss: 0.3087 - val_mae: 0.3087\nEpoch 439/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8014 - mae: 0.8014\nEpoch 439: val_loss did not improve from 0.29576\n7/7 [==============================] - 0s 7ms/step - loss: 0.4926 - mae: 0.4926 - val_loss: 0.3396 - val_mae: 0.3396\nEpoch 440/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4969 - mae: 0.4969\nEpoch 440: val_loss did not improve from 0.29576\n7/7 [==============================] - 0s 7ms/step - loss: 0.5059 - mae: 0.5059 - val_loss: 0.3748 - val_mae: 0.3748\nEpoch 441/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5580 - mae: 0.5580\nEpoch 441: val_loss did not improve from 0.29576\n7/7 [==============================] - 0s 8ms/step - loss: 0.4970 - mae: 0.4970 - val_loss: 0.3716 - val_mae: 0.3716\nEpoch 442/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9050 - mae: 0.9050\nEpoch 442: val_loss did not improve from 0.29576\n7/7 [==============================] - 0s 9ms/step - loss: 0.5113 - mae: 0.5113 - val_loss: 0.4578 - val_mae: 0.4578\nEpoch 443/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9488 - mae: 0.9488\nEpoch 443: val_loss did not improve from 0.29576\n7/7 [==============================] - 0s 7ms/step - loss: 0.5619 - mae: 0.5619 - val_loss: 0.4411 - val_mae: 0.4411\nEpoch 444/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3851 - mae: 0.3851\nEpoch 444: val_loss did not improve from 0.29576\n7/7 [==============================] - 0s 7ms/step - loss: 0.5284 - mae: 0.5284 - val_loss: 0.3962 - val_mae: 0.3962\nEpoch 445/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8925 - mae: 0.8925\nEpoch 445: val_loss improved from 0.29576 to 0.28574, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 0.5309 - mae: 0.5309 - val_loss: 0.2857 - val_mae: 0.2857\nEpoch 446/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2352 - mae: 0.2352\nEpoch 446: val_loss did not improve from 0.28574\n7/7 [==============================] - 0s 7ms/step - loss: 0.5282 - mae: 0.5282 - val_loss: 0.3165 - val_mae: 0.3165\nEpoch 447/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2632 - mae: 0.2632\nEpoch 447: val_loss did not improve from 0.28574\n7/7 [==============================] - 0s 7ms/step - loss: 0.5034 - mae: 0.5034 - val_loss: 0.3031 - val_mae: 0.3031\nEpoch 448/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3844 - mae: 0.3844\nEpoch 448: val_loss did not improve from 0.28574\n7/7 [==============================] - 0s 7ms/step - loss: 0.4892 - mae: 0.4892 - val_loss: 0.3079 - val_mae: 0.3079\nEpoch 449/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2576 - mae: 0.2576\nEpoch 449: val_loss did not improve from 0.28574\n7/7 [==============================] - 0s 7ms/step - loss: 0.4781 - mae: 0.4781 - val_loss: 0.2906 - val_mae: 0.2906\nEpoch 450/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9134 - mae: 0.9134\nEpoch 450: val_loss did not improve from 0.28574\n7/7 [==============================] - 0s 7ms/step - loss: 0.4885 - mae: 0.4885 - val_loss: 0.3121 - val_mae: 0.3121\nEpoch 451/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6506 - mae: 0.6506\nEpoch 451: val_loss did not improve from 0.28574\n7/7 [==============================] - 0s 7ms/step - loss: 0.4924 - mae: 0.4924 - val_loss: 0.2952 - val_mae: 0.2952\nEpoch 452/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3116 - mae: 0.3116\nEpoch 452: val_loss improved from 0.28574 to 0.28319, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.4834 - mae: 0.4834 - val_loss: 0.2832 - val_mae: 0.2832\nEpoch 453/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.2141 - mae: 1.2141\nEpoch 453: val_loss did not improve from 0.28319\n7/7 [==============================] - 0s 7ms/step - loss: 0.5731 - mae: 0.5731 - val_loss: 0.3535 - val_mae: 0.3535\nEpoch 454/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8254 - mae: 0.8254\nEpoch 454: val_loss did not improve from 0.28319\n7/7 [==============================] - 0s 7ms/step - loss: 0.5414 - mae: 0.5414 - val_loss: 0.5279 - val_mae: 0.5279\nEpoch 455/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6201 - mae: 0.6201\nEpoch 455: val_loss did not improve from 0.28319\n7/7 [==============================] - 0s 8ms/step - loss: 0.6206 - mae: 0.6206 - val_loss: 0.4796 - val_mae: 0.4796\nEpoch 456/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4642 - mae: 0.4642\nEpoch 456: val_loss did not improve from 0.28319\n7/7 [==============================] - 0s 7ms/step - loss: 0.5414 - mae: 0.5414 - val_loss: 0.4142 - val_mae: 0.4142\nEpoch 457/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3624 - mae: 0.3624\nEpoch 457: val_loss did not improve from 0.28319\n7/7 [==============================] - 0s 7ms/step - loss: 0.5205 - mae: 0.5205 - val_loss: 0.3714 - val_mae: 0.3714\nEpoch 458/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7390 - mae: 0.7390\nEpoch 458: val_loss did not improve from 0.28319\n7/7 [==============================] - 0s 7ms/step - loss: 0.5191 - mae: 0.5191 - val_loss: 0.3035 - val_mae: 0.3035\nEpoch 459/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5340 - mae: 0.5340\nEpoch 459: val_loss did not improve from 0.28319\n7/7 [==============================] - 0s 9ms/step - loss: 0.4724 - mae: 0.4724 - val_loss: 0.3004 - val_mae: 0.3004\nEpoch 460/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4580 - mae: 0.4580\nEpoch 460: val_loss did not improve from 0.28319\n7/7 [==============================] - 0s 7ms/step - loss: 0.4751 - mae: 0.4751 - val_loss: 0.3238 - val_mae: 0.3238\nEpoch 461/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3835 - mae: 0.3835\nEpoch 461: val_loss did not improve from 0.28319\n7/7 [==============================] - 0s 7ms/step - loss: 0.4835 - mae: 0.4835 - val_loss: 0.3503 - val_mae: 0.3503\nEpoch 462/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3409 - mae: 0.3409\nEpoch 462: val_loss did not improve from 0.28319\n7/7 [==============================] - 0s 9ms/step - loss: 0.4975 - mae: 0.4975 - val_loss: 0.5237 - val_mae: 0.5237\nEpoch 463/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0222 - mae: 1.0222\nEpoch 463: val_loss did not improve from 0.28319\n7/7 [==============================] - 0s 7ms/step - loss: 0.5819 - mae: 0.5819 - val_loss: 0.4485 - val_mae: 0.4485\nEpoch 464/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1686 - mae: 1.1686\nEpoch 464: val_loss did not improve from 0.28319\n7/7 [==============================] - 0s 7ms/step - loss: 0.5647 - mae: 0.5647 - val_loss: 0.3254 - val_mae: 0.3254\nEpoch 465/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4608 - mae: 0.4608\nEpoch 465: val_loss did not improve from 0.28319\n7/7 [==============================] - 0s 7ms/step - loss: 0.4780 - mae: 0.4780 - val_loss: 0.2891 - val_mae: 0.2891\nEpoch 466/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3785 - mae: 0.3785\nEpoch 466: val_loss did not improve from 0.28319\n7/7 [==============================] - 0s 11ms/step - loss: 0.4795 - mae: 0.4795 - val_loss: 0.3355 - val_mae: 0.3355\nEpoch 467/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8712 - mae: 0.8712\nEpoch 467: val_loss improved from 0.28319 to 0.27155, saving model to best_model.h5\n7/7 [==============================] - 0s 11ms/step - loss: 0.4742 - mae: 0.4742 - val_loss: 0.2716 - val_mae: 0.2716\nEpoch 468/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2895 - mae: 0.2895\nEpoch 468: val_loss did not improve from 0.27155\n7/7 [==============================] - 0s 7ms/step - loss: 0.4697 - mae: 0.4697 - val_loss: 0.2778 - val_mae: 0.2778\nEpoch 469/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4777 - mae: 0.4777\nEpoch 469: val_loss improved from 0.27155 to 0.26438, saving model to best_model.h5\n7/7 [==============================] - 0s 11ms/step - loss: 0.4704 - mae: 0.4704 - val_loss: 0.2644 - val_mae: 0.2644\nEpoch 470/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3277 - mae: 0.3277\nEpoch 470: val_loss improved from 0.26438 to 0.26098, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.4631 - mae: 0.4631 - val_loss: 0.2610 - val_mae: 0.2610\nEpoch 471/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2969 - mae: 0.2969\nEpoch 471: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 7ms/step - loss: 0.5189 - mae: 0.5189 - val_loss: 0.3033 - val_mae: 0.3033\nEpoch 472/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4144 - mae: 0.4144\nEpoch 472: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 7ms/step - loss: 0.4947 - mae: 0.4947 - val_loss: 0.2663 - val_mae: 0.2663\nEpoch 473/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6127 - mae: 0.6127\nEpoch 473: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 8ms/step - loss: 0.4846 - mae: 0.4846 - val_loss: 0.2867 - val_mae: 0.2867\nEpoch 474/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5577 - mae: 0.5577\nEpoch 474: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 10ms/step - loss: 0.4712 - mae: 0.4712 - val_loss: 0.3040 - val_mae: 0.3040\nEpoch 475/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2347 - mae: 0.2347\nEpoch 475: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 7ms/step - loss: 0.4777 - mae: 0.4777 - val_loss: 0.3077 - val_mae: 0.3077\nEpoch 476/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2031 - mae: 0.2031\nEpoch 476: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 8ms/step - loss: 0.5644 - mae: 0.5644 - val_loss: 0.2623 - val_mae: 0.2623\nEpoch 477/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2451 - mae: 0.2451\nEpoch 477: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 8ms/step - loss: 0.5217 - mae: 0.5217 - val_loss: 0.3053 - val_mae: 0.3053\nEpoch 478/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.3507 - mae: 1.3507\nEpoch 478: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 7ms/step - loss: 0.4707 - mae: 0.4707 - val_loss: 0.2630 - val_mae: 0.2630\nEpoch 479/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1938 - mae: 0.1938\nEpoch 479: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 7ms/step - loss: 0.5170 - mae: 0.5170 - val_loss: 0.2661 - val_mae: 0.2661\nEpoch 480/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6764 - mae: 0.6764\nEpoch 480: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 10ms/step - loss: 0.4725 - mae: 0.4725 - val_loss: 0.2829 - val_mae: 0.2829\nEpoch 481/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7821 - mae: 0.7821\nEpoch 481: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 7ms/step - loss: 0.4659 - mae: 0.4659 - val_loss: 0.3391 - val_mae: 0.3391\nEpoch 482/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2168 - mae: 0.2168\nEpoch 482: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 10ms/step - loss: 0.4728 - mae: 0.4728 - val_loss: 0.2981 - val_mae: 0.2981\nEpoch 483/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9079 - mae: 0.9079\nEpoch 483: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 7ms/step - loss: 0.4636 - mae: 0.4636 - val_loss: 0.3029 - val_mae: 0.3029\nEpoch 484/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2295 - mae: 0.2295\nEpoch 484: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 10ms/step - loss: 0.4709 - mae: 0.4709 - val_loss: 0.3158 - val_mae: 0.3158\nEpoch 485/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7449 - mae: 0.7449\nEpoch 485: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 7ms/step - loss: 0.4921 - mae: 0.4921 - val_loss: 0.2726 - val_mae: 0.2726\nEpoch 486/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2424 - mae: 0.2424\nEpoch 486: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 7ms/step - loss: 0.4926 - mae: 0.4926 - val_loss: 0.2740 - val_mae: 0.2740\nEpoch 487/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2301 - mae: 0.2301\nEpoch 487: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 7ms/step - loss: 0.4871 - mae: 0.4871 - val_loss: 0.3148 - val_mae: 0.3148\nEpoch 488/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3205 - mae: 0.3205\nEpoch 488: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 7ms/step - loss: 0.5076 - mae: 0.5076 - val_loss: 0.4007 - val_mae: 0.4007\nEpoch 489/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3801 - mae: 0.3801\nEpoch 489: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 7ms/step - loss: 0.5161 - mae: 0.5161 - val_loss: 0.3532 - val_mae: 0.3532\nEpoch 490/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4255 - mae: 0.4255\nEpoch 490: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 7ms/step - loss: 0.4710 - mae: 0.4710 - val_loss: 0.3422 - val_mae: 0.3422\nEpoch 491/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6832 - mae: 0.6832\nEpoch 491: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 7ms/step - loss: 0.5089 - mae: 0.5089 - val_loss: 0.2908 - val_mae: 0.2908\nEpoch 492/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2998 - mae: 0.2998\nEpoch 492: val_loss did not improve from 0.26098\n7/7 [==============================] - 0s 7ms/step - loss: 0.4755 - mae: 0.4755 - val_loss: 0.2699 - val_mae: 0.2699\nEpoch 493/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2030 - mae: 0.2030\nEpoch 493: val_loss improved from 0.26098 to 0.25518, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.4705 - mae: 0.4705 - val_loss: 0.2552 - val_mae: 0.2552\nEpoch 494/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5551 - mae: 0.5551\nEpoch 494: val_loss did not improve from 0.25518\n7/7 [==============================] - 0s 7ms/step - loss: 0.4544 - mae: 0.4544 - val_loss: 0.3163 - val_mae: 0.3163\nEpoch 495/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2491 - mae: 0.2491\nEpoch 495: val_loss did not improve from 0.25518\n7/7 [==============================] - 0s 7ms/step - loss: 0.4595 - mae: 0.4595 - val_loss: 0.3080 - val_mae: 0.3080\nEpoch 496/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7397 - mae: 0.7397\nEpoch 496: val_loss did not improve from 0.25518\n7/7 [==============================] - 0s 7ms/step - loss: 0.4646 - mae: 0.4646 - val_loss: 0.2777 - val_mae: 0.2777\nEpoch 497/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3801 - mae: 0.3801\nEpoch 497: val_loss did not improve from 0.25518\n7/7 [==============================] - 0s 7ms/step - loss: 0.4966 - mae: 0.4966 - val_loss: 0.3383 - val_mae: 0.3383\nEpoch 498/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2801 - mae: 0.2801\nEpoch 498: val_loss did not improve from 0.25518\n7/7 [==============================] - 0s 7ms/step - loss: 0.5031 - mae: 0.5031 - val_loss: 0.4662 - val_mae: 0.4662\nEpoch 499/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6770 - mae: 0.6770\nEpoch 499: val_loss did not improve from 0.25518\n7/7 [==============================] - 0s 7ms/step - loss: 0.5392 - mae: 0.5392 - val_loss: 0.4572 - val_mae: 0.4572\nEpoch 500/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6537 - mae: 0.6537\nEpoch 500: val_loss did not improve from 0.25518\n7/7 [==============================] - 0s 7ms/step - loss: 0.4987 - mae: 0.4987 - val_loss: 0.3752 - val_mae: 0.3752\nEpoch 501/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3546 - mae: 0.3546\nEpoch 501: val_loss did not improve from 0.25518\n7/7 [==============================] - 0s 7ms/step - loss: 0.5299 - mae: 0.5299 - val_loss: 0.2809 - val_mae: 0.2809\nEpoch 502/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7231 - mae: 0.7231\nEpoch 502: val_loss did not improve from 0.25518\n7/7 [==============================] - 0s 7ms/step - loss: 0.4589 - mae: 0.4589 - val_loss: 0.2754 - val_mae: 0.2754\nEpoch 503/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0547 - mae: 1.0547\nEpoch 503: val_loss did not improve from 0.25518\n7/7 [==============================] - 0s 7ms/step - loss: 0.4891 - mae: 0.4891 - val_loss: 0.2775 - val_mae: 0.2775\nEpoch 504/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3007 - mae: 0.3007\nEpoch 504: val_loss improved from 0.25518 to 0.24229, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.4742 - mae: 0.4742 - val_loss: 0.2423 - val_mae: 0.2423\nEpoch 505/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3960 - mae: 0.3960\nEpoch 505: val_loss did not improve from 0.24229\n7/7 [==============================] - 0s 10ms/step - loss: 0.4375 - mae: 0.4375 - val_loss: 0.2446 - val_mae: 0.2446\nEpoch 506/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2539 - mae: 0.2539\nEpoch 506: val_loss did not improve from 0.24229\n7/7 [==============================] - 0s 7ms/step - loss: 0.4385 - mae: 0.4385 - val_loss: 0.3572 - val_mae: 0.3572\nEpoch 507/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4012 - mae: 0.4012\nEpoch 507: val_loss did not improve from 0.24229\n7/7 [==============================] - 0s 7ms/step - loss: 0.4974 - mae: 0.4974 - val_loss: 0.3342 - val_mae: 0.3342\nEpoch 508/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3007 - mae: 0.3007\nEpoch 508: val_loss did not improve from 0.24229\n7/7 [==============================] - 0s 7ms/step - loss: 0.4890 - mae: 0.4890 - val_loss: 0.4287 - val_mae: 0.4287\nEpoch 509/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4545 - mae: 0.4545\nEpoch 509: val_loss did not improve from 0.24229\n7/7 [==============================] - 0s 7ms/step - loss: 0.5989 - mae: 0.5989 - val_loss: 0.3047 - val_mae: 0.3047\nEpoch 510/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2404 - mae: 0.2404\nEpoch 510: val_loss did not improve from 0.24229\n7/7 [==============================] - 0s 6ms/step - loss: 0.4782 - mae: 0.4782 - val_loss: 0.2766 - val_mae: 0.2766\nEpoch 511/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1951 - mae: 0.1951\nEpoch 511: val_loss did not improve from 0.24229\n7/7 [==============================] - 0s 7ms/step - loss: 0.4425 - mae: 0.4425 - val_loss: 0.2948 - val_mae: 0.2948\nEpoch 512/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3885 - mae: 0.3885\nEpoch 512: val_loss did not improve from 0.24229\n7/7 [==============================] - 0s 7ms/step - loss: 0.4381 - mae: 0.4381 - val_loss: 0.2946 - val_mae: 0.2946\nEpoch 513/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8865 - mae: 0.8865\nEpoch 513: val_loss did not improve from 0.24229\n7/7 [==============================] - 0s 7ms/step - loss: 0.4408 - mae: 0.4408 - val_loss: 0.2498 - val_mae: 0.2498\nEpoch 514/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2372 - mae: 0.2372\nEpoch 514: val_loss did not improve from 0.24229\n7/7 [==============================] - 0s 7ms/step - loss: 0.4335 - mae: 0.4335 - val_loss: 0.3799 - val_mae: 0.3799\nEpoch 515/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5097 - mae: 0.5097\nEpoch 515: val_loss did not improve from 0.24229\n7/7 [==============================] - 0s 6ms/step - loss: 0.5030 - mae: 0.5030 - val_loss: 0.3438 - val_mae: 0.3438\nEpoch 516/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3043 - mae: 0.3043\nEpoch 516: val_loss did not improve from 0.24229\n7/7 [==============================] - 0s 6ms/step - loss: 0.4881 - mae: 0.4881 - val_loss: 0.3464 - val_mae: 0.3464\nEpoch 517/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6279 - mae: 0.6279\nEpoch 517: val_loss did not improve from 0.24229\n7/7 [==============================] - 0s 6ms/step - loss: 0.4745 - mae: 0.4745 - val_loss: 0.4003 - val_mae: 0.4003\nEpoch 518/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0006 - mae: 1.0006\nEpoch 518: val_loss did not improve from 0.24229\n7/7 [==============================] - 0s 7ms/step - loss: 0.5833 - mae: 0.5833 - val_loss: 0.3723 - val_mae: 0.3723\nEpoch 519/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8518 - mae: 0.8518\nEpoch 519: val_loss did not improve from 0.24229\n7/7 [==============================] - 0s 7ms/step - loss: 0.5667 - mae: 0.5667 - val_loss: 0.3009 - val_mae: 0.3009\nEpoch 520/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2281 - mae: 0.2281\nEpoch 520: val_loss did not improve from 0.24229\n7/7 [==============================] - 0s 7ms/step - loss: 0.4735 - mae: 0.4735 - val_loss: 0.2602 - val_mae: 0.2602\nEpoch 521/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4526 - mae: 0.4526\nEpoch 521: val_loss did not improve from 0.24229\n7/7 [==============================] - 0s 7ms/step - loss: 0.4315 - mae: 0.4315 - val_loss: 0.2722 - val_mae: 0.2722\nEpoch 522/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2422 - mae: 0.2422\nEpoch 522: val_loss improved from 0.24229 to 0.23896, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.4614 - mae: 0.4614 - val_loss: 0.2390 - val_mae: 0.2390\nEpoch 523/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2716 - mae: 0.2716\nEpoch 523: val_loss did not improve from 0.23896\n7/7 [==============================] - 0s 7ms/step - loss: 0.5030 - mae: 0.5030 - val_loss: 0.2478 - val_mae: 0.2478\nEpoch 524/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7299 - mae: 0.7299\nEpoch 524: val_loss did not improve from 0.23896\n7/7 [==============================] - 0s 7ms/step - loss: 0.4487 - mae: 0.4487 - val_loss: 0.2721 - val_mae: 0.2721\nEpoch 525/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9444 - mae: 0.9444\nEpoch 525: val_loss did not improve from 0.23896\n7/7 [==============================] - 0s 7ms/step - loss: 0.4460 - mae: 0.4460 - val_loss: 0.2517 - val_mae: 0.2517\nEpoch 526/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2601 - mae: 0.2601\nEpoch 526: val_loss did not improve from 0.23896\n7/7 [==============================] - 0s 7ms/step - loss: 0.4473 - mae: 0.4473 - val_loss: 0.2543 - val_mae: 0.2543\nEpoch 527/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5200 - mae: 0.5200\nEpoch 527: val_loss improved from 0.23896 to 0.23250, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.4546 - mae: 0.4546 - val_loss: 0.2325 - val_mae: 0.2325\nEpoch 528/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6006 - mae: 0.6006\nEpoch 528: val_loss did not improve from 0.23250\n7/7 [==============================] - 0s 7ms/step - loss: 0.4247 - mae: 0.4247 - val_loss: 0.2448 - val_mae: 0.2448\nEpoch 529/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2056 - mae: 0.2056\nEpoch 529: val_loss did not improve from 0.23250\n7/7 [==============================] - 0s 7ms/step - loss: 0.4589 - mae: 0.4589 - val_loss: 0.2663 - val_mae: 0.2663\nEpoch 530/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8418 - mae: 0.8418\nEpoch 530: val_loss did not improve from 0.23250\n7/7 [==============================] - 0s 7ms/step - loss: 0.4208 - mae: 0.4208 - val_loss: 0.3407 - val_mae: 0.3407\nEpoch 531/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2927 - mae: 0.2927\nEpoch 531: val_loss did not improve from 0.23250\n7/7 [==============================] - 0s 7ms/step - loss: 0.4487 - mae: 0.4487 - val_loss: 0.2432 - val_mae: 0.2432\nEpoch 532/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6354 - mae: 0.6354\nEpoch 532: val_loss did not improve from 0.23250\n7/7 [==============================] - 0s 7ms/step - loss: 0.4290 - mae: 0.4290 - val_loss: 0.2904 - val_mae: 0.2904\nEpoch 533/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2833 - mae: 0.2833\nEpoch 533: val_loss improved from 0.23250 to 0.22593, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.4511 - mae: 0.4511 - val_loss: 0.2259 - val_mae: 0.2259\nEpoch 534/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4160 - mae: 0.4160\nEpoch 534: val_loss did not improve from 0.22593\n7/7 [==============================] - 0s 7ms/step - loss: 0.4411 - mae: 0.4411 - val_loss: 0.2627 - val_mae: 0.2627\nEpoch 535/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4910 - mae: 0.4910\nEpoch 535: val_loss did not improve from 0.22593\n7/7 [==============================] - 0s 7ms/step - loss: 0.4547 - mae: 0.4547 - val_loss: 0.2626 - val_mae: 0.2626\nEpoch 536/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8740 - mae: 0.8740\nEpoch 536: val_loss did not improve from 0.22593\n7/7 [==============================] - 0s 7ms/step - loss: 0.4482 - mae: 0.4482 - val_loss: 0.2735 - val_mae: 0.2735\nEpoch 537/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2119 - mae: 0.2119\nEpoch 537: val_loss did not improve from 0.22593\n7/7 [==============================] - 0s 7ms/step - loss: 0.4285 - mae: 0.4285 - val_loss: 0.2545 - val_mae: 0.2545\nEpoch 538/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2053 - mae: 0.2053\nEpoch 538: val_loss did not improve from 0.22593\n7/7 [==============================] - 0s 8ms/step - loss: 0.4467 - mae: 0.4467 - val_loss: 0.2547 - val_mae: 0.2547\nEpoch 539/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5414 - mae: 0.5414\nEpoch 539: val_loss improved from 0.22593 to 0.22411, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.4245 - mae: 0.4245 - val_loss: 0.2241 - val_mae: 0.2241\nEpoch 540/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3924 - mae: 0.3924\nEpoch 540: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.4079 - mae: 0.4079 - val_loss: 0.3698 - val_mae: 0.3698\nEpoch 541/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6334 - mae: 0.6334\nEpoch 541: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.4627 - mae: 0.4627 - val_loss: 0.2493 - val_mae: 0.2493\nEpoch 542/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7615 - mae: 0.7615\nEpoch 542: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 9ms/step - loss: 0.4408 - mae: 0.4408 - val_loss: 0.2957 - val_mae: 0.2957\nEpoch 543/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3674 - mae: 0.3674\nEpoch 543: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.4263 - mae: 0.4263 - val_loss: 0.3800 - val_mae: 0.3800\nEpoch 544/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3144 - mae: 0.3144\nEpoch 544: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.5020 - mae: 0.5020 - val_loss: 0.5164 - val_mae: 0.5164\nEpoch 545/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5807 - mae: 0.5807\nEpoch 545: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.5914 - mae: 0.5914 - val_loss: 0.5897 - val_mae: 0.5897\nEpoch 546/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8313 - mae: 0.8313\nEpoch 546: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.7251 - mae: 0.7251 - val_loss: 0.3307 - val_mae: 0.3307\nEpoch 547/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4366 - mae: 0.4366\nEpoch 547: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.5446 - mae: 0.5446 - val_loss: 0.3547 - val_mae: 0.3547\nEpoch 548/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3737 - mae: 0.3737\nEpoch 548: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.4690 - mae: 0.4690 - val_loss: 0.2302 - val_mae: 0.2302\nEpoch 549/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2132 - mae: 0.2132\nEpoch 549: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.4264 - mae: 0.4264 - val_loss: 0.2258 - val_mae: 0.2258\nEpoch 550/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1964 - mae: 0.1964\nEpoch 550: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.4237 - mae: 0.4237 - val_loss: 0.2847 - val_mae: 0.2847\nEpoch 551/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2514 - mae: 0.2514\nEpoch 551: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.4693 - mae: 0.4693 - val_loss: 0.4767 - val_mae: 0.4767\nEpoch 552/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9902 - mae: 0.9902\nEpoch 552: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.5634 - mae: 0.5634 - val_loss: 0.2294 - val_mae: 0.2294\nEpoch 553/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9328 - mae: 0.9328\nEpoch 553: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.4496 - mae: 0.4496 - val_loss: 0.2704 - val_mae: 0.2704\nEpoch 554/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6591 - mae: 0.6591\nEpoch 554: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.4488 - mae: 0.4488 - val_loss: 0.2340 - val_mae: 0.2340\nEpoch 555/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0356 - mae: 1.0356\nEpoch 555: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.4243 - mae: 0.4243 - val_loss: 0.2994 - val_mae: 0.2994\nEpoch 556/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3439 - mae: 0.3439\nEpoch 556: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.4700 - mae: 0.4700 - val_loss: 0.2982 - val_mae: 0.2982\nEpoch 557/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3048 - mae: 0.3048\nEpoch 557: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.4833 - mae: 0.4833 - val_loss: 0.3367 - val_mae: 0.3367\nEpoch 558/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2959 - mae: 0.2959\nEpoch 558: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.5025 - mae: 0.5025 - val_loss: 0.5397 - val_mae: 0.5397\nEpoch 559/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7435 - mae: 0.7435\nEpoch 559: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.5771 - mae: 0.5771 - val_loss: 0.3443 - val_mae: 0.3443\nEpoch 560/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0995 - mae: 1.0995\nEpoch 560: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.5377 - mae: 0.5377 - val_loss: 0.2957 - val_mae: 0.2957\nEpoch 561/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9311 - mae: 0.9311\nEpoch 561: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.4221 - mae: 0.4221 - val_loss: 0.3239 - val_mae: 0.3239\nEpoch 562/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2401 - mae: 0.2401\nEpoch 562: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.4256 - mae: 0.4256 - val_loss: 0.2410 - val_mae: 0.2410\nEpoch 563/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1874 - mae: 0.1874\nEpoch 563: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.4569 - mae: 0.4569 - val_loss: 0.2297 - val_mae: 0.2297\nEpoch 564/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1481 - mae: 0.1481\nEpoch 564: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.4253 - mae: 0.4253 - val_loss: 0.2389 - val_mae: 0.2389\nEpoch 565/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3002 - mae: 0.3002\nEpoch 565: val_loss did not improve from 0.22411\n7/7 [==============================] - 0s 7ms/step - loss: 0.4084 - mae: 0.4084 - val_loss: 0.2386 - val_mae: 0.2386\nEpoch 566/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7311 - mae: 0.7311\nEpoch 566: val_loss improved from 0.22411 to 0.22199, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.4060 - mae: 0.4060 - val_loss: 0.2220 - val_mae: 0.2220\nEpoch 567/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4816 - mae: 0.4816\nEpoch 567: val_loss did not improve from 0.22199\n7/7 [==============================] - 0s 7ms/step - loss: 0.4131 - mae: 0.4131 - val_loss: 0.2231 - val_mae: 0.2231\nEpoch 568/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3351 - mae: 0.3351\nEpoch 568: val_loss did not improve from 0.22199\n7/7 [==============================] - 0s 7ms/step - loss: 0.4240 - mae: 0.4240 - val_loss: 0.2422 - val_mae: 0.2422\nEpoch 569/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2472 - mae: 0.2472\nEpoch 569: val_loss improved from 0.22199 to 0.22017, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.3985 - mae: 0.3985 - val_loss: 0.2202 - val_mae: 0.2202\nEpoch 570/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5665 - mae: 0.5665\nEpoch 570: val_loss did not improve from 0.22017\n7/7 [==============================] - 0s 7ms/step - loss: 0.4012 - mae: 0.4012 - val_loss: 0.2516 - val_mae: 0.2516\nEpoch 571/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9030 - mae: 0.9030\nEpoch 571: val_loss improved from 0.22017 to 0.21667, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 0.4000 - mae: 0.4000 - val_loss: 0.2167 - val_mae: 0.2167\nEpoch 572/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2347 - mae: 0.2347\nEpoch 572: val_loss did not improve from 0.21667\n7/7 [==============================] - 0s 7ms/step - loss: 0.3996 - mae: 0.3996 - val_loss: 0.3371 - val_mae: 0.3371\nEpoch 573/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6380 - mae: 0.6380\nEpoch 573: val_loss did not improve from 0.21667\n7/7 [==============================] - 0s 7ms/step - loss: 0.4470 - mae: 0.4470 - val_loss: 0.2531 - val_mae: 0.2531\nEpoch 574/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4090 - mae: 0.4090\nEpoch 574: val_loss did not improve from 0.21667\n7/7 [==============================] - 0s 7ms/step - loss: 0.4127 - mae: 0.4127 - val_loss: 0.2539 - val_mae: 0.2539\nEpoch 575/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6259 - mae: 0.6259\nEpoch 575: val_loss improved from 0.21667 to 0.20501, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 0.4130 - mae: 0.4130 - val_loss: 0.2050 - val_mae: 0.2050\nEpoch 576/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3244 - mae: 0.3244\nEpoch 576: val_loss did not improve from 0.20501\n7/7 [==============================] - 0s 7ms/step - loss: 0.4260 - mae: 0.4260 - val_loss: 0.3150 - val_mae: 0.3150\nEpoch 577/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3354 - mae: 0.3354\nEpoch 577: val_loss did not improve from 0.20501\n7/7 [==============================] - 0s 7ms/step - loss: 0.4690 - mae: 0.4690 - val_loss: 0.2589 - val_mae: 0.2589\nEpoch 578/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8131 - mae: 0.8131\nEpoch 578: val_loss did not improve from 0.20501\n7/7 [==============================] - 0s 7ms/step - loss: 0.4269 - mae: 0.4269 - val_loss: 0.3031 - val_mae: 0.3031\nEpoch 579/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5778 - mae: 0.5778\nEpoch 579: val_loss did not improve from 0.20501\n7/7 [==============================] - 0s 7ms/step - loss: 0.4722 - mae: 0.4722 - val_loss: 0.4060 - val_mae: 0.4060\nEpoch 580/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5317 - mae: 0.5317\nEpoch 580: val_loss did not improve from 0.20501\n7/7 [==============================] - 0s 7ms/step - loss: 0.4750 - mae: 0.4750 - val_loss: 0.2638 - val_mae: 0.2638\nEpoch 581/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5750 - mae: 0.5750\nEpoch 581: val_loss did not improve from 0.20501\n7/7 [==============================] - 0s 7ms/step - loss: 0.4234 - mae: 0.4234 - val_loss: 0.2808 - val_mae: 0.2808\nEpoch 582/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2598 - mae: 0.2598\nEpoch 582: val_loss did not improve from 0.20501\n7/7 [==============================] - 0s 7ms/step - loss: 0.4156 - mae: 0.4156 - val_loss: 0.2059 - val_mae: 0.2059\nEpoch 583/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2670 - mae: 0.2670\nEpoch 583: val_loss did not improve from 0.20501\n7/7 [==============================] - 0s 7ms/step - loss: 0.4288 - mae: 0.4288 - val_loss: 0.2184 - val_mae: 0.2184\nEpoch 584/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3267 - mae: 0.3267\nEpoch 584: val_loss did not improve from 0.20501\n7/7 [==============================] - 0s 7ms/step - loss: 0.4049 - mae: 0.4049 - val_loss: 0.3322 - val_mae: 0.3322\nEpoch 585/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3011 - mae: 0.3011\nEpoch 585: val_loss did not improve from 0.20501\n7/7 [==============================] - 0s 7ms/step - loss: 0.4493 - mae: 0.4493 - val_loss: 0.2857 - val_mae: 0.2857\nEpoch 586/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8735 - mae: 0.8735\nEpoch 586: val_loss did not improve from 0.20501\n7/7 [==============================] - 0s 7ms/step - loss: 0.4298 - mae: 0.4298 - val_loss: 0.2111 - val_mae: 0.2111\nEpoch 587/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1757 - mae: 0.1757\nEpoch 587: val_loss did not improve from 0.20501\n7/7 [==============================] - 0s 7ms/step - loss: 0.3944 - mae: 0.3944 - val_loss: 0.2657 - val_mae: 0.2657\nEpoch 588/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3658 - mae: 0.3658\nEpoch 588: val_loss did not improve from 0.20501\n7/7 [==============================] - 0s 7ms/step - loss: 0.3954 - mae: 0.3954 - val_loss: 0.4015 - val_mae: 0.4015\nEpoch 589/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3716 - mae: 0.3716\nEpoch 589: val_loss did not improve from 0.20501\n7/7 [==============================] - 0s 7ms/step - loss: 0.4706 - mae: 0.4706 - val_loss: 0.2635 - val_mae: 0.2635\nEpoch 590/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4361 - mae: 0.4361\nEpoch 590: val_loss improved from 0.20501 to 0.19350, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.4355 - mae: 0.4355 - val_loss: 0.1935 - val_mae: 0.1935\nEpoch 591/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2104 - mae: 0.2104\nEpoch 591: val_loss did not improve from 0.19350\n7/7 [==============================] - 0s 7ms/step - loss: 0.4184 - mae: 0.4184 - val_loss: 0.2556 - val_mae: 0.2556\nEpoch 592/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2678 - mae: 0.2678\nEpoch 592: val_loss did not improve from 0.19350\n7/7 [==============================] - 0s 7ms/step - loss: 0.4151 - mae: 0.4151 - val_loss: 0.2087 - val_mae: 0.2087\nEpoch 593/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2349 - mae: 0.2349\nEpoch 593: val_loss did not improve from 0.19350\n7/7 [==============================] - 0s 7ms/step - loss: 0.3976 - mae: 0.3976 - val_loss: 0.2185 - val_mae: 0.2185\nEpoch 594/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1794 - mae: 0.1794\nEpoch 594: val_loss did not improve from 0.19350\n7/7 [==============================] - 0s 7ms/step - loss: 0.3932 - mae: 0.3932 - val_loss: 0.2288 - val_mae: 0.2288\nEpoch 595/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3865 - mae: 0.3865\nEpoch 595: val_loss did not improve from 0.19350\n7/7 [==============================] - 0s 7ms/step - loss: 0.3928 - mae: 0.3928 - val_loss: 0.2097 - val_mae: 0.2097\nEpoch 596/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1673 - mae: 0.1673\nEpoch 596: val_loss did not improve from 0.19350\n7/7 [==============================] - 0s 7ms/step - loss: 0.3858 - mae: 0.3858 - val_loss: 0.1981 - val_mae: 0.1981\nEpoch 597/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1630 - mae: 0.1630\nEpoch 597: val_loss improved from 0.19350 to 0.19339, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.3906 - mae: 0.3906 - val_loss: 0.1934 - val_mae: 0.1934\nEpoch 598/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7144 - mae: 0.7144\nEpoch 598: val_loss improved from 0.19339 to 0.19081, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.4000 - mae: 0.4000 - val_loss: 0.1908 - val_mae: 0.1908\nEpoch 599/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2772 - mae: 0.2772\nEpoch 599: val_loss did not improve from 0.19081\n7/7 [==============================] - 0s 7ms/step - loss: 0.3814 - mae: 0.3814 - val_loss: 0.1958 - val_mae: 0.1958\nEpoch 600/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7937 - mae: 0.7937\nEpoch 600: val_loss did not improve from 0.19081\n7/7 [==============================] - 0s 7ms/step - loss: 0.3872 - mae: 0.3872 - val_loss: 0.1942 - val_mae: 0.1942\nEpoch 601/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4707 - mae: 0.4707\nEpoch 601: val_loss did not improve from 0.19081\n7/7 [==============================] - 0s 7ms/step - loss: 0.3832 - mae: 0.3832 - val_loss: 0.1982 - val_mae: 0.1982\nEpoch 602/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7018 - mae: 0.7018\nEpoch 602: val_loss did not improve from 0.19081\n7/7 [==============================] - 0s 7ms/step - loss: 0.3871 - mae: 0.3871 - val_loss: 0.2645 - val_mae: 0.2645\nEpoch 603/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3415 - mae: 0.3415\nEpoch 603: val_loss did not improve from 0.19081\n7/7 [==============================] - 0s 7ms/step - loss: 0.4299 - mae: 0.4299 - val_loss: 0.2805 - val_mae: 0.2805\nEpoch 604/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2893 - mae: 0.2893\nEpoch 604: val_loss did not improve from 0.19081\n7/7 [==============================] - 0s 7ms/step - loss: 0.4395 - mae: 0.4395 - val_loss: 0.2045 - val_mae: 0.2045\nEpoch 605/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3290 - mae: 0.3290\nEpoch 605: val_loss did not improve from 0.19081\n7/7 [==============================] - 0s 7ms/step - loss: 0.4181 - mae: 0.4181 - val_loss: 0.2570 - val_mae: 0.2570\nEpoch 606/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2501 - mae: 0.2501\nEpoch 606: val_loss did not improve from 0.19081\n7/7 [==============================] - 0s 7ms/step - loss: 0.4401 - mae: 0.4401 - val_loss: 0.3122 - val_mae: 0.3122\nEpoch 607/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2906 - mae: 0.2906\nEpoch 607: val_loss did not improve from 0.19081\n7/7 [==============================] - 0s 7ms/step - loss: 0.4493 - mae: 0.4493 - val_loss: 0.2412 - val_mae: 0.2412\nEpoch 608/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4276 - mae: 0.4276\nEpoch 608: val_loss did not improve from 0.19081\n7/7 [==============================] - 0s 7ms/step - loss: 0.3843 - mae: 0.3843 - val_loss: 0.2237 - val_mae: 0.2237\nEpoch 609/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1726 - mae: 0.1726\nEpoch 609: val_loss did not improve from 0.19081\n7/7 [==============================] - 0s 7ms/step - loss: 0.4182 - mae: 0.4182 - val_loss: 0.1968 - val_mae: 0.1968\nEpoch 610/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3172 - mae: 0.3172\nEpoch 610: val_loss improved from 0.19081 to 0.18229, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.4116 - mae: 0.4116 - val_loss: 0.1823 - val_mae: 0.1823\nEpoch 611/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2119 - mae: 0.2119\nEpoch 611: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.3904 - mae: 0.3904 - val_loss: 0.2607 - val_mae: 0.2607\nEpoch 612/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3471 - mae: 0.3471\nEpoch 612: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 9ms/step - loss: 0.4065 - mae: 0.4065 - val_loss: 0.2053 - val_mae: 0.2053\nEpoch 613/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3090 - mae: 0.3090\nEpoch 613: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.4200 - mae: 0.4200 - val_loss: 0.2630 - val_mae: 0.2630\nEpoch 614/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2516 - mae: 0.2516\nEpoch 614: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 10ms/step - loss: 0.4179 - mae: 0.4179 - val_loss: 0.2321 - val_mae: 0.2321\nEpoch 615/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7204 - mae: 0.7204\nEpoch 615: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.3869 - mae: 0.3869 - val_loss: 0.3330 - val_mae: 0.3330\nEpoch 616/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4442 - mae: 0.4442\nEpoch 616: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.4205 - mae: 0.4205 - val_loss: 0.1961 - val_mae: 0.1961\nEpoch 617/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2062 - mae: 0.2062\nEpoch 617: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 6ms/step - loss: 0.4099 - mae: 0.4099 - val_loss: 0.1849 - val_mae: 0.1849\nEpoch 618/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1818 - mae: 0.1818\nEpoch 618: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 6ms/step - loss: 0.3738 - mae: 0.3738 - val_loss: 0.2789 - val_mae: 0.2789\nEpoch 619/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2542 - mae: 0.2542\nEpoch 619: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.4155 - mae: 0.4155 - val_loss: 0.1870 - val_mae: 0.1870\nEpoch 620/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4065 - mae: 0.4065\nEpoch 620: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.3750 - mae: 0.3750 - val_loss: 0.2328 - val_mae: 0.2328\nEpoch 621/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2333 - mae: 0.2333\nEpoch 621: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.3867 - mae: 0.3867 - val_loss: 0.2092 - val_mae: 0.2092\nEpoch 622/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1913 - mae: 0.1913\nEpoch 622: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.4470 - mae: 0.4470 - val_loss: 0.1892 - val_mae: 0.1892\nEpoch 623/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3273 - mae: 0.3273\nEpoch 623: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.4205 - mae: 0.4205 - val_loss: 0.2365 - val_mae: 0.2365\nEpoch 624/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4742 - mae: 0.4742\nEpoch 624: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.4215 - mae: 0.4215 - val_loss: 0.1847 - val_mae: 0.1847\nEpoch 625/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3883 - mae: 0.3883\nEpoch 625: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.3832 - mae: 0.3832 - val_loss: 0.1852 - val_mae: 0.1852\nEpoch 626/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2563 - mae: 0.2563\nEpoch 626: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.3998 - mae: 0.3998 - val_loss: 0.2776 - val_mae: 0.2776\nEpoch 627/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5269 - mae: 0.5269\nEpoch 627: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.4074 - mae: 0.4074 - val_loss: 0.2330 - val_mae: 0.2330\nEpoch 628/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4735 - mae: 0.4735\nEpoch 628: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.3789 - mae: 0.3789 - val_loss: 0.2256 - val_mae: 0.2256\nEpoch 629/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2006 - mae: 0.2006\nEpoch 629: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.4008 - mae: 0.4008 - val_loss: 0.2683 - val_mae: 0.2683\nEpoch 630/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6097 - mae: 0.6097\nEpoch 630: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.3968 - mae: 0.3968 - val_loss: 0.1828 - val_mae: 0.1828\nEpoch 631/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4191 - mae: 0.4191\nEpoch 631: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.4029 - mae: 0.4029 - val_loss: 0.2348 - val_mae: 0.2348\nEpoch 632/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2075 - mae: 0.2075\nEpoch 632: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.4114 - mae: 0.4114 - val_loss: 0.2126 - val_mae: 0.2126\nEpoch 633/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2220 - mae: 0.2220\nEpoch 633: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.3889 - mae: 0.3889 - val_loss: 0.1846 - val_mae: 0.1846\nEpoch 634/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7119 - mae: 0.7119\nEpoch 634: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.3720 - mae: 0.3720 - val_loss: 0.1971 - val_mae: 0.1971\nEpoch 635/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3540 - mae: 0.3540\nEpoch 635: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.3843 - mae: 0.3843 - val_loss: 0.2736 - val_mae: 0.2736\nEpoch 636/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4836 - mae: 0.4836\nEpoch 636: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.3926 - mae: 0.3926 - val_loss: 0.1910 - val_mae: 0.1910\nEpoch 637/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3054 - mae: 0.3054\nEpoch 637: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.3863 - mae: 0.3863 - val_loss: 0.2513 - val_mae: 0.2513\nEpoch 638/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2144 - mae: 0.2144\nEpoch 638: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.3922 - mae: 0.3922 - val_loss: 0.1867 - val_mae: 0.1867\nEpoch 639/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9430 - mae: 0.9430\nEpoch 639: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 10ms/step - loss: 0.3750 - mae: 0.3750 - val_loss: 0.1884 - val_mae: 0.1884\nEpoch 640/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4420 - mae: 0.4420\nEpoch 640: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 9ms/step - loss: 0.4064 - mae: 0.4064 - val_loss: 0.1995 - val_mae: 0.1995\nEpoch 641/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5002 - mae: 0.5002\nEpoch 641: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.4087 - mae: 0.4087 - val_loss: 0.1938 - val_mae: 0.1938\nEpoch 642/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9618 - mae: 0.9618\nEpoch 642: val_loss did not improve from 0.18229\n7/7 [==============================] - 0s 7ms/step - loss: 0.3899 - mae: 0.3899 - val_loss: 0.1943 - val_mae: 0.1943\nEpoch 643/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4004 - mae: 0.4004\nEpoch 643: val_loss improved from 0.18229 to 0.17169, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.3948 - mae: 0.3948 - val_loss: 0.1717 - val_mae: 0.1717\nEpoch 644/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2266 - mae: 0.2266\nEpoch 644: val_loss did not improve from 0.17169\n7/7 [==============================] - 0s 7ms/step - loss: 0.3671 - mae: 0.3671 - val_loss: 0.1817 - val_mae: 0.1817\nEpoch 645/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1283 - mae: 0.1283\nEpoch 645: val_loss did not improve from 0.17169\n7/7 [==============================] - 0s 7ms/step - loss: 0.3732 - mae: 0.3732 - val_loss: 0.2954 - val_mae: 0.2954\nEpoch 646/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3038 - mae: 0.3038\nEpoch 646: val_loss did not improve from 0.17169\n7/7 [==============================] - 0s 7ms/step - loss: 0.4047 - mae: 0.4047 - val_loss: 0.1825 - val_mae: 0.1825\nEpoch 647/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1932 - mae: 0.1932\nEpoch 647: val_loss did not improve from 0.17169\n7/7 [==============================] - 0s 7ms/step - loss: 0.3875 - mae: 0.3875 - val_loss: 0.1813 - val_mae: 0.1813\nEpoch 648/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2223 - mae: 0.2223\nEpoch 648: val_loss improved from 0.17169 to 0.17075, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.3909 - mae: 0.3909 - val_loss: 0.1708 - val_mae: 0.1708\nEpoch 649/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6996 - mae: 0.6996\nEpoch 649: val_loss did not improve from 0.17075\n7/7 [==============================] - 0s 7ms/step - loss: 0.3788 - mae: 0.3788 - val_loss: 0.1858 - val_mae: 0.1858\nEpoch 650/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1746 - mae: 0.1746\nEpoch 650: val_loss did not improve from 0.17075\n7/7 [==============================] - 0s 7ms/step - loss: 0.3946 - mae: 0.3946 - val_loss: 0.1742 - val_mae: 0.1742\nEpoch 651/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2687 - mae: 0.2687\nEpoch 651: val_loss did not improve from 0.17075\n7/7 [==============================] - 0s 7ms/step - loss: 0.3667 - mae: 0.3667 - val_loss: 0.2929 - val_mae: 0.2929\nEpoch 652/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3754 - mae: 0.3754\nEpoch 652: val_loss improved from 0.17075 to 0.16806, saving model to best_model.h5\n7/7 [==============================] - 0s 13ms/step - loss: 0.4130 - mae: 0.4130 - val_loss: 0.1681 - val_mae: 0.1681\nEpoch 653/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2885 - mae: 0.2885\nEpoch 653: val_loss improved from 0.16806 to 0.16803, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.3630 - mae: 0.3630 - val_loss: 0.1680 - val_mae: 0.1680\nEpoch 654/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2019 - mae: 0.2019\nEpoch 654: val_loss did not improve from 0.16803\n7/7 [==============================] - 0s 7ms/step - loss: 0.3729 - mae: 0.3729 - val_loss: 0.1791 - val_mae: 0.1791\nEpoch 655/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7018 - mae: 0.7018\nEpoch 655: val_loss did not improve from 0.16803\n7/7 [==============================] - 0s 7ms/step - loss: 0.3621 - mae: 0.3621 - val_loss: 0.2087 - val_mae: 0.2087\nEpoch 656/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1862 - mae: 0.1862\nEpoch 656: val_loss did not improve from 0.16803\n7/7 [==============================] - 0s 7ms/step - loss: 0.3733 - mae: 0.3733 - val_loss: 0.2662 - val_mae: 0.2662\nEpoch 657/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5996 - mae: 0.5996\nEpoch 657: val_loss did not improve from 0.16803\n7/7 [==============================] - 0s 7ms/step - loss: 0.4316 - mae: 0.4316 - val_loss: 0.2112 - val_mae: 0.2112\nEpoch 658/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3188 - mae: 0.3188\nEpoch 658: val_loss did not improve from 0.16803\n7/7 [==============================] - 0s 7ms/step - loss: 0.4511 - mae: 0.4511 - val_loss: 0.1773 - val_mae: 0.1773\nEpoch 659/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3162 - mae: 0.3162\nEpoch 659: val_loss did not improve from 0.16803\n7/7 [==============================] - 0s 7ms/step - loss: 0.4247 - mae: 0.4247 - val_loss: 0.1934 - val_mae: 0.1934\nEpoch 660/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7208 - mae: 0.7208\nEpoch 660: val_loss did not improve from 0.16803\n7/7 [==============================] - 0s 7ms/step - loss: 0.3833 - mae: 0.3833 - val_loss: 0.1849 - val_mae: 0.1849\nEpoch 661/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3941 - mae: 0.3941\nEpoch 661: val_loss did not improve from 0.16803\n7/7 [==============================] - 0s 7ms/step - loss: 0.3604 - mae: 0.3604 - val_loss: 0.1923 - val_mae: 0.1923\nEpoch 662/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6439 - mae: 0.6439\nEpoch 662: val_loss did not improve from 0.16803\n7/7 [==============================] - 0s 7ms/step - loss: 0.3763 - mae: 0.3763 - val_loss: 0.1844 - val_mae: 0.1844\nEpoch 663/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3108 - mae: 0.3108\nEpoch 663: val_loss did not improve from 0.16803\n7/7 [==============================] - 0s 7ms/step - loss: 0.3931 - mae: 0.3931 - val_loss: 0.2916 - val_mae: 0.2916\nEpoch 664/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3896 - mae: 0.3896\nEpoch 664: val_loss did not improve from 0.16803\n7/7 [==============================] - 0s 7ms/step - loss: 0.4141 - mae: 0.4141 - val_loss: 0.1925 - val_mae: 0.1925\nEpoch 665/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1827 - mae: 0.1827\nEpoch 665: val_loss did not improve from 0.16803\n7/7 [==============================] - 0s 7ms/step - loss: 0.4431 - mae: 0.4431 - val_loss: 0.2241 - val_mae: 0.2241\nEpoch 666/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4549 - mae: 0.4549\nEpoch 666: val_loss did not improve from 0.16803\n7/7 [==============================] - 0s 7ms/step - loss: 0.4278 - mae: 0.4278 - val_loss: 0.2089 - val_mae: 0.2089\nEpoch 667/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3978 - mae: 0.3978\nEpoch 667: val_loss improved from 0.16803 to 0.16319, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.3711 - mae: 0.3711 - val_loss: 0.1632 - val_mae: 0.1632\nEpoch 668/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9163 - mae: 0.9163\nEpoch 668: val_loss improved from 0.16319 to 0.15981, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.3536 - mae: 0.3536 - val_loss: 0.1598 - val_mae: 0.1598\nEpoch 669/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1682 - mae: 0.1682\nEpoch 669: val_loss did not improve from 0.15981\n7/7 [==============================] - 0s 7ms/step - loss: 0.3494 - mae: 0.3494 - val_loss: 0.1650 - val_mae: 0.1650\nEpoch 670/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3489 - mae: 0.3489\nEpoch 670: val_loss did not improve from 0.15981\n7/7 [==============================] - 0s 7ms/step - loss: 0.3576 - mae: 0.3576 - val_loss: 0.1631 - val_mae: 0.1631\nEpoch 671/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4713 - mae: 0.4713\nEpoch 671: val_loss did not improve from 0.15981\n7/7 [==============================] - 0s 7ms/step - loss: 0.3574 - mae: 0.3574 - val_loss: 0.2216 - val_mae: 0.2216\nEpoch 672/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5110 - mae: 0.5110\nEpoch 672: val_loss did not improve from 0.15981\n7/7 [==============================] - 0s 7ms/step - loss: 0.3979 - mae: 0.3979 - val_loss: 0.1628 - val_mae: 0.1628\nEpoch 673/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8087 - mae: 0.8087\nEpoch 673: val_loss did not improve from 0.15981\n7/7 [==============================] - 0s 7ms/step - loss: 0.3749 - mae: 0.3749 - val_loss: 0.1811 - val_mae: 0.1811\nEpoch 674/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1497 - mae: 0.1497\nEpoch 674: val_loss did not improve from 0.15981\n7/7 [==============================] - 0s 7ms/step - loss: 0.3504 - mae: 0.3504 - val_loss: 0.2309 - val_mae: 0.2309\nEpoch 675/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2237 - mae: 0.2237\nEpoch 675: val_loss improved from 0.15981 to 0.15658, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.3674 - mae: 0.3674 - val_loss: 0.1566 - val_mae: 0.1566\nEpoch 676/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7886 - mae: 0.7886\nEpoch 676: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.3662 - mae: 0.3662 - val_loss: 0.1694 - val_mae: 0.1694\nEpoch 677/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1126 - mae: 0.1126\nEpoch 677: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 10ms/step - loss: 0.3512 - mae: 0.3512 - val_loss: 0.2024 - val_mae: 0.2024\nEpoch 678/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1833 - mae: 0.1833\nEpoch 678: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.3531 - mae: 0.3531 - val_loss: 0.1925 - val_mae: 0.1925\nEpoch 679/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3546 - mae: 0.3546\nEpoch 679: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.3782 - mae: 0.3782 - val_loss: 0.1663 - val_mae: 0.1663\nEpoch 680/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1762 - mae: 0.1762\nEpoch 680: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.3512 - mae: 0.3512 - val_loss: 0.1566 - val_mae: 0.1566\nEpoch 681/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3021 - mae: 0.3021\nEpoch 681: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.3543 - mae: 0.3543 - val_loss: 0.2495 - val_mae: 0.2495\nEpoch 682/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1812 - mae: 0.1812\nEpoch 682: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.3661 - mae: 0.3661 - val_loss: 0.2435 - val_mae: 0.2435\nEpoch 683/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4962 - mae: 0.4962\nEpoch 683: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.3654 - mae: 0.3654 - val_loss: 0.1567 - val_mae: 0.1567\nEpoch 684/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7416 - mae: 0.7416\nEpoch 684: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.3509 - mae: 0.3509 - val_loss: 0.1949 - val_mae: 0.1949\nEpoch 685/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2354 - mae: 0.2354\nEpoch 685: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.4005 - mae: 0.4005 - val_loss: 0.1588 - val_mae: 0.1588\nEpoch 686/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1735 - mae: 0.1735\nEpoch 686: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.3467 - mae: 0.3467 - val_loss: 0.2880 - val_mae: 0.2880\nEpoch 687/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2728 - mae: 0.2728\nEpoch 687: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.4082 - mae: 0.4082 - val_loss: 0.2059 - val_mae: 0.2059\nEpoch 688/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3986 - mae: 0.3986\nEpoch 688: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.3573 - mae: 0.3573 - val_loss: 0.1569 - val_mae: 0.1569\nEpoch 689/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2937 - mae: 0.2937\nEpoch 689: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.3453 - mae: 0.3453 - val_loss: 0.2167 - val_mae: 0.2167\nEpoch 690/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6918 - mae: 0.6918\nEpoch 690: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.3792 - mae: 0.3792 - val_loss: 0.1684 - val_mae: 0.1684\nEpoch 691/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8849 - mae: 0.8849\nEpoch 691: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.3614 - mae: 0.3614 - val_loss: 0.2592 - val_mae: 0.2592\nEpoch 692/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3361 - mae: 0.3361\nEpoch 692: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.4131 - mae: 0.4131 - val_loss: 0.1572 - val_mae: 0.1572\nEpoch 693/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2780 - mae: 0.2780\nEpoch 693: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.3547 - mae: 0.3547 - val_loss: 0.2313 - val_mae: 0.2313\nEpoch 694/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2102 - mae: 0.2102\nEpoch 694: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.3751 - mae: 0.3751 - val_loss: 0.2428 - val_mae: 0.2428\nEpoch 695/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4757 - mae: 0.4757\nEpoch 695: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.3585 - mae: 0.3585 - val_loss: 0.1894 - val_mae: 0.1894\nEpoch 696/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2190 - mae: 0.2190\nEpoch 696: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.4041 - mae: 0.4041 - val_loss: 0.4043 - val_mae: 0.4043\nEpoch 697/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0187 - mae: 1.0187\nEpoch 697: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.4678 - mae: 0.4678 - val_loss: 0.2839 - val_mae: 0.2839\nEpoch 698/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3205 - mae: 0.3205\nEpoch 698: val_loss did not improve from 0.15658\n7/7 [==============================] - 0s 7ms/step - loss: 0.4325 - mae: 0.4325 - val_loss: 0.2381 - val_mae: 0.2381\nEpoch 699/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5488 - mae: 0.5488\nEpoch 699: val_loss improved from 0.15658 to 0.15048, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.4007 - mae: 0.4007 - val_loss: 0.1505 - val_mae: 0.1505\nEpoch 700/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2513 - mae: 0.2513\nEpoch 700: val_loss did not improve from 0.15048\n7/7 [==============================] - 0s 7ms/step - loss: 0.3509 - mae: 0.3509 - val_loss: 0.2471 - val_mae: 0.2471\nEpoch 701/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1822 - mae: 0.1822\nEpoch 701: val_loss did not improve from 0.15048\n7/7 [==============================] - 0s 7ms/step - loss: 0.3608 - mae: 0.3608 - val_loss: 0.2219 - val_mae: 0.2219\nEpoch 702/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1670 - mae: 0.1670\nEpoch 702: val_loss did not improve from 0.15048\n7/7 [==============================] - 0s 7ms/step - loss: 0.3413 - mae: 0.3413 - val_loss: 0.1641 - val_mae: 0.1641\nEpoch 703/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4704 - mae: 0.4704\nEpoch 703: val_loss did not improve from 0.15048\n7/7 [==============================] - 0s 7ms/step - loss: 0.3503 - mae: 0.3503 - val_loss: 0.1616 - val_mae: 0.1616\nEpoch 704/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1550 - mae: 0.1550\nEpoch 704: val_loss did not improve from 0.15048\n7/7 [==============================] - 0s 7ms/step - loss: 0.3367 - mae: 0.3367 - val_loss: 0.1642 - val_mae: 0.1642\nEpoch 705/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2922 - mae: 0.2922\nEpoch 705: val_loss improved from 0.15048 to 0.14266, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.3651 - mae: 0.3651 - val_loss: 0.1427 - val_mae: 0.1427\nEpoch 706/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2004 - mae: 0.2004\nEpoch 706: val_loss did not improve from 0.14266\n7/7 [==============================] - 0s 7ms/step - loss: 0.3505 - mae: 0.3505 - val_loss: 0.1776 - val_mae: 0.1776\nEpoch 707/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2770 - mae: 0.2770\nEpoch 707: val_loss did not improve from 0.14266\n7/7 [==============================] - 0s 7ms/step - loss: 0.3477 - mae: 0.3477 - val_loss: 0.1847 - val_mae: 0.1847\nEpoch 708/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0136 - mae: 1.0136\nEpoch 708: val_loss did not improve from 0.14266\n7/7 [==============================] - 0s 7ms/step - loss: 0.3493 - mae: 0.3493 - val_loss: 0.1700 - val_mae: 0.1700\nEpoch 709/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1965 - mae: 0.1965\nEpoch 709: val_loss did not improve from 0.14266\n7/7 [==============================] - 0s 7ms/step - loss: 0.4208 - mae: 0.4208 - val_loss: 0.1640 - val_mae: 0.1640\nEpoch 710/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0275 - mae: 1.0275\nEpoch 710: val_loss did not improve from 0.14266\n7/7 [==============================] - 0s 6ms/step - loss: 0.3983 - mae: 0.3983 - val_loss: 0.2006 - val_mae: 0.2006\nEpoch 711/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5302 - mae: 0.5302\nEpoch 711: val_loss did not improve from 0.14266\n7/7 [==============================] - 0s 7ms/step - loss: 0.3670 - mae: 0.3670 - val_loss: 0.2106 - val_mae: 0.2106\nEpoch 712/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2906 - mae: 0.2906\nEpoch 712: val_loss did not improve from 0.14266\n7/7 [==============================] - 0s 7ms/step - loss: 0.3680 - mae: 0.3680 - val_loss: 0.1615 - val_mae: 0.1615\nEpoch 713/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1114 - mae: 0.1114\nEpoch 713: val_loss did not improve from 0.14266\n7/7 [==============================] - 0s 7ms/step - loss: 0.3511 - mae: 0.3511 - val_loss: 0.1517 - val_mae: 0.1517\nEpoch 714/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3067 - mae: 0.3067\nEpoch 714: val_loss did not improve from 0.14266\n7/7 [==============================] - 0s 7ms/step - loss: 0.3754 - mae: 0.3754 - val_loss: 0.2473 - val_mae: 0.2473\nEpoch 715/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5697 - mae: 0.5697\nEpoch 715: val_loss did not improve from 0.14266\n7/7 [==============================] - 0s 7ms/step - loss: 0.4005 - mae: 0.4005 - val_loss: 0.1574 - val_mae: 0.1574\nEpoch 716/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2318 - mae: 0.2318\nEpoch 716: val_loss improved from 0.14266 to 0.13931, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.3881 - mae: 0.3881 - val_loss: 0.1393 - val_mae: 0.1393\nEpoch 717/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1964 - mae: 0.1964\nEpoch 717: val_loss did not improve from 0.13931\n7/7 [==============================] - 0s 9ms/step - loss: 0.3622 - mae: 0.3622 - val_loss: 0.1947 - val_mae: 0.1947\nEpoch 718/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2985 - mae: 0.2985\nEpoch 718: val_loss improved from 0.13931 to 0.13616, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 0.3500 - mae: 0.3500 - val_loss: 0.1362 - val_mae: 0.1362\nEpoch 719/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1515 - mae: 0.1515\nEpoch 719: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3464 - mae: 0.3464 - val_loss: 0.2307 - val_mae: 0.2307\nEpoch 720/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2228 - mae: 0.2228\nEpoch 720: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3883 - mae: 0.3883 - val_loss: 0.2739 - val_mae: 0.2739\nEpoch 721/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2694 - mae: 0.2694\nEpoch 721: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3585 - mae: 0.3585 - val_loss: 0.1584 - val_mae: 0.1584\nEpoch 722/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4877 - mae: 0.4877\nEpoch 722: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3605 - mae: 0.3605 - val_loss: 0.1499 - val_mae: 0.1499\nEpoch 723/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3765 - mae: 0.3765\nEpoch 723: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3312 - mae: 0.3312 - val_loss: 0.1467 - val_mae: 0.1467\nEpoch 724/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3005 - mae: 0.3005\nEpoch 724: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3574 - mae: 0.3574 - val_loss: 0.1477 - val_mae: 0.1477\nEpoch 725/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3520 - mae: 0.3520\nEpoch 725: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3418 - mae: 0.3418 - val_loss: 0.2088 - val_mae: 0.2088\nEpoch 726/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3679 - mae: 0.3679\nEpoch 726: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3346 - mae: 0.3346 - val_loss: 0.2224 - val_mae: 0.2224\nEpoch 727/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2356 - mae: 0.2356\nEpoch 727: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3498 - mae: 0.3498 - val_loss: 0.1626 - val_mae: 0.1626\nEpoch 728/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1593 - mae: 0.1593\nEpoch 728: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3389 - mae: 0.3389 - val_loss: 0.1425 - val_mae: 0.1425\nEpoch 729/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5373 - mae: 0.5373\nEpoch 729: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 8ms/step - loss: 0.3320 - mae: 0.3320 - val_loss: 0.1933 - val_mae: 0.1933\nEpoch 730/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4515 - mae: 0.4515\nEpoch 730: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3388 - mae: 0.3388 - val_loss: 0.1486 - val_mae: 0.1486\nEpoch 731/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1624 - mae: 0.1624\nEpoch 731: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3347 - mae: 0.3347 - val_loss: 0.1406 - val_mae: 0.1406\nEpoch 732/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1454 - mae: 0.1454\nEpoch 732: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3333 - mae: 0.3333 - val_loss: 0.1575 - val_mae: 0.1575\nEpoch 733/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2009 - mae: 0.2009\nEpoch 733: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3315 - mae: 0.3315 - val_loss: 0.1829 - val_mae: 0.1829\nEpoch 734/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3097 - mae: 0.3097\nEpoch 734: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3545 - mae: 0.3545 - val_loss: 0.1606 - val_mae: 0.1606\nEpoch 735/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1723 - mae: 0.1723\nEpoch 735: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3419 - mae: 0.3419 - val_loss: 0.1975 - val_mae: 0.1975\nEpoch 736/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8229 - mae: 0.8229\nEpoch 736: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3490 - mae: 0.3490 - val_loss: 0.1543 - val_mae: 0.1543\nEpoch 737/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3292 - mae: 0.3292\nEpoch 737: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3460 - mae: 0.3460 - val_loss: 0.1804 - val_mae: 0.1804\nEpoch 738/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1918 - mae: 0.1918\nEpoch 738: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3364 - mae: 0.3364 - val_loss: 0.1583 - val_mae: 0.1583\nEpoch 739/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1531 - mae: 0.1531\nEpoch 739: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3483 - mae: 0.3483 - val_loss: 0.1512 - val_mae: 0.1512\nEpoch 740/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5278 - mae: 0.5278\nEpoch 740: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3496 - mae: 0.3496 - val_loss: 0.1645 - val_mae: 0.1645\nEpoch 741/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1178 - mae: 0.1178\nEpoch 741: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3826 - mae: 0.3826 - val_loss: 0.1380 - val_mae: 0.1380\nEpoch 742/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3216 - mae: 0.3216\nEpoch 742: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3461 - mae: 0.3461 - val_loss: 0.1381 - val_mae: 0.1381\nEpoch 743/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1600 - mae: 0.1600\nEpoch 743: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3546 - mae: 0.3546 - val_loss: 0.1795 - val_mae: 0.1795\nEpoch 744/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2764 - mae: 0.2764\nEpoch 744: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3369 - mae: 0.3369 - val_loss: 0.1822 - val_mae: 0.1822\nEpoch 745/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2071 - mae: 0.2071\nEpoch 745: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3398 - mae: 0.3398 - val_loss: 0.1437 - val_mae: 0.1437\nEpoch 746/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1640 - mae: 0.1640\nEpoch 746: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3360 - mae: 0.3360 - val_loss: 0.2746 - val_mae: 0.2746\nEpoch 747/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2391 - mae: 0.2391\nEpoch 747: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.4042 - mae: 0.4042 - val_loss: 0.3360 - val_mae: 0.3360\nEpoch 748/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5528 - mae: 0.5528\nEpoch 748: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3788 - mae: 0.3788 - val_loss: 0.1986 - val_mae: 0.1986\nEpoch 749/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3452 - mae: 0.3452\nEpoch 749: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3545 - mae: 0.3545 - val_loss: 0.1553 - val_mae: 0.1553\nEpoch 750/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4303 - mae: 0.4303\nEpoch 750: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3380 - mae: 0.3380 - val_loss: 0.2032 - val_mae: 0.2032\nEpoch 751/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1543 - mae: 0.1543\nEpoch 751: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3507 - mae: 0.3507 - val_loss: 0.1365 - val_mae: 0.1365\nEpoch 752/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1776 - mae: 0.1776\nEpoch 752: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3326 - mae: 0.3326 - val_loss: 0.1449 - val_mae: 0.1449\nEpoch 753/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4621 - mae: 0.4621\nEpoch 753: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3244 - mae: 0.3244 - val_loss: 0.1496 - val_mae: 0.1496\nEpoch 754/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1312 - mae: 0.1312\nEpoch 754: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3299 - mae: 0.3299 - val_loss: 0.1549 - val_mae: 0.1549\nEpoch 755/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2970 - mae: 0.2970\nEpoch 755: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3409 - mae: 0.3409 - val_loss: 0.1881 - val_mae: 0.1881\nEpoch 756/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3460 - mae: 0.3460\nEpoch 756: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3520 - mae: 0.3520 - val_loss: 0.1531 - val_mae: 0.1531\nEpoch 757/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7271 - mae: 0.7271\nEpoch 757: val_loss did not improve from 0.13616\n7/7 [==============================] - 0s 7ms/step - loss: 0.3376 - mae: 0.3376 - val_loss: 0.1435 - val_mae: 0.1435\nEpoch 758/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1684 - mae: 0.1684\nEpoch 758: val_loss improved from 0.13616 to 0.12695, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 0.3917 - mae: 0.3917 - val_loss: 0.1270 - val_mae: 0.1270\nEpoch 759/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7096 - mae: 0.7096\nEpoch 759: val_loss did not improve from 0.12695\n7/7 [==============================] - 0s 7ms/step - loss: 0.3369 - mae: 0.3369 - val_loss: 0.1613 - val_mae: 0.1613\nEpoch 760/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1449 - mae: 0.1449\nEpoch 760: val_loss did not improve from 0.12695\n7/7 [==============================] - 0s 7ms/step - loss: 0.3629 - mae: 0.3629 - val_loss: 0.1309 - val_mae: 0.1309\nEpoch 761/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2780 - mae: 0.2780\nEpoch 761: val_loss did not improve from 0.12695\n7/7 [==============================] - 0s 7ms/step - loss: 0.3246 - mae: 0.3246 - val_loss: 0.1742 - val_mae: 0.1742\nEpoch 762/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4281 - mae: 0.4281\nEpoch 762: val_loss did not improve from 0.12695\n7/7 [==============================] - 0s 7ms/step - loss: 0.3232 - mae: 0.3232 - val_loss: 0.1677 - val_mae: 0.1677\nEpoch 763/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9369 - mae: 0.9369\nEpoch 763: val_loss did not improve from 0.12695\n7/7 [==============================] - 0s 7ms/step - loss: 0.3662 - mae: 0.3662 - val_loss: 0.1692 - val_mae: 0.1692\nEpoch 764/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3545 - mae: 0.3545\nEpoch 764: val_loss did not improve from 0.12695\n7/7 [==============================] - 0s 7ms/step - loss: 0.3566 - mae: 0.3566 - val_loss: 0.2677 - val_mae: 0.2677\nEpoch 765/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3597 - mae: 0.3597\nEpoch 765: val_loss did not improve from 0.12695\n7/7 [==============================] - 0s 7ms/step - loss: 0.3687 - mae: 0.3687 - val_loss: 0.1966 - val_mae: 0.1966\nEpoch 766/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6902 - mae: 0.6902\nEpoch 766: val_loss did not improve from 0.12695\n7/7 [==============================] - 0s 7ms/step - loss: 0.3312 - mae: 0.3312 - val_loss: 0.1876 - val_mae: 0.1876\nEpoch 767/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2022 - mae: 0.2022\nEpoch 767: val_loss did not improve from 0.12695\n7/7 [==============================] - 0s 7ms/step - loss: 0.3782 - mae: 0.3782 - val_loss: 0.1314 - val_mae: 0.1314\nEpoch 768/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1936 - mae: 0.1936\nEpoch 768: val_loss improved from 0.12695 to 0.12617, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 0.3201 - mae: 0.3201 - val_loss: 0.1262 - val_mae: 0.1262\nEpoch 769/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1223 - mae: 0.1223\nEpoch 769: val_loss did not improve from 0.12617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3379 - mae: 0.3379 - val_loss: 0.1452 - val_mae: 0.1452\nEpoch 770/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3741 - mae: 0.3741\nEpoch 770: val_loss did not improve from 0.12617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3582 - mae: 0.3582 - val_loss: 0.1366 - val_mae: 0.1366\nEpoch 771/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1195 - mae: 0.1195\nEpoch 771: val_loss did not improve from 0.12617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3278 - mae: 0.3278 - val_loss: 0.2028 - val_mae: 0.2028\nEpoch 772/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2782 - mae: 0.2782\nEpoch 772: val_loss did not improve from 0.12617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3648 - mae: 0.3648 - val_loss: 0.1440 - val_mae: 0.1440\nEpoch 773/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4584 - mae: 0.4584\nEpoch 773: val_loss did not improve from 0.12617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3613 - mae: 0.3613 - val_loss: 0.2405 - val_mae: 0.2405\nEpoch 774/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2317 - mae: 0.2317\nEpoch 774: val_loss did not improve from 0.12617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3622 - mae: 0.3622 - val_loss: 0.1337 - val_mae: 0.1337\nEpoch 775/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8723 - mae: 0.8723\nEpoch 775: val_loss improved from 0.12617 to 0.12489, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 0.3444 - mae: 0.3444 - val_loss: 0.1249 - val_mae: 0.1249\nEpoch 776/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8206 - mae: 0.8206\nEpoch 776: val_loss did not improve from 0.12489\n7/7 [==============================] - 0s 7ms/step - loss: 0.3495 - mae: 0.3495 - val_loss: 0.2019 - val_mae: 0.2019\nEpoch 777/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1984 - mae: 0.1984\nEpoch 777: val_loss did not improve from 0.12489\n7/7 [==============================] - 0s 7ms/step - loss: 0.3306 - mae: 0.3306 - val_loss: 0.1262 - val_mae: 0.1262\nEpoch 778/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2643 - mae: 0.2643\nEpoch 778: val_loss improved from 0.12489 to 0.12132, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.3233 - mae: 0.3233 - val_loss: 0.1213 - val_mae: 0.1213\nEpoch 779/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1245 - mae: 1.1245\nEpoch 779: val_loss did not improve from 0.12132\n7/7 [==============================] - 0s 7ms/step - loss: 0.3280 - mae: 0.3280 - val_loss: 0.1433 - val_mae: 0.1433\nEpoch 780/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5527 - mae: 0.5527\nEpoch 780: val_loss did not improve from 0.12132\n7/7 [==============================] - 0s 7ms/step - loss: 0.3407 - mae: 0.3407 - val_loss: 0.2083 - val_mae: 0.2083\nEpoch 781/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7450 - mae: 0.7450\nEpoch 781: val_loss did not improve from 0.12132\n7/7 [==============================] - 0s 7ms/step - loss: 0.3917 - mae: 0.3917 - val_loss: 0.2423 - val_mae: 0.2423\nEpoch 782/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5585 - mae: 0.5585\nEpoch 782: val_loss did not improve from 0.12132\n7/7 [==============================] - 0s 7ms/step - loss: 0.3769 - mae: 0.3769 - val_loss: 0.1425 - val_mae: 0.1425\nEpoch 783/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1411 - mae: 0.1411\nEpoch 783: val_loss did not improve from 0.12132\n7/7 [==============================] - 0s 7ms/step - loss: 0.3236 - mae: 0.3236 - val_loss: 0.1410 - val_mae: 0.1410\nEpoch 784/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7460 - mae: 0.7460\nEpoch 784: val_loss did not improve from 0.12132\n7/7 [==============================] - 0s 7ms/step - loss: 0.3298 - mae: 0.3298 - val_loss: 0.2034 - val_mae: 0.2034\nEpoch 785/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4566 - mae: 0.4566\nEpoch 785: val_loss did not improve from 0.12132\n7/7 [==============================] - 0s 10ms/step - loss: 0.3265 - mae: 0.3265 - val_loss: 0.1272 - val_mae: 0.1272\nEpoch 786/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1305 - mae: 0.1305\nEpoch 786: val_loss did not improve from 0.12132\n7/7 [==============================] - 0s 7ms/step - loss: 0.3197 - mae: 0.3197 - val_loss: 0.1259 - val_mae: 0.1259\nEpoch 787/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1703 - mae: 0.1703\nEpoch 787: val_loss did not improve from 0.12132\n7/7 [==============================] - 0s 7ms/step - loss: 0.3232 - mae: 0.3232 - val_loss: 0.2379 - val_mae: 0.2379\nEpoch 788/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2367 - mae: 0.2367\nEpoch 788: val_loss did not improve from 0.12132\n7/7 [==============================] - 0s 7ms/step - loss: 0.3758 - mae: 0.3758 - val_loss: 0.1396 - val_mae: 0.1396\nEpoch 789/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6623 - mae: 0.6623\nEpoch 789: val_loss did not improve from 0.12132\n7/7 [==============================] - 0s 7ms/step - loss: 0.3116 - mae: 0.3116 - val_loss: 0.1391 - val_mae: 0.1391\nEpoch 790/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2578 - mae: 0.2578\nEpoch 790: val_loss did not improve from 0.12132\n7/7 [==============================] - 0s 7ms/step - loss: 0.3169 - mae: 0.3169 - val_loss: 0.1277 - val_mae: 0.1277\nEpoch 791/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1673 - mae: 0.1673\nEpoch 791: val_loss did not improve from 0.12132\n7/7 [==============================] - 0s 7ms/step - loss: 0.3212 - mae: 0.3212 - val_loss: 0.1504 - val_mae: 0.1504\nEpoch 792/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7014 - mae: 0.7014\nEpoch 792: val_loss did not improve from 0.12132\n7/7 [==============================] - 0s 7ms/step - loss: 0.3404 - mae: 0.3404 - val_loss: 0.1779 - val_mae: 0.1779\nEpoch 793/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2947 - mae: 0.2947\nEpoch 793: val_loss did not improve from 0.12132\n7/7 [==============================] - 0s 7ms/step - loss: 0.3475 - mae: 0.3475 - val_loss: 0.1224 - val_mae: 0.1224\nEpoch 794/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0051 - mae: 1.0051\nEpoch 794: val_loss did not improve from 0.12132\n7/7 [==============================] - 0s 7ms/step - loss: 0.3453 - mae: 0.3453 - val_loss: 0.1852 - val_mae: 0.1852\nEpoch 795/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1805 - mae: 0.1805\nEpoch 795: val_loss improved from 0.12132 to 0.11823, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.3412 - mae: 0.3412 - val_loss: 0.1182 - val_mae: 0.1182\nEpoch 796/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2561 - mae: 0.2561\nEpoch 796: val_loss did not improve from 0.11823\n7/7 [==============================] - 0s 7ms/step - loss: 0.3355 - mae: 0.3355 - val_loss: 0.1296 - val_mae: 0.1296\nEpoch 797/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1494 - mae: 0.1494\nEpoch 797: val_loss did not improve from 0.11823\n7/7 [==============================] - 0s 7ms/step - loss: 0.3344 - mae: 0.3344 - val_loss: 0.1707 - val_mae: 0.1707\nEpoch 798/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9535 - mae: 0.9535\nEpoch 798: val_loss did not improve from 0.11823\n7/7 [==============================] - 0s 8ms/step - loss: 0.3245 - mae: 0.3245 - val_loss: 0.1926 - val_mae: 0.1926\nEpoch 799/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3911 - mae: 0.3911\nEpoch 799: val_loss did not improve from 0.11823\n7/7 [==============================] - 0s 7ms/step - loss: 0.3589 - mae: 0.3589 - val_loss: 0.1759 - val_mae: 0.1759\nEpoch 800/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1699 - mae: 0.1699\nEpoch 800: val_loss did not improve from 0.11823\n7/7 [==============================] - 0s 7ms/step - loss: 0.3273 - mae: 0.3273 - val_loss: 0.2204 - val_mae: 0.2204\nEpoch 801/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3324 - mae: 0.3324\nEpoch 801: val_loss did not improve from 0.11823\n7/7 [==============================] - 0s 7ms/step - loss: 0.3553 - mae: 0.3553 - val_loss: 0.1386 - val_mae: 0.1386\nEpoch 802/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1757 - mae: 0.1757\nEpoch 802: val_loss did not improve from 0.11823\n7/7 [==============================] - 0s 7ms/step - loss: 0.3233 - mae: 0.3233 - val_loss: 0.1896 - val_mae: 0.1896\nEpoch 803/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1447 - mae: 0.1447\nEpoch 803: val_loss did not improve from 0.11823\n7/7 [==============================] - 0s 7ms/step - loss: 0.3377 - mae: 0.3377 - val_loss: 0.1381 - val_mae: 0.1381\nEpoch 804/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2045 - mae: 0.2045\nEpoch 804: val_loss did not improve from 0.11823\n7/7 [==============================] - 0s 7ms/step - loss: 0.3366 - mae: 0.3366 - val_loss: 0.1431 - val_mae: 0.1431\nEpoch 805/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3254 - mae: 0.3254\nEpoch 805: val_loss did not improve from 0.11823\n7/7 [==============================] - 0s 7ms/step - loss: 0.3379 - mae: 0.3379 - val_loss: 0.2042 - val_mae: 0.2042\nEpoch 806/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3749 - mae: 0.3749\nEpoch 806: val_loss did not improve from 0.11823\n7/7 [==============================] - 0s 7ms/step - loss: 0.3443 - mae: 0.3443 - val_loss: 0.1589 - val_mae: 0.1589\nEpoch 807/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1156 - mae: 0.1156\nEpoch 807: val_loss did not improve from 0.11823\n7/7 [==============================] - 0s 7ms/step - loss: 0.3315 - mae: 0.3315 - val_loss: 0.1389 - val_mae: 0.1389\nEpoch 808/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1587 - mae: 0.1587\nEpoch 808: val_loss did not improve from 0.11823\n7/7 [==============================] - 0s 7ms/step - loss: 0.3177 - mae: 0.3177 - val_loss: 0.1286 - val_mae: 0.1286\nEpoch 809/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2406 - mae: 0.2406\nEpoch 809: val_loss improved from 0.11823 to 0.11691, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.3145 - mae: 0.3145 - val_loss: 0.1169 - val_mae: 0.1169\nEpoch 810/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1542 - mae: 0.1542\nEpoch 810: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 7ms/step - loss: 0.3395 - mae: 0.3395 - val_loss: 0.1239 - val_mae: 0.1239\nEpoch 811/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3197 - mae: 0.3197\nEpoch 811: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 7ms/step - loss: 0.3553 - mae: 0.3553 - val_loss: 0.1877 - val_mae: 0.1877\nEpoch 812/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2462 - mae: 0.2462\nEpoch 812: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 7ms/step - loss: 0.3251 - mae: 0.3251 - val_loss: 0.1184 - val_mae: 0.1184\nEpoch 813/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6960 - mae: 0.6960\nEpoch 813: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 6ms/step - loss: 0.3172 - mae: 0.3172 - val_loss: 0.1712 - val_mae: 0.1712\nEpoch 814/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5578 - mae: 0.5578\nEpoch 814: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 7ms/step - loss: 0.3386 - mae: 0.3386 - val_loss: 0.1220 - val_mae: 0.1220\nEpoch 815/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1735 - mae: 0.1735\nEpoch 815: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 7ms/step - loss: 0.3127 - mae: 0.3127 - val_loss: 0.1184 - val_mae: 0.1184\nEpoch 816/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2822 - mae: 0.2822\nEpoch 816: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 7ms/step - loss: 0.3214 - mae: 0.3214 - val_loss: 0.1187 - val_mae: 0.1187\nEpoch 817/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1123 - mae: 0.1123\nEpoch 817: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 7ms/step - loss: 0.3168 - mae: 0.3168 - val_loss: 0.1171 - val_mae: 0.1171\nEpoch 818/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2320 - mae: 0.2320\nEpoch 818: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 7ms/step - loss: 0.3120 - mae: 0.3120 - val_loss: 0.1415 - val_mae: 0.1415\nEpoch 819/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3862 - mae: 0.3862\nEpoch 819: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 6ms/step - loss: 0.3277 - mae: 0.3277 - val_loss: 0.1667 - val_mae: 0.1667\nEpoch 820/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5380 - mae: 0.5380\nEpoch 820: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 7ms/step - loss: 0.3311 - mae: 0.3311 - val_loss: 0.2461 - val_mae: 0.2461\nEpoch 821/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4487 - mae: 0.4487\nEpoch 821: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 7ms/step - loss: 0.3930 - mae: 0.3930 - val_loss: 0.1226 - val_mae: 0.1226\nEpoch 822/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2869 - mae: 0.2869\nEpoch 822: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 7ms/step - loss: 0.3336 - mae: 0.3336 - val_loss: 0.2201 - val_mae: 0.2201\nEpoch 823/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2093 - mae: 0.2093\nEpoch 823: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 7ms/step - loss: 0.3463 - mae: 0.3463 - val_loss: 0.1384 - val_mae: 0.1384\nEpoch 824/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2233 - mae: 0.2233\nEpoch 824: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 7ms/step - loss: 0.3470 - mae: 0.3470 - val_loss: 0.1192 - val_mae: 0.1192\nEpoch 825/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2353 - mae: 0.2353\nEpoch 825: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 7ms/step - loss: 0.3283 - mae: 0.3283 - val_loss: 0.1472 - val_mae: 0.1472\nEpoch 826/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3907 - mae: 0.3907\nEpoch 826: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 8ms/step - loss: 0.3219 - mae: 0.3219 - val_loss: 0.1374 - val_mae: 0.1374\nEpoch 827/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2204 - mae: 0.2204\nEpoch 827: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 7ms/step - loss: 0.3208 - mae: 0.3208 - val_loss: 0.1526 - val_mae: 0.1526\nEpoch 828/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4239 - mae: 0.4239\nEpoch 828: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 7ms/step - loss: 0.3230 - mae: 0.3230 - val_loss: 0.1256 - val_mae: 0.1256\nEpoch 829/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6949 - mae: 0.6949\nEpoch 829: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 8ms/step - loss: 0.3162 - mae: 0.3162 - val_loss: 0.1866 - val_mae: 0.1866\nEpoch 830/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1666 - mae: 0.1666\nEpoch 830: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 7ms/step - loss: 0.3313 - mae: 0.3313 - val_loss: 0.1292 - val_mae: 0.1292\nEpoch 831/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1743 - mae: 0.1743\nEpoch 831: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 7ms/step - loss: 0.3067 - mae: 0.3067 - val_loss: 0.1462 - val_mae: 0.1462\nEpoch 832/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2287 - mae: 0.2287\nEpoch 832: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 7ms/step - loss: 0.3442 - mae: 0.3442 - val_loss: 0.1433 - val_mae: 0.1433\nEpoch 833/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2778 - mae: 0.2778\nEpoch 833: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 7ms/step - loss: 0.3201 - mae: 0.3201 - val_loss: 0.2240 - val_mae: 0.2240\nEpoch 834/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3721 - mae: 0.3721\nEpoch 834: val_loss did not improve from 0.11691\n7/7 [==============================] - 0s 7ms/step - loss: 0.3662 - mae: 0.3662 - val_loss: 0.1260 - val_mae: 0.1260\nEpoch 835/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1512 - mae: 0.1512\nEpoch 835: val_loss improved from 0.11691 to 0.10902, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.3176 - mae: 0.3176 - val_loss: 0.1090 - val_mae: 0.1090\nEpoch 836/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1311 - mae: 0.1311\nEpoch 836: val_loss did not improve from 0.10902\n7/7 [==============================] - 0s 7ms/step - loss: 0.3183 - mae: 0.3183 - val_loss: 0.1226 - val_mae: 0.1226\nEpoch 837/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1142 - mae: 0.1142\nEpoch 837: val_loss did not improve from 0.10902\n7/7 [==============================] - 0s 10ms/step - loss: 0.3129 - mae: 0.3129 - val_loss: 0.1161 - val_mae: 0.1161\nEpoch 838/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2647 - mae: 0.2647\nEpoch 838: val_loss did not improve from 0.10902\n7/7 [==============================] - 0s 7ms/step - loss: 0.3099 - mae: 0.3099 - val_loss: 0.1219 - val_mae: 0.1219\nEpoch 839/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3754 - mae: 0.3754\nEpoch 839: val_loss improved from 0.10902 to 0.10814, saving model to best_model.h5\n7/7 [==============================] - 0s 13ms/step - loss: 0.3154 - mae: 0.3154 - val_loss: 0.1081 - val_mae: 0.1081\nEpoch 840/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1354 - mae: 0.1354\nEpoch 840: val_loss did not improve from 0.10814\n7/7 [==============================] - 0s 7ms/step - loss: 0.3214 - mae: 0.3214 - val_loss: 0.2647 - val_mae: 0.2647\nEpoch 841/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2919 - mae: 0.2919\nEpoch 841: val_loss did not improve from 0.10814\n7/7 [==============================] - 0s 7ms/step - loss: 0.3567 - mae: 0.3567 - val_loss: 0.1205 - val_mae: 0.1205\nEpoch 842/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3779 - mae: 0.3779\nEpoch 842: val_loss improved from 0.10814 to 0.10800, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.3201 - mae: 0.3201 - val_loss: 0.1080 - val_mae: 0.1080\nEpoch 843/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1391 - mae: 0.1391\nEpoch 843: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3138 - mae: 0.3138 - val_loss: 0.1157 - val_mae: 0.1157\nEpoch 844/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2475 - mae: 0.2475\nEpoch 844: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3218 - mae: 0.3218 - val_loss: 0.1318 - val_mae: 0.1318\nEpoch 845/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6931 - mae: 0.6931\nEpoch 845: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3131 - mae: 0.3131 - val_loss: 0.1148 - val_mae: 0.1148\nEpoch 846/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3931 - mae: 0.3931\nEpoch 846: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3175 - mae: 0.3175 - val_loss: 0.2065 - val_mae: 0.2065\nEpoch 847/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4771 - mae: 0.4771\nEpoch 847: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3931 - mae: 0.3931 - val_loss: 0.1219 - val_mae: 0.1219\nEpoch 848/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2470 - mae: 0.2470\nEpoch 848: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.4133 - mae: 0.4133 - val_loss: 0.2218 - val_mae: 0.2218\nEpoch 849/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2889 - mae: 0.2889\nEpoch 849: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3835 - mae: 0.3835 - val_loss: 0.2551 - val_mae: 0.2551\nEpoch 850/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2107 - mae: 0.2107\nEpoch 850: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3707 - mae: 0.3707 - val_loss: 0.1216 - val_mae: 0.1216\nEpoch 851/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3193 - mae: 0.3193\nEpoch 851: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 6ms/step - loss: 0.3246 - mae: 0.3246 - val_loss: 0.1303 - val_mae: 0.1303\nEpoch 852/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1322 - mae: 0.1322\nEpoch 852: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 9ms/step - loss: 0.3294 - mae: 0.3294 - val_loss: 0.1401 - val_mae: 0.1401\nEpoch 853/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1606 - mae: 0.1606\nEpoch 853: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3100 - mae: 0.3100 - val_loss: 0.1121 - val_mae: 0.1121\nEpoch 854/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1165 - mae: 0.1165\nEpoch 854: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3178 - mae: 0.3178 - val_loss: 0.1310 - val_mae: 0.1310\nEpoch 855/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2914 - mae: 0.2914\nEpoch 855: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3260 - mae: 0.3260 - val_loss: 0.1452 - val_mae: 0.1452\nEpoch 856/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1708 - mae: 0.1708\nEpoch 856: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 6ms/step - loss: 0.3362 - mae: 0.3362 - val_loss: 0.1370 - val_mae: 0.1370\nEpoch 857/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1273 - mae: 0.1273\nEpoch 857: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3274 - mae: 0.3274 - val_loss: 0.1246 - val_mae: 0.1246\nEpoch 858/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6587 - mae: 0.6587\nEpoch 858: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3100 - mae: 0.3100 - val_loss: 0.1152 - val_mae: 0.1152\nEpoch 859/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1904 - mae: 0.1904\nEpoch 859: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3088 - mae: 0.3088 - val_loss: 0.1298 - val_mae: 0.1298\nEpoch 860/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4208 - mae: 0.4208\nEpoch 860: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3197 - mae: 0.3197 - val_loss: 0.1119 - val_mae: 0.1119\nEpoch 861/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1139 - mae: 0.1139\nEpoch 861: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3072 - mae: 0.3072 - val_loss: 0.1421 - val_mae: 0.1421\nEpoch 862/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3979 - mae: 0.3979\nEpoch 862: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3342 - mae: 0.3342 - val_loss: 0.1237 - val_mae: 0.1237\nEpoch 863/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1156 - mae: 0.1156\nEpoch 863: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3376 - mae: 0.3376 - val_loss: 0.1835 - val_mae: 0.1835\nEpoch 864/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3972 - mae: 0.3972\nEpoch 864: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3073 - mae: 0.3073 - val_loss: 0.1488 - val_mae: 0.1488\nEpoch 865/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0102 - mae: 1.0102\nEpoch 865: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3604 - mae: 0.3604 - val_loss: 0.1817 - val_mae: 0.1817\nEpoch 866/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2904 - mae: 0.2904\nEpoch 866: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3297 - mae: 0.3297 - val_loss: 0.1131 - val_mae: 0.1131\nEpoch 867/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0012 - mae: 1.0012\nEpoch 867: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3047 - mae: 0.3047 - val_loss: 0.1466 - val_mae: 0.1466\nEpoch 868/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0110 - mae: 1.0110\nEpoch 868: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3427 - mae: 0.3427 - val_loss: 0.2238 - val_mae: 0.2238\nEpoch 869/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2094 - mae: 0.2094\nEpoch 869: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3648 - mae: 0.3648 - val_loss: 0.2568 - val_mae: 0.2568\nEpoch 870/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2640 - mae: 0.2640\nEpoch 870: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3790 - mae: 0.3790 - val_loss: 0.1390 - val_mae: 0.1390\nEpoch 871/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9556 - mae: 0.9556\nEpoch 871: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3647 - mae: 0.3647 - val_loss: 0.2048 - val_mae: 0.2048\nEpoch 872/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3961 - mae: 0.3961\nEpoch 872: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3825 - mae: 0.3825 - val_loss: 0.1630 - val_mae: 0.1630\nEpoch 873/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1608 - mae: 0.1608\nEpoch 873: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3570 - mae: 0.3570 - val_loss: 0.2851 - val_mae: 0.2851\nEpoch 874/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0493 - mae: 1.0493\nEpoch 874: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.4211 - mae: 0.4211 - val_loss: 0.4919 - val_mae: 0.4919\nEpoch 875/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5840 - mae: 0.5840\nEpoch 875: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.4716 - mae: 0.4716 - val_loss: 0.2843 - val_mae: 0.2843\nEpoch 876/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2300 - mae: 0.2300\nEpoch 876: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 10ms/step - loss: 0.3581 - mae: 0.3581 - val_loss: 0.1248 - val_mae: 0.1248\nEpoch 877/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2362 - mae: 0.2362\nEpoch 877: val_loss did not improve from 0.10800\n7/7 [==============================] - 0s 7ms/step - loss: 0.3241 - mae: 0.3241 - val_loss: 0.1136 - val_mae: 0.1136\nEpoch 878/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8382 - mae: 0.8382\nEpoch 878: val_loss improved from 0.10800 to 0.10693, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.3062 - mae: 0.3062 - val_loss: 0.1069 - val_mae: 0.1069\nEpoch 879/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1116 - mae: 0.1116\nEpoch 879: val_loss did not improve from 0.10693\n7/7 [==============================] - 0s 7ms/step - loss: 0.3344 - mae: 0.3344 - val_loss: 0.1337 - val_mae: 0.1337\nEpoch 880/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1708 - mae: 0.1708\nEpoch 880: val_loss did not improve from 0.10693\n7/7 [==============================] - 0s 7ms/step - loss: 0.3214 - mae: 0.3214 - val_loss: 0.1255 - val_mae: 0.1255\nEpoch 881/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5744 - mae: 0.5744\nEpoch 881: val_loss did not improve from 0.10693\n7/7 [==============================] - 0s 7ms/step - loss: 0.3131 - mae: 0.3131 - val_loss: 0.1162 - val_mae: 0.1162\nEpoch 882/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2815 - mae: 0.2815\nEpoch 882: val_loss improved from 0.10693 to 0.10246, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.3088 - mae: 0.3088 - val_loss: 0.1025 - val_mae: 0.1025\nEpoch 883/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7154 - mae: 0.7154\nEpoch 883: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3223 - mae: 0.3223 - val_loss: 0.1107 - val_mae: 0.1107\nEpoch 884/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2388 - mae: 0.2388\nEpoch 884: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3210 - mae: 0.3210 - val_loss: 0.1467 - val_mae: 0.1467\nEpoch 885/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1226 - mae: 0.1226\nEpoch 885: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3157 - mae: 0.3157 - val_loss: 0.1104 - val_mae: 0.1104\nEpoch 886/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3272 - mae: 0.3272\nEpoch 886: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3146 - mae: 0.3146 - val_loss: 0.1286 - val_mae: 0.1286\nEpoch 887/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3058 - mae: 0.3058\nEpoch 887: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3284 - mae: 0.3284 - val_loss: 0.1552 - val_mae: 0.1552\nEpoch 888/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2748 - mae: 0.2748\nEpoch 888: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3486 - mae: 0.3486 - val_loss: 0.1503 - val_mae: 0.1503\nEpoch 889/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1252 - mae: 0.1252\nEpoch 889: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3337 - mae: 0.3337 - val_loss: 0.1265 - val_mae: 0.1265\nEpoch 890/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1447 - mae: 0.1447\nEpoch 890: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3269 - mae: 0.3269 - val_loss: 0.1800 - val_mae: 0.1800\nEpoch 891/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4111 - mae: 0.4111\nEpoch 891: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3609 - mae: 0.3609 - val_loss: 0.2126 - val_mae: 0.2126\nEpoch 892/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3216 - mae: 0.3216\nEpoch 892: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3443 - mae: 0.3443 - val_loss: 0.1361 - val_mae: 0.1361\nEpoch 893/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2178 - mae: 0.2178\nEpoch 893: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3478 - mae: 0.3478 - val_loss: 0.1222 - val_mae: 0.1222\nEpoch 894/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6523 - mae: 0.6523\nEpoch 894: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3488 - mae: 0.3488 - val_loss: 0.1676 - val_mae: 0.1676\nEpoch 895/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1648 - mae: 0.1648\nEpoch 895: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3127 - mae: 0.3127 - val_loss: 0.2171 - val_mae: 0.2171\nEpoch 896/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1495 - mae: 0.1495\nEpoch 896: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3620 - mae: 0.3620 - val_loss: 0.1489 - val_mae: 0.1489\nEpoch 897/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3789 - mae: 0.3789\nEpoch 897: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3508 - mae: 0.3508 - val_loss: 0.1165 - val_mae: 0.1165\nEpoch 898/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2309 - mae: 0.2309\nEpoch 898: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3092 - mae: 0.3092 - val_loss: 0.1641 - val_mae: 0.1641\nEpoch 899/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2782 - mae: 0.2782\nEpoch 899: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3344 - mae: 0.3344 - val_loss: 0.2482 - val_mae: 0.2482\nEpoch 900/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2096 - mae: 0.2096\nEpoch 900: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3395 - mae: 0.3395 - val_loss: 0.1126 - val_mae: 0.1126\nEpoch 901/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2706 - mae: 0.2706\nEpoch 901: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 9ms/step - loss: 0.3292 - mae: 0.3292 - val_loss: 0.1300 - val_mae: 0.1300\nEpoch 902/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1354 - mae: 0.1354\nEpoch 902: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3200 - mae: 0.3200 - val_loss: 0.1243 - val_mae: 0.1243\nEpoch 903/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6410 - mae: 0.6410\nEpoch 903: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3311 - mae: 0.3311 - val_loss: 0.2442 - val_mae: 0.2442\nEpoch 904/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2201 - mae: 0.2201\nEpoch 904: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3431 - mae: 0.3431 - val_loss: 0.1090 - val_mae: 0.1090\nEpoch 905/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1542 - mae: 0.1542\nEpoch 905: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3097 - mae: 0.3097 - val_loss: 0.1832 - val_mae: 0.1832\nEpoch 906/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4468 - mae: 0.4468\nEpoch 906: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3408 - mae: 0.3408 - val_loss: 0.1115 - val_mae: 0.1115\nEpoch 907/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4118 - mae: 0.4118\nEpoch 907: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3769 - mae: 0.3769 - val_loss: 0.1490 - val_mae: 0.1490\nEpoch 908/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1374 - mae: 0.1374\nEpoch 908: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3210 - mae: 0.3210 - val_loss: 0.1050 - val_mae: 0.1050\nEpoch 909/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1724 - mae: 0.1724\nEpoch 909: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3115 - mae: 0.3115 - val_loss: 0.1083 - val_mae: 0.1083\nEpoch 910/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4480 - mae: 0.4480\nEpoch 910: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3283 - mae: 0.3283 - val_loss: 0.2564 - val_mae: 0.2564\nEpoch 911/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1977 - mae: 0.1977\nEpoch 911: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3875 - mae: 0.3875 - val_loss: 0.2319 - val_mae: 0.2319\nEpoch 912/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2123 - mae: 0.2123\nEpoch 912: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3345 - mae: 0.3345 - val_loss: 0.1072 - val_mae: 0.1072\nEpoch 913/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9402 - mae: 0.9402\nEpoch 913: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3441 - mae: 0.3441 - val_loss: 0.1624 - val_mae: 0.1624\nEpoch 914/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4110 - mae: 0.4110\nEpoch 914: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3645 - mae: 0.3645 - val_loss: 0.1691 - val_mae: 0.1691\nEpoch 915/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1368 - mae: 0.1368\nEpoch 915: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3232 - mae: 0.3232 - val_loss: 0.1480 - val_mae: 0.1480\nEpoch 916/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7278 - mae: 0.7278\nEpoch 916: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3293 - mae: 0.3293 - val_loss: 0.1169 - val_mae: 0.1169\nEpoch 917/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6139 - mae: 0.6139\nEpoch 917: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 6ms/step - loss: 0.3100 - mae: 0.3100 - val_loss: 0.1227 - val_mae: 0.1227\nEpoch 918/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7410 - mae: 0.7410\nEpoch 918: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 9ms/step - loss: 0.3229 - mae: 0.3229 - val_loss: 0.3887 - val_mae: 0.3887\nEpoch 919/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1469 - mae: 1.1469\nEpoch 919: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 6ms/step - loss: 0.4546 - mae: 0.4546 - val_loss: 0.2284 - val_mae: 0.2284\nEpoch 920/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2224 - mae: 0.2224\nEpoch 920: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3485 - mae: 0.3485 - val_loss: 0.1537 - val_mae: 0.1537\nEpoch 921/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1376 - mae: 0.1376\nEpoch 921: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3864 - mae: 0.3864 - val_loss: 0.1193 - val_mae: 0.1193\nEpoch 922/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1055 - mae: 0.1055\nEpoch 922: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3957 - mae: 0.3957 - val_loss: 0.2288 - val_mae: 0.2288\nEpoch 923/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7505 - mae: 0.7505\nEpoch 923: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.4456 - mae: 0.4456 - val_loss: 0.2652 - val_mae: 0.2652\nEpoch 924/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2732 - mae: 0.2732\nEpoch 924: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.4345 - mae: 0.4345 - val_loss: 0.2296 - val_mae: 0.2296\nEpoch 925/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3198 - mae: 0.3198\nEpoch 925: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.4308 - mae: 0.4308 - val_loss: 0.2595 - val_mae: 0.2595\nEpoch 926/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8160 - mae: 0.8160\nEpoch 926: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.4033 - mae: 0.4033 - val_loss: 0.1762 - val_mae: 0.1762\nEpoch 927/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2250 - mae: 0.2250\nEpoch 927: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3190 - mae: 0.3190 - val_loss: 0.1063 - val_mae: 0.1063\nEpoch 928/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2996 - mae: 0.2996\nEpoch 928: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3083 - mae: 0.3083 - val_loss: 0.1508 - val_mae: 0.1508\nEpoch 929/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4303 - mae: 0.4303\nEpoch 929: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3203 - mae: 0.3203 - val_loss: 0.1693 - val_mae: 0.1693\nEpoch 930/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4120 - mae: 0.4120\nEpoch 930: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3183 - mae: 0.3183 - val_loss: 0.1225 - val_mae: 0.1225\nEpoch 931/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1802 - mae: 0.1802\nEpoch 931: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3264 - mae: 0.3264 - val_loss: 0.1166 - val_mae: 0.1166\nEpoch 932/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1193 - mae: 0.1193\nEpoch 932: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3113 - mae: 0.3113 - val_loss: 0.1625 - val_mae: 0.1625\nEpoch 933/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1605 - mae: 0.1605\nEpoch 933: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3130 - mae: 0.3130 - val_loss: 0.1347 - val_mae: 0.1347\nEpoch 934/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3703 - mae: 0.3703\nEpoch 934: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3068 - mae: 0.3068 - val_loss: 0.1741 - val_mae: 0.1741\nEpoch 935/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1507 - mae: 0.1507\nEpoch 935: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3183 - mae: 0.3183 - val_loss: 0.1051 - val_mae: 0.1051\nEpoch 936/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.0967 - mae: 0.0967\nEpoch 936: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3116 - mae: 0.3116 - val_loss: 0.1506 - val_mae: 0.1506\nEpoch 937/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1421 - mae: 0.1421\nEpoch 937: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3075 - mae: 0.3075 - val_loss: 0.1331 - val_mae: 0.1331\nEpoch 938/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1169 - mae: 0.1169\nEpoch 938: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3120 - mae: 0.3120 - val_loss: 0.1521 - val_mae: 0.1521\nEpoch 939/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3354 - mae: 0.3354\nEpoch 939: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3416 - mae: 0.3416 - val_loss: 0.1395 - val_mae: 0.1395\nEpoch 940/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1355 - mae: 0.1355\nEpoch 940: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3171 - mae: 0.3171 - val_loss: 0.1092 - val_mae: 0.1092\nEpoch 941/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6644 - mae: 0.6644\nEpoch 941: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3050 - mae: 0.3050 - val_loss: 0.1113 - val_mae: 0.1113\nEpoch 942/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6418 - mae: 0.6418\nEpoch 942: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3251 - mae: 0.3251 - val_loss: 0.1045 - val_mae: 0.1045\nEpoch 943/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1817 - mae: 0.1817\nEpoch 943: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3378 - mae: 0.3378 - val_loss: 0.1208 - val_mae: 0.1208\nEpoch 944/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1553 - mae: 0.1553\nEpoch 944: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3415 - mae: 0.3415 - val_loss: 0.2232 - val_mae: 0.2232\nEpoch 945/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1789 - mae: 0.1789\nEpoch 945: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3269 - mae: 0.3269 - val_loss: 0.1292 - val_mae: 0.1292\nEpoch 946/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1614 - mae: 0.1614\nEpoch 946: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3188 - mae: 0.3188 - val_loss: 0.1076 - val_mae: 0.1076\nEpoch 947/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2447 - mae: 0.2447\nEpoch 947: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3103 - mae: 0.3103 - val_loss: 0.1270 - val_mae: 0.1270\nEpoch 948/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1111 - mae: 0.1111\nEpoch 948: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3342 - mae: 0.3342 - val_loss: 0.1029 - val_mae: 0.1029\nEpoch 949/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1130 - mae: 0.1130\nEpoch 949: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3353 - mae: 0.3353 - val_loss: 0.2258 - val_mae: 0.2258\nEpoch 950/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1882 - mae: 0.1882\nEpoch 950: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 9ms/step - loss: 0.3274 - mae: 0.3274 - val_loss: 0.1704 - val_mae: 0.1704\nEpoch 951/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6796 - mae: 0.6796\nEpoch 951: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3160 - mae: 0.3160 - val_loss: 0.1046 - val_mae: 0.1046\nEpoch 952/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9267 - mae: 0.9267\nEpoch 952: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3161 - mae: 0.3161 - val_loss: 0.1060 - val_mae: 0.1060\nEpoch 953/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1857 - mae: 0.1857\nEpoch 953: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3045 - mae: 0.3045 - val_loss: 0.1278 - val_mae: 0.1278\nEpoch 954/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1017 - mae: 0.1017\nEpoch 954: val_loss did not improve from 0.10246\n7/7 [==============================] - 0s 7ms/step - loss: 0.3062 - mae: 0.3062 - val_loss: 0.1147 - val_mae: 0.1147\nEpoch 955/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1551 - mae: 0.1551\nEpoch 955: val_loss improved from 0.10246 to 0.10236, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.3068 - mae: 0.3068 - val_loss: 0.1024 - val_mae: 0.1024\nEpoch 956/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2872 - mae: 0.2872\nEpoch 956: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3095 - mae: 0.3095 - val_loss: 0.1925 - val_mae: 0.1925\nEpoch 957/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1736 - mae: 0.1736\nEpoch 957: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3217 - mae: 0.3217 - val_loss: 0.1108 - val_mae: 0.1108\nEpoch 958/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3929 - mae: 0.3929\nEpoch 958: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3413 - mae: 0.3413 - val_loss: 0.1547 - val_mae: 0.1547\nEpoch 959/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1719 - mae: 0.1719\nEpoch 959: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3469 - mae: 0.3469 - val_loss: 0.1065 - val_mae: 0.1065\nEpoch 960/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2808 - mae: 0.2808\nEpoch 960: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3105 - mae: 0.3105 - val_loss: 0.1501 - val_mae: 0.1501\nEpoch 961/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3548 - mae: 0.3548\nEpoch 961: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 6ms/step - loss: 0.3257 - mae: 0.3257 - val_loss: 0.1273 - val_mae: 0.1273\nEpoch 962/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4522 - mae: 0.4522\nEpoch 962: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3107 - mae: 0.3107 - val_loss: 0.1196 - val_mae: 0.1196\nEpoch 963/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6674 - mae: 0.6674\nEpoch 963: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3173 - mae: 0.3173 - val_loss: 0.1441 - val_mae: 0.1441\nEpoch 964/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1231 - mae: 0.1231\nEpoch 964: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 6ms/step - loss: 0.3438 - mae: 0.3438 - val_loss: 0.1680 - val_mae: 0.1680\nEpoch 965/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2723 - mae: 0.2723\nEpoch 965: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3358 - mae: 0.3358 - val_loss: 0.1498 - val_mae: 0.1498\nEpoch 966/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3237 - mae: 0.3237\nEpoch 966: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3295 - mae: 0.3295 - val_loss: 0.1421 - val_mae: 0.1421\nEpoch 967/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7029 - mae: 0.7029\nEpoch 967: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3212 - mae: 0.3212 - val_loss: 0.1728 - val_mae: 0.1728\nEpoch 968/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.2685 - mae: 1.2685\nEpoch 968: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3267 - mae: 0.3267 - val_loss: 0.1191 - val_mae: 0.1191\nEpoch 969/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1829 - mae: 0.1829\nEpoch 969: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3119 - mae: 0.3119 - val_loss: 0.1082 - val_mae: 0.1082\nEpoch 970/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1414 - mae: 0.1414\nEpoch 970: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3223 - mae: 0.3223 - val_loss: 0.2376 - val_mae: 0.2376\nEpoch 971/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2026 - mae: 0.2026\nEpoch 971: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3863 - mae: 0.3863 - val_loss: 0.3549 - val_mae: 0.3549\nEpoch 972/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3904 - mae: 0.3904\nEpoch 972: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.4074 - mae: 0.4074 - val_loss: 0.2458 - val_mae: 0.2458\nEpoch 973/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2079 - mae: 0.2079\nEpoch 973: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3899 - mae: 0.3899 - val_loss: 0.1609 - val_mae: 0.1609\nEpoch 974/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1279 - mae: 0.1279\nEpoch 974: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3269 - mae: 0.3269 - val_loss: 0.1430 - val_mae: 0.1430\nEpoch 975/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4463 - mae: 0.4463\nEpoch 975: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3276 - mae: 0.3276 - val_loss: 0.1205 - val_mae: 0.1205\nEpoch 976/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.0917 - mae: 0.0917\nEpoch 976: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3119 - mae: 0.3119 - val_loss: 0.1093 - val_mae: 0.1093\nEpoch 977/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1081 - mae: 0.1081\nEpoch 977: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3122 - mae: 0.3122 - val_loss: 0.1107 - val_mae: 0.1107\nEpoch 978/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1491 - mae: 0.1491\nEpoch 978: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3279 - mae: 0.3279 - val_loss: 0.2869 - val_mae: 0.2869\nEpoch 979/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7653 - mae: 0.7653\nEpoch 979: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3611 - mae: 0.3611 - val_loss: 0.2615 - val_mae: 0.2615\nEpoch 980/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4236 - mae: 0.4236\nEpoch 980: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3537 - mae: 0.3537 - val_loss: 0.1140 - val_mae: 0.1140\nEpoch 981/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1521 - mae: 0.1521\nEpoch 981: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3184 - mae: 0.3184 - val_loss: 0.1371 - val_mae: 0.1371\nEpoch 982/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1085 - mae: 0.1085\nEpoch 982: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3146 - mae: 0.3146 - val_loss: 0.1182 - val_mae: 0.1182\nEpoch 983/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.0923 - mae: 0.0923\nEpoch 983: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3246 - mae: 0.3246 - val_loss: 0.1045 - val_mae: 0.1045\nEpoch 984/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0031 - mae: 1.0031\nEpoch 984: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 6ms/step - loss: 0.3159 - mae: 0.3159 - val_loss: 0.1029 - val_mae: 0.1029\nEpoch 985/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1191 - mae: 0.1191\nEpoch 985: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3170 - mae: 0.3170 - val_loss: 0.1070 - val_mae: 0.1070\nEpoch 986/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3381 - mae: 0.3381\nEpoch 986: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 10ms/step - loss: 0.3179 - mae: 0.3179 - val_loss: 0.1977 - val_mae: 0.1977\nEpoch 987/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1704 - mae: 0.1704\nEpoch 987: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3168 - mae: 0.3168 - val_loss: 0.1203 - val_mae: 0.1203\nEpoch 988/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5310 - mae: 0.5310\nEpoch 988: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3259 - mae: 0.3259 - val_loss: 0.1197 - val_mae: 0.1197\nEpoch 989/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8173 - mae: 0.8173\nEpoch 989: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3190 - mae: 0.3190 - val_loss: 0.2242 - val_mae: 0.2242\nEpoch 990/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1849 - mae: 0.1849\nEpoch 990: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3427 - mae: 0.3427 - val_loss: 0.1385 - val_mae: 0.1385\nEpoch 991/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1610 - mae: 0.1610\nEpoch 991: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3361 - mae: 0.3361 - val_loss: 0.1087 - val_mae: 0.1087\nEpoch 992/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1458 - mae: 0.1458\nEpoch 992: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3060 - mae: 0.3060 - val_loss: 0.1440 - val_mae: 0.1440\nEpoch 993/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7496 - mae: 0.7496\nEpoch 993: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3430 - mae: 0.3430 - val_loss: 0.1348 - val_mae: 0.1348\nEpoch 994/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3929 - mae: 0.3929\nEpoch 994: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3152 - mae: 0.3152 - val_loss: 0.2450 - val_mae: 0.2450\nEpoch 995/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3035 - mae: 0.3035\nEpoch 995: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3725 - mae: 0.3725 - val_loss: 0.1266 - val_mae: 0.1266\nEpoch 996/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1451 - mae: 0.1451\nEpoch 996: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 9ms/step - loss: 0.3414 - mae: 0.3414 - val_loss: 0.1826 - val_mae: 0.1826\nEpoch 997/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1728 - mae: 0.1728\nEpoch 997: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3441 - mae: 0.3441 - val_loss: 0.1465 - val_mae: 0.1465\nEpoch 998/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1730 - mae: 0.1730\nEpoch 998: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3210 - mae: 0.3210 - val_loss: 0.1062 - val_mae: 0.1062\nEpoch 999/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1417 - mae: 0.1417\nEpoch 999: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3032 - mae: 0.3032 - val_loss: 0.1410 - val_mae: 0.1410\nEpoch 1000/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3165 - mae: 0.3165\nEpoch 1000: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3146 - mae: 0.3146 - val_loss: 0.1115 - val_mae: 0.1115\nEpoch 1001/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1583 - mae: 0.1583\nEpoch 1001: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3197 - mae: 0.3197 - val_loss: 0.1051 - val_mae: 0.1051\nEpoch 1002/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1177 - mae: 0.1177\nEpoch 1002: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3072 - mae: 0.3072 - val_loss: 0.1910 - val_mae: 0.1910\nEpoch 1003/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1810 - mae: 0.1810\nEpoch 1003: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3229 - mae: 0.3229 - val_loss: 0.1834 - val_mae: 0.1834\nEpoch 1004/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7941 - mae: 0.7941\nEpoch 1004: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3503 - mae: 0.3503 - val_loss: 0.1199 - val_mae: 0.1199\nEpoch 1005/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1721 - mae: 0.1721\nEpoch 1005: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 10ms/step - loss: 0.3117 - mae: 0.3117 - val_loss: 0.1603 - val_mae: 0.1603\nEpoch 1006/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2406 - mae: 0.2406\nEpoch 1006: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3197 - mae: 0.3197 - val_loss: 0.1105 - val_mae: 0.1105\nEpoch 1007/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.0914 - mae: 0.0914\nEpoch 1007: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3545 - mae: 0.3545 - val_loss: 0.1383 - val_mae: 0.1383\nEpoch 1008/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2019 - mae: 0.2019\nEpoch 1008: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3481 - mae: 0.3481 - val_loss: 0.1197 - val_mae: 0.1197\nEpoch 1009/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1032 - mae: 0.1032\nEpoch 1009: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3107 - mae: 0.3107 - val_loss: 0.1742 - val_mae: 0.1742\nEpoch 1010/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3115 - mae: 0.3115\nEpoch 1010: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3273 - mae: 0.3273 - val_loss: 0.1793 - val_mae: 0.1793\nEpoch 1011/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1603 - mae: 0.1603\nEpoch 1011: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3122 - mae: 0.3122 - val_loss: 0.1178 - val_mae: 0.1178\nEpoch 1012/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6640 - mae: 0.6640\nEpoch 1012: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3106 - mae: 0.3106 - val_loss: 0.2408 - val_mae: 0.2408\nEpoch 1013/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3683 - mae: 0.3683\nEpoch 1013: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3764 - mae: 0.3764 - val_loss: 0.2024 - val_mae: 0.2024\nEpoch 1014/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2042 - mae: 0.2042\nEpoch 1014: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3501 - mae: 0.3501 - val_loss: 0.2051 - val_mae: 0.2051\nEpoch 1015/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9178 - mae: 0.9178\nEpoch 1015: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3734 - mae: 0.3734 - val_loss: 0.1045 - val_mae: 0.1045\nEpoch 1016/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1140 - mae: 0.1140\nEpoch 1016: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3241 - mae: 0.3241 - val_loss: 0.2143 - val_mae: 0.2143\nEpoch 1017/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1920 - mae: 0.1920\nEpoch 1017: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 10ms/step - loss: 0.3220 - mae: 0.3220 - val_loss: 0.1618 - val_mae: 0.1618\nEpoch 1018/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1420 - mae: 0.1420\nEpoch 1018: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3499 - mae: 0.3499 - val_loss: 0.1185 - val_mae: 0.1185\nEpoch 1019/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2262 - mae: 0.2262\nEpoch 1019: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3402 - mae: 0.3402 - val_loss: 0.1042 - val_mae: 0.1042\nEpoch 1020/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1215 - mae: 0.1215\nEpoch 1020: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3343 - mae: 0.3343 - val_loss: 0.1917 - val_mae: 0.1917\nEpoch 1021/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1837 - mae: 0.1837\nEpoch 1021: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3277 - mae: 0.3277 - val_loss: 0.1080 - val_mae: 0.1080\nEpoch 1022/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7492 - mae: 0.7492\nEpoch 1022: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3122 - mae: 0.3122 - val_loss: 0.1948 - val_mae: 0.1948\nEpoch 1023/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4438 - mae: 0.4438\nEpoch 1023: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3490 - mae: 0.3490 - val_loss: 0.1262 - val_mae: 0.1262\nEpoch 1024/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4046 - mae: 0.4046\nEpoch 1024: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3060 - mae: 0.3060 - val_loss: 0.1110 - val_mae: 0.1110\nEpoch 1025/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1125 - mae: 0.1125\nEpoch 1025: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3136 - mae: 0.3136 - val_loss: 0.1117 - val_mae: 0.1117\nEpoch 1026/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1447 - mae: 0.1447\nEpoch 1026: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3165 - mae: 0.3165 - val_loss: 0.1227 - val_mae: 0.1227\nEpoch 1027/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1574 - mae: 0.1574\nEpoch 1027: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3159 - mae: 0.3159 - val_loss: 0.2058 - val_mae: 0.2058\nEpoch 1028/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1682 - mae: 0.1682\nEpoch 1028: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3414 - mae: 0.3414 - val_loss: 0.1768 - val_mae: 0.1768\nEpoch 1029/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1438 - mae: 0.1438\nEpoch 1029: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3377 - mae: 0.3377 - val_loss: 0.1938 - val_mae: 0.1938\nEpoch 1030/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2567 - mae: 0.2567\nEpoch 1030: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.4131 - mae: 0.4131 - val_loss: 0.1839 - val_mae: 0.1839\nEpoch 1031/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2030 - mae: 0.2030\nEpoch 1031: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3465 - mae: 0.3465 - val_loss: 0.1112 - val_mae: 0.1112\nEpoch 1032/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1116 - mae: 0.1116\nEpoch 1032: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 6ms/step - loss: 0.3276 - mae: 0.3276 - val_loss: 0.1044 - val_mae: 0.1044\nEpoch 1033/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2309 - mae: 0.2309\nEpoch 1033: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 6ms/step - loss: 0.3147 - mae: 0.3147 - val_loss: 0.1149 - val_mae: 0.1149\nEpoch 1034/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1512 - mae: 0.1512\nEpoch 1034: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3212 - mae: 0.3212 - val_loss: 0.1072 - val_mae: 0.1072\nEpoch 1035/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1715 - mae: 0.1715\nEpoch 1035: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3044 - mae: 0.3044 - val_loss: 0.1119 - val_mae: 0.1119\nEpoch 1036/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1112 - mae: 0.1112\nEpoch 1036: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3521 - mae: 0.3521 - val_loss: 0.2515 - val_mae: 0.2515\nEpoch 1037/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2783 - mae: 0.2783\nEpoch 1037: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3870 - mae: 0.3870 - val_loss: 0.1443 - val_mae: 0.1443\nEpoch 1038/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2065 - mae: 0.2065\nEpoch 1038: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3856 - mae: 0.3856 - val_loss: 0.1313 - val_mae: 0.1313\nEpoch 1039/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1110 - mae: 0.1110\nEpoch 1039: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3351 - mae: 0.3351 - val_loss: 0.2025 - val_mae: 0.2025\nEpoch 1040/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4308 - mae: 0.4308\nEpoch 1040: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3318 - mae: 0.3318 - val_loss: 0.2053 - val_mae: 0.2053\nEpoch 1041/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3066 - mae: 0.3066\nEpoch 1041: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3151 - mae: 0.3151 - val_loss: 0.1428 - val_mae: 0.1428\nEpoch 1042/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1331 - mae: 0.1331\nEpoch 1042: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 8ms/step - loss: 0.3182 - mae: 0.3182 - val_loss: 0.1530 - val_mae: 0.1530\nEpoch 1043/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4336 - mae: 0.4336\nEpoch 1043: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3110 - mae: 0.3110 - val_loss: 0.1944 - val_mae: 0.1944\nEpoch 1044/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1707 - mae: 0.1707\nEpoch 1044: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 9ms/step - loss: 0.3677 - mae: 0.3677 - val_loss: 0.1136 - val_mae: 0.1136\nEpoch 1045/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1264 - mae: 0.1264\nEpoch 1045: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 8ms/step - loss: 0.3493 - mae: 0.3493 - val_loss: 0.1838 - val_mae: 0.1838\nEpoch 1046/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3122 - mae: 0.3122\nEpoch 1046: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3149 - mae: 0.3149 - val_loss: 0.1076 - val_mae: 0.1076\nEpoch 1047/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2789 - mae: 0.2789\nEpoch 1047: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 8ms/step - loss: 0.3175 - mae: 0.3175 - val_loss: 0.2368 - val_mae: 0.2368\nEpoch 1048/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2340 - mae: 0.2340\nEpoch 1048: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3803 - mae: 0.3803 - val_loss: 0.3601 - val_mae: 0.3601\nEpoch 1049/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8492 - mae: 0.8492\nEpoch 1049: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3769 - mae: 0.3769 - val_loss: 0.1255 - val_mae: 0.1255\nEpoch 1050/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.0886 - mae: 0.0886\nEpoch 1050: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3526 - mae: 0.3526 - val_loss: 0.1665 - val_mae: 0.1665\nEpoch 1051/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3659 - mae: 0.3659\nEpoch 1051: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3483 - mae: 0.3483 - val_loss: 0.1366 - val_mae: 0.1366\nEpoch 1052/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3909 - mae: 0.3909\nEpoch 1052: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3201 - mae: 0.3201 - val_loss: 0.1076 - val_mae: 0.1076\nEpoch 1053/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1389 - mae: 0.1389\nEpoch 1053: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 6ms/step - loss: 0.3274 - mae: 0.3274 - val_loss: 0.1076 - val_mae: 0.1076\nEpoch 1054/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.0993 - mae: 0.0993\nEpoch 1054: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 6ms/step - loss: 0.3073 - mae: 0.3073 - val_loss: 0.1346 - val_mae: 0.1346\nEpoch 1055/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1740 - mae: 0.1740\nEpoch 1055: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3174 - mae: 0.3174 - val_loss: 0.1323 - val_mae: 0.1323\nEpoch 1056/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5680 - mae: 0.5680\nEpoch 1056: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 6ms/step - loss: 0.3507 - mae: 0.3507 - val_loss: 0.1533 - val_mae: 0.1533\nEpoch 1057/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1105 - mae: 0.1105\nEpoch 1057: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3700 - mae: 0.3700 - val_loss: 0.1907 - val_mae: 0.1907\nEpoch 1058/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8528 - mae: 0.8528\nEpoch 1058: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 6ms/step - loss: 0.3559 - mae: 0.3559 - val_loss: 0.1130 - val_mae: 0.1130\nEpoch 1059/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1221 - mae: 0.1221\nEpoch 1059: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3096 - mae: 0.3096 - val_loss: 0.1753 - val_mae: 0.1753\nEpoch 1060/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2514 - mae: 0.2514\nEpoch 1060: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3539 - mae: 0.3539 - val_loss: 0.1032 - val_mae: 0.1032\nEpoch 1061/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8142 - mae: 0.8142\nEpoch 1061: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3586 - mae: 0.3586 - val_loss: 0.2605 - val_mae: 0.2605\nEpoch 1062/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2605 - mae: 0.2605\nEpoch 1062: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3665 - mae: 0.3665 - val_loss: 0.2761 - val_mae: 0.2761\nEpoch 1063/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7965 - mae: 0.7965\nEpoch 1063: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3482 - mae: 0.3482 - val_loss: 0.1319 - val_mae: 0.1319\nEpoch 1064/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1018 - mae: 0.1018\nEpoch 1064: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3360 - mae: 0.3360 - val_loss: 0.1840 - val_mae: 0.1840\nEpoch 1065/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7950 - mae: 0.7950\nEpoch 1065: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3405 - mae: 0.3405 - val_loss: 0.1035 - val_mae: 0.1035\nEpoch 1066/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1611 - mae: 0.1611\nEpoch 1066: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 10ms/step - loss: 0.3198 - mae: 0.3198 - val_loss: 0.1306 - val_mae: 0.1306\nEpoch 1067/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1292 - mae: 0.1292\nEpoch 1067: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3013 - mae: 0.3013 - val_loss: 0.1154 - val_mae: 0.1154\nEpoch 1068/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1414 - mae: 0.1414\nEpoch 1068: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3184 - mae: 0.3184 - val_loss: 0.1044 - val_mae: 0.1044\nEpoch 1069/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1142 - mae: 0.1142\nEpoch 1069: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3398 - mae: 0.3398 - val_loss: 0.1503 - val_mae: 0.1503\nEpoch 1070/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4133 - mae: 0.4133\nEpoch 1070: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 9ms/step - loss: 0.3206 - mae: 0.3206 - val_loss: 0.1576 - val_mae: 0.1576\nEpoch 1071/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1273 - mae: 0.1273\nEpoch 1071: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3198 - mae: 0.3198 - val_loss: 0.1046 - val_mae: 0.1046\nEpoch 1072/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4220 - mae: 0.4220\nEpoch 1072: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3079 - mae: 0.3079 - val_loss: 0.1195 - val_mae: 0.1195\nEpoch 1073/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1146 - mae: 0.1146\nEpoch 1073: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3122 - mae: 0.3122 - val_loss: 0.1247 - val_mae: 0.1247\nEpoch 1074/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1122 - mae: 0.1122\nEpoch 1074: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3229 - mae: 0.3229 - val_loss: 0.1224 - val_mae: 0.1224\nEpoch 1075/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2931 - mae: 0.2931\nEpoch 1075: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3092 - mae: 0.3092 - val_loss: 0.1573 - val_mae: 0.1573\nEpoch 1076/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4260 - mae: 0.4260\nEpoch 1076: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3371 - mae: 0.3371 - val_loss: 0.2084 - val_mae: 0.2084\nEpoch 1077/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1556 - mae: 0.1556\nEpoch 1077: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3269 - mae: 0.3269 - val_loss: 0.1560 - val_mae: 0.1560\nEpoch 1078/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2981 - mae: 0.2981\nEpoch 1078: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3111 - mae: 0.3111 - val_loss: 0.1149 - val_mae: 0.1149\nEpoch 1079/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3944 - mae: 0.3944\nEpoch 1079: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3254 - mae: 0.3254 - val_loss: 0.2411 - val_mae: 0.2411\nEpoch 1080/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2886 - mae: 0.2886\nEpoch 1080: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3899 - mae: 0.3899 - val_loss: 0.1165 - val_mae: 0.1165\nEpoch 1081/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1851 - mae: 0.1851\nEpoch 1081: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3458 - mae: 0.3458 - val_loss: 0.1155 - val_mae: 0.1155\nEpoch 1082/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1526 - mae: 0.1526\nEpoch 1082: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3095 - mae: 0.3095 - val_loss: 0.1274 - val_mae: 0.1274\nEpoch 1083/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2207 - mae: 0.2207\nEpoch 1083: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3067 - mae: 0.3067 - val_loss: 0.1027 - val_mae: 0.1027\nEpoch 1084/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1443 - mae: 0.1443\nEpoch 1084: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3181 - mae: 0.3181 - val_loss: 0.1796 - val_mae: 0.1796\nEpoch 1085/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4086 - mae: 0.4086\nEpoch 1085: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3253 - mae: 0.3253 - val_loss: 0.1058 - val_mae: 0.1058\nEpoch 1086/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7170 - mae: 0.7170\nEpoch 1086: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3223 - mae: 0.3223 - val_loss: 0.1763 - val_mae: 0.1763\nEpoch 1087/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1465 - mae: 0.1465\nEpoch 1087: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3184 - mae: 0.3184 - val_loss: 0.1196 - val_mae: 0.1196\nEpoch 1088/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1227 - mae: 0.1227\nEpoch 1088: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3234 - mae: 0.3234 - val_loss: 0.1191 - val_mae: 0.1191\nEpoch 1089/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1493 - mae: 0.1493\nEpoch 1089: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 8ms/step - loss: 0.3107 - mae: 0.3107 - val_loss: 0.1043 - val_mae: 0.1043\nEpoch 1090/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2792 - mae: 0.2792\nEpoch 1090: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3300 - mae: 0.3300 - val_loss: 0.1855 - val_mae: 0.1855\nEpoch 1091/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1682 - mae: 0.1682\nEpoch 1091: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3095 - mae: 0.3095 - val_loss: 0.1190 - val_mae: 0.1190\nEpoch 1092/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3460 - mae: 0.3460\nEpoch 1092: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3270 - mae: 0.3270 - val_loss: 0.1088 - val_mae: 0.1088\nEpoch 1093/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1780 - mae: 0.1780\nEpoch 1093: val_loss did not improve from 0.10236\n7/7 [==============================] - 0s 7ms/step - loss: 0.3130 - mae: 0.3130 - val_loss: 0.1275 - val_mae: 0.1275\nEpoch 1094/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4849 - mae: 0.4849\nEpoch 1094: val_loss improved from 0.10236 to 0.10016, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.3297 - mae: 0.3297 - val_loss: 0.1002 - val_mae: 0.1002\nEpoch 1095/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2792 - mae: 0.2792\nEpoch 1095: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3175 - mae: 0.3175 - val_loss: 0.1036 - val_mae: 0.1036\nEpoch 1096/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1145 - mae: 0.1145\nEpoch 1096: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3156 - mae: 0.3156 - val_loss: 0.1808 - val_mae: 0.1808\nEpoch 1097/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6530 - mae: 0.6530\nEpoch 1097: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3627 - mae: 0.3627 - val_loss: 0.1003 - val_mae: 0.1003\nEpoch 1098/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2751 - mae: 0.2751\nEpoch 1098: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3219 - mae: 0.3219 - val_loss: 0.1523 - val_mae: 0.1523\nEpoch 1099/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1314 - mae: 0.1314\nEpoch 1099: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 9ms/step - loss: 0.3120 - mae: 0.3120 - val_loss: 0.1252 - val_mae: 0.1252\nEpoch 1100/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2589 - mae: 0.2589\nEpoch 1100: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 9ms/step - loss: 0.3404 - mae: 0.3404 - val_loss: 0.1155 - val_mae: 0.1155\nEpoch 1101/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3214 - mae: 0.3214\nEpoch 1101: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3173 - mae: 0.3173 - val_loss: 0.2046 - val_mae: 0.2046\nEpoch 1102/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2419 - mae: 0.2419\nEpoch 1102: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3332 - mae: 0.3332 - val_loss: 0.1014 - val_mae: 0.1014\nEpoch 1103/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1365 - mae: 0.1365\nEpoch 1103: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3555 - mae: 0.3555 - val_loss: 0.1837 - val_mae: 0.1837\nEpoch 1104/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4884 - mae: 0.4884\nEpoch 1104: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3291 - mae: 0.3291 - val_loss: 0.1252 - val_mae: 0.1252\nEpoch 1105/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7690 - mae: 0.7690\nEpoch 1105: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3284 - mae: 0.3284 - val_loss: 0.2055 - val_mae: 0.2055\nEpoch 1106/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1739 - mae: 0.1739\nEpoch 1106: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 10ms/step - loss: 0.3369 - mae: 0.3369 - val_loss: 0.1069 - val_mae: 0.1069\nEpoch 1107/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3299 - mae: 0.3299\nEpoch 1107: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3256 - mae: 0.3256 - val_loss: 0.1754 - val_mae: 0.1754\nEpoch 1108/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6931 - mae: 0.6931\nEpoch 1108: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3316 - mae: 0.3316 - val_loss: 0.1905 - val_mae: 0.1905\nEpoch 1109/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4199 - mae: 0.4199\nEpoch 1109: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3418 - mae: 0.3418 - val_loss: 0.1471 - val_mae: 0.1471\nEpoch 1110/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1149 - mae: 0.1149\nEpoch 1110: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3463 - mae: 0.3463 - val_loss: 0.1334 - val_mae: 0.1334\nEpoch 1111/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8086 - mae: 0.8086\nEpoch 1111: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3219 - mae: 0.3219 - val_loss: 0.1356 - val_mae: 0.1356\nEpoch 1112/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7192 - mae: 0.7192\nEpoch 1112: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3224 - mae: 0.3224 - val_loss: 0.1126 - val_mae: 0.1126\nEpoch 1113/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1459 - mae: 0.1459\nEpoch 1113: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3055 - mae: 0.3055 - val_loss: 0.1417 - val_mae: 0.1417\nEpoch 1114/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1331 - mae: 0.1331\nEpoch 1114: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3061 - mae: 0.3061 - val_loss: 0.1351 - val_mae: 0.1351\nEpoch 1115/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1065 - mae: 0.1065\nEpoch 1115: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3172 - mae: 0.3172 - val_loss: 0.1071 - val_mae: 0.1071\nEpoch 1116/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3049 - mae: 0.3049\nEpoch 1116: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3395 - mae: 0.3395 - val_loss: 0.1899 - val_mae: 0.1899\nEpoch 1117/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2688 - mae: 0.2688\nEpoch 1117: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 6ms/step - loss: 0.3881 - mae: 0.3881 - val_loss: 0.1218 - val_mae: 0.1218\nEpoch 1118/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7960 - mae: 0.7960\nEpoch 1118: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3438 - mae: 0.3438 - val_loss: 0.1191 - val_mae: 0.1191\nEpoch 1119/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1620 - mae: 0.1620\nEpoch 1119: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3067 - mae: 0.3067 - val_loss: 0.1090 - val_mae: 0.1090\nEpoch 1120/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2563 - mae: 0.2563\nEpoch 1120: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3080 - mae: 0.3080 - val_loss: 0.1185 - val_mae: 0.1185\nEpoch 1121/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1337 - mae: 0.1337\nEpoch 1121: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3042 - mae: 0.3042 - val_loss: 0.1232 - val_mae: 0.1232\nEpoch 1122/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1220 - mae: 0.1220\nEpoch 1122: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3025 - mae: 0.3025 - val_loss: 0.1500 - val_mae: 0.1500\nEpoch 1123/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3458 - mae: 0.3458\nEpoch 1123: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3329 - mae: 0.3329 - val_loss: 0.1331 - val_mae: 0.1331\nEpoch 1124/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1515 - mae: 0.1515\nEpoch 1124: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3055 - mae: 0.3055 - val_loss: 0.1591 - val_mae: 0.1591\nEpoch 1125/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1461 - mae: 0.1461\nEpoch 1125: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3374 - mae: 0.3374 - val_loss: 0.1511 - val_mae: 0.1511\nEpoch 1126/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1521 - mae: 0.1521\nEpoch 1126: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3342 - mae: 0.3342 - val_loss: 0.1037 - val_mae: 0.1037\nEpoch 1127/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2649 - mae: 0.2649\nEpoch 1127: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3274 - mae: 0.3274 - val_loss: 0.1430 - val_mae: 0.1430\nEpoch 1128/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2094 - mae: 0.2094\nEpoch 1128: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3251 - mae: 0.3251 - val_loss: 0.1655 - val_mae: 0.1655\nEpoch 1129/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2979 - mae: 0.2979\nEpoch 1129: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 10ms/step - loss: 0.3283 - mae: 0.3283 - val_loss: 0.1161 - val_mae: 0.1161\nEpoch 1130/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2021 - mae: 0.2021\nEpoch 1130: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3123 - mae: 0.3123 - val_loss: 0.1050 - val_mae: 0.1050\nEpoch 1131/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2940 - mae: 0.2940\nEpoch 1131: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3101 - mae: 0.3101 - val_loss: 0.1166 - val_mae: 0.1166\nEpoch 1132/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2740 - mae: 0.2740\nEpoch 1132: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3080 - mae: 0.3080 - val_loss: 0.1406 - val_mae: 0.1406\nEpoch 1133/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1390 - mae: 0.1390\nEpoch 1133: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3143 - mae: 0.3143 - val_loss: 0.1139 - val_mae: 0.1139\nEpoch 1134/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6764 - mae: 0.6764\nEpoch 1134: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3147 - mae: 0.3147 - val_loss: 0.1334 - val_mae: 0.1334\nEpoch 1135/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1232 - mae: 0.1232\nEpoch 1135: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3098 - mae: 0.3098 - val_loss: 0.1046 - val_mae: 0.1046\nEpoch 1136/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2330 - mae: 0.2330\nEpoch 1136: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3187 - mae: 0.3187 - val_loss: 0.1447 - val_mae: 0.1447\nEpoch 1137/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4636 - mae: 0.4636\nEpoch 1137: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3557 - mae: 0.3557 - val_loss: 0.1373 - val_mae: 0.1373\nEpoch 1138/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1433 - mae: 0.1433\nEpoch 1138: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3405 - mae: 0.3405 - val_loss: 0.1436 - val_mae: 0.1436\nEpoch 1139/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3907 - mae: 0.3907\nEpoch 1139: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3058 - mae: 0.3058 - val_loss: 0.1132 - val_mae: 0.1132\nEpoch 1140/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1811 - mae: 0.1811\nEpoch 1140: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3185 - mae: 0.3185 - val_loss: 0.1261 - val_mae: 0.1261\nEpoch 1141/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1177 - mae: 0.1177\nEpoch 1141: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3520 - mae: 0.3520 - val_loss: 0.1065 - val_mae: 0.1065\nEpoch 1142/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1638 - mae: 0.1638\nEpoch 1142: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3100 - mae: 0.3100 - val_loss: 0.1097 - val_mae: 0.1097\nEpoch 1143/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2412 - mae: 0.2412\nEpoch 1143: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 9ms/step - loss: 0.3125 - mae: 0.3125 - val_loss: 0.2363 - val_mae: 0.2363\nEpoch 1144/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1851 - mae: 0.1851\nEpoch 1144: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3395 - mae: 0.3395 - val_loss: 0.1400 - val_mae: 0.1400\nEpoch 1145/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1511 - mae: 0.1511\nEpoch 1145: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3211 - mae: 0.3211 - val_loss: 0.1404 - val_mae: 0.1404\nEpoch 1146/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4780 - mae: 0.4780\nEpoch 1146: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3335 - mae: 0.3335 - val_loss: 0.1086 - val_mae: 0.1086\nEpoch 1147/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1131 - mae: 0.1131\nEpoch 1147: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3154 - mae: 0.3154 - val_loss: 0.1842 - val_mae: 0.1842\nEpoch 1148/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8326 - mae: 0.8326\nEpoch 1148: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3269 - mae: 0.3269 - val_loss: 0.1034 - val_mae: 0.1034\nEpoch 1149/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2489 - mae: 0.2489\nEpoch 1149: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3145 - mae: 0.3145 - val_loss: 0.1056 - val_mae: 0.1056\nEpoch 1150/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1348 - mae: 0.1348\nEpoch 1150: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 10ms/step - loss: 0.3048 - mae: 0.3048 - val_loss: 0.1162 - val_mae: 0.1162\nEpoch 1151/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1406 - mae: 0.1406\nEpoch 1151: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 10ms/step - loss: 0.3070 - mae: 0.3070 - val_loss: 0.1223 - val_mae: 0.1223\nEpoch 1152/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1409 - mae: 0.1409\nEpoch 1152: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3150 - mae: 0.3150 - val_loss: 0.1453 - val_mae: 0.1453\nEpoch 1153/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2793 - mae: 0.2793\nEpoch 1153: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3144 - mae: 0.3144 - val_loss: 0.1338 - val_mae: 0.1338\nEpoch 1154/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1544 - mae: 0.1544\nEpoch 1154: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3087 - mae: 0.3087 - val_loss: 0.1189 - val_mae: 0.1189\nEpoch 1155/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7925 - mae: 0.7925\nEpoch 1155: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3166 - mae: 0.3166 - val_loss: 0.1112 - val_mae: 0.1112\nEpoch 1156/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1637 - mae: 0.1637\nEpoch 1156: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3189 - mae: 0.3189 - val_loss: 0.1195 - val_mae: 0.1195\nEpoch 1157/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2471 - mae: 0.2471\nEpoch 1157: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3208 - mae: 0.3208 - val_loss: 0.1806 - val_mae: 0.1806\nEpoch 1158/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1573 - mae: 0.1573\nEpoch 1158: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3447 - mae: 0.3447 - val_loss: 0.1098 - val_mae: 0.1098\nEpoch 1159/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2943 - mae: 0.2943\nEpoch 1159: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3416 - mae: 0.3416 - val_loss: 0.1525 - val_mae: 0.1525\nEpoch 1160/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1766 - mae: 0.1766\nEpoch 1160: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3253 - mae: 0.3253 - val_loss: 0.1580 - val_mae: 0.1580\nEpoch 1161/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1906 - mae: 0.1906\nEpoch 1161: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3360 - mae: 0.3360 - val_loss: 0.1661 - val_mae: 0.1661\nEpoch 1162/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1297 - mae: 0.1297\nEpoch 1162: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3163 - mae: 0.3163 - val_loss: 0.1089 - val_mae: 0.1089\nEpoch 1163/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2674 - mae: 0.2674\nEpoch 1163: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3186 - mae: 0.3186 - val_loss: 0.2252 - val_mae: 0.2252\nEpoch 1164/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7088 - mae: 0.7088\nEpoch 1164: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3413 - mae: 0.3413 - val_loss: 0.1579 - val_mae: 0.1579\nEpoch 1165/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2477 - mae: 0.2477\nEpoch 1165: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3410 - mae: 0.3410 - val_loss: 0.1040 - val_mae: 0.1040\nEpoch 1166/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2600 - mae: 0.2600\nEpoch 1166: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3110 - mae: 0.3110 - val_loss: 0.2360 - val_mae: 0.2360\nEpoch 1167/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1908 - mae: 0.1908\nEpoch 1167: val_loss did not improve from 0.10016\n7/7 [==============================] - 0s 7ms/step - loss: 0.3360 - mae: 0.3360 - val_loss: 0.1546 - val_mae: 0.1546\nEpoch 1168/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1239 - mae: 0.1239\nEpoch 1168: val_loss improved from 0.10016 to 0.10001, saving model to best_model.h5\n7/7 [==============================] - 0s 9ms/step - loss: 0.3449 - mae: 0.3449 - val_loss: 0.1000 - val_mae: 0.1000\nEpoch 1169/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6430 - mae: 0.6430\nEpoch 1169: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 9ms/step - loss: 0.3149 - mae: 0.3149 - val_loss: 0.1140 - val_mae: 0.1140\nEpoch 1170/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1493 - mae: 0.1493\nEpoch 1170: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 6ms/step - loss: 0.3367 - mae: 0.3367 - val_loss: 0.1020 - val_mae: 0.1020\nEpoch 1171/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1377 - mae: 0.1377\nEpoch 1171: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 9ms/step - loss: 0.3036 - mae: 0.3036 - val_loss: 0.1047 - val_mae: 0.1047\nEpoch 1172/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1008 - mae: 0.1008\nEpoch 1172: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3080 - mae: 0.3080 - val_loss: 0.1077 - val_mae: 0.1077\nEpoch 1173/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1690 - mae: 0.1690\nEpoch 1173: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3054 - mae: 0.3054 - val_loss: 0.1958 - val_mae: 0.1958\nEpoch 1174/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2905 - mae: 0.2905\nEpoch 1174: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3492 - mae: 0.3492 - val_loss: 0.1087 - val_mae: 0.1087\nEpoch 1175/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2871 - mae: 0.2871\nEpoch 1175: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 6ms/step - loss: 0.3286 - mae: 0.3286 - val_loss: 0.1328 - val_mae: 0.1328\nEpoch 1176/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1432 - mae: 0.1432\nEpoch 1176: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3297 - mae: 0.3297 - val_loss: 0.2079 - val_mae: 0.2079\nEpoch 1177/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2374 - mae: 0.2374\nEpoch 1177: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3790 - mae: 0.3790 - val_loss: 0.1304 - val_mae: 0.1304\nEpoch 1178/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1685 - mae: 0.1685\nEpoch 1178: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3586 - mae: 0.3586 - val_loss: 0.3508 - val_mae: 0.3508\nEpoch 1179/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4287 - mae: 0.4287\nEpoch 1179: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.4412 - mae: 0.4412 - val_loss: 0.3989 - val_mae: 0.3989\nEpoch 1180/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3313 - mae: 0.3313\nEpoch 1180: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.4473 - mae: 0.4473 - val_loss: 0.3497 - val_mae: 0.3497\nEpoch 1181/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.1476 - mae: 1.1476\nEpoch 1181: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.4150 - mae: 0.4150 - val_loss: 0.3130 - val_mae: 0.3130\nEpoch 1182/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2753 - mae: 0.2753\nEpoch 1182: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.4153 - mae: 0.4153 - val_loss: 0.2738 - val_mae: 0.2738\nEpoch 1183/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2337 - mae: 0.2337\nEpoch 1183: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 9ms/step - loss: 0.3754 - mae: 0.3754 - val_loss: 0.1497 - val_mae: 0.1497\nEpoch 1184/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1843 - mae: 0.1843\nEpoch 1184: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3456 - mae: 0.3456 - val_loss: 0.2256 - val_mae: 0.2256\nEpoch 1185/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5510 - mae: 0.5510\nEpoch 1185: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 8ms/step - loss: 0.3960 - mae: 0.3960 - val_loss: 0.2049 - val_mae: 0.2049\nEpoch 1186/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8435 - mae: 0.8435\nEpoch 1186: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3682 - mae: 0.3682 - val_loss: 0.1992 - val_mae: 0.1992\nEpoch 1187/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1609 - mae: 0.1609\nEpoch 1187: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3906 - mae: 0.3906 - val_loss: 0.3577 - val_mae: 0.3577\nEpoch 1188/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3025 - mae: 0.3025\nEpoch 1188: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3947 - mae: 0.3947 - val_loss: 0.2972 - val_mae: 0.2972\nEpoch 1189/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2410 - mae: 0.2410\nEpoch 1189: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3722 - mae: 0.3722 - val_loss: 0.2630 - val_mae: 0.2630\nEpoch 1190/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7058 - mae: 0.7058\nEpoch 1190: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 10ms/step - loss: 0.3297 - mae: 0.3297 - val_loss: 0.1297 - val_mae: 0.1297\nEpoch 1191/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1542 - mae: 0.1542\nEpoch 1191: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3114 - mae: 0.3114 - val_loss: 0.1887 - val_mae: 0.1887\nEpoch 1192/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1571 - mae: 0.1571\nEpoch 1192: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3188 - mae: 0.3188 - val_loss: 0.2372 - val_mae: 0.2372\nEpoch 1193/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7222 - mae: 0.7222\nEpoch 1193: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 8ms/step - loss: 0.3377 - mae: 0.3377 - val_loss: 0.1130 - val_mae: 0.1130\nEpoch 1194/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1534 - mae: 0.1534\nEpoch 1194: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3118 - mae: 0.3118 - val_loss: 0.1298 - val_mae: 0.1298\nEpoch 1195/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1562 - mae: 0.1562\nEpoch 1195: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3292 - mae: 0.3292 - val_loss: 0.1905 - val_mae: 0.1905\nEpoch 1196/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3172 - mae: 0.3172\nEpoch 1196: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3125 - mae: 0.3125 - val_loss: 0.1612 - val_mae: 0.1612\nEpoch 1197/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1578 - mae: 0.1578\nEpoch 1197: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3254 - mae: 0.3254 - val_loss: 0.1088 - val_mae: 0.1088\nEpoch 1198/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1358 - mae: 0.1358\nEpoch 1198: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3177 - mae: 0.3177 - val_loss: 0.2870 - val_mae: 0.2870\nEpoch 1199/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2506 - mae: 0.2506\nEpoch 1199: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3551 - mae: 0.3551 - val_loss: 0.1855 - val_mae: 0.1855\nEpoch 1200/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4671 - mae: 0.4671\nEpoch 1200: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3347 - mae: 0.3347 - val_loss: 0.1883 - val_mae: 0.1883\nEpoch 1201/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3768 - mae: 0.3768\nEpoch 1201: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3552 - mae: 0.3552 - val_loss: 0.1595 - val_mae: 0.1595\nEpoch 1202/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2216 - mae: 0.2216\nEpoch 1202: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3591 - mae: 0.3591 - val_loss: 0.1794 - val_mae: 0.1794\nEpoch 1203/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4689 - mae: 0.4689\nEpoch 1203: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 6ms/step - loss: 0.3499 - mae: 0.3499 - val_loss: 0.2059 - val_mae: 0.2059\nEpoch 1204/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0481 - mae: 1.0481\nEpoch 1204: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3422 - mae: 0.3422 - val_loss: 0.2289 - val_mae: 0.2289\nEpoch 1205/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2350 - mae: 0.2350\nEpoch 1205: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3126 - mae: 0.3126 - val_loss: 0.1036 - val_mae: 0.1036\nEpoch 1206/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4765 - mae: 0.4765\nEpoch 1206: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3103 - mae: 0.3103 - val_loss: 0.1220 - val_mae: 0.1220\nEpoch 1207/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1547 - mae: 0.1547\nEpoch 1207: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3286 - mae: 0.3286 - val_loss: 0.1158 - val_mae: 0.1158\nEpoch 1208/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1773 - mae: 0.1773\nEpoch 1208: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3072 - mae: 0.3072 - val_loss: 0.1050 - val_mae: 0.1050\nEpoch 1209/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3988 - mae: 0.3988\nEpoch 1209: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3059 - mae: 0.3059 - val_loss: 0.1502 - val_mae: 0.1502\nEpoch 1210/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2915 - mae: 0.2915\nEpoch 1210: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3131 - mae: 0.3131 - val_loss: 0.1956 - val_mae: 0.1956\nEpoch 1211/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2305 - mae: 0.2305\nEpoch 1211: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3152 - mae: 0.3152 - val_loss: 0.1176 - val_mae: 0.1176\nEpoch 1212/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2761 - mae: 0.2761\nEpoch 1212: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 6ms/step - loss: 0.3294 - mae: 0.3294 - val_loss: 0.1161 - val_mae: 0.1161\nEpoch 1213/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1612 - mae: 0.1612\nEpoch 1213: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3370 - mae: 0.3370 - val_loss: 0.1961 - val_mae: 0.1961\nEpoch 1214/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2952 - mae: 0.2952\nEpoch 1214: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3131 - mae: 0.3131 - val_loss: 0.1012 - val_mae: 0.1012\nEpoch 1215/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4008 - mae: 0.4008\nEpoch 1215: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3098 - mae: 0.3098 - val_loss: 0.1047 - val_mae: 0.1047\nEpoch 1216/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2628 - mae: 0.2628\nEpoch 1216: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3044 - mae: 0.3044 - val_loss: 0.1169 - val_mae: 0.1169\nEpoch 1217/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1068 - mae: 0.1068\nEpoch 1217: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3041 - mae: 0.3041 - val_loss: 0.1163 - val_mae: 0.1163\nEpoch 1218/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1598 - mae: 0.1598\nEpoch 1218: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3031 - mae: 0.3031 - val_loss: 0.1375 - val_mae: 0.1375\nEpoch 1219/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4392 - mae: 0.4392\nEpoch 1219: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3203 - mae: 0.3203 - val_loss: 0.1231 - val_mae: 0.1231\nEpoch 1220/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2890 - mae: 0.2890\nEpoch 1220: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3154 - mae: 0.3154 - val_loss: 0.1115 - val_mae: 0.1115\nEpoch 1221/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1669 - mae: 0.1669\nEpoch 1221: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3071 - mae: 0.3071 - val_loss: 0.1248 - val_mae: 0.1248\nEpoch 1222/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2356 - mae: 0.2356\nEpoch 1222: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 10ms/step - loss: 0.3060 - mae: 0.3060 - val_loss: 0.1045 - val_mae: 0.1045\nEpoch 1223/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1020 - mae: 0.1020\nEpoch 1223: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3252 - mae: 0.3252 - val_loss: 0.1726 - val_mae: 0.1726\nEpoch 1224/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1548 - mae: 0.1548\nEpoch 1224: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3192 - mae: 0.3192 - val_loss: 0.1062 - val_mae: 0.1062\nEpoch 1225/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.0974 - mae: 0.0974\nEpoch 1225: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3040 - mae: 0.3040 - val_loss: 0.1686 - val_mae: 0.1686\nEpoch 1226/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2946 - mae: 0.2946\nEpoch 1226: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3118 - mae: 0.3118 - val_loss: 0.1022 - val_mae: 0.1022\nEpoch 1227/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1495 - mae: 0.1495\nEpoch 1227: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3178 - mae: 0.3178 - val_loss: 0.1138 - val_mae: 0.1138\nEpoch 1228/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2592 - mae: 0.2592\nEpoch 1228: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3278 - mae: 0.3278 - val_loss: 0.1365 - val_mae: 0.1365\nEpoch 1229/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1254 - mae: 0.1254\nEpoch 1229: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3405 - mae: 0.3405 - val_loss: 0.1966 - val_mae: 0.1966\nEpoch 1230/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2341 - mae: 0.2341\nEpoch 1230: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3798 - mae: 0.3798 - val_loss: 0.1152 - val_mae: 0.1152\nEpoch 1231/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9255 - mae: 0.9255\nEpoch 1231: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3971 - mae: 0.3971 - val_loss: 0.2527 - val_mae: 0.2527\nEpoch 1232/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2523 - mae: 0.2523\nEpoch 1232: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3943 - mae: 0.3943 - val_loss: 0.2419 - val_mae: 0.2419\nEpoch 1233/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2071 - mae: 0.2071\nEpoch 1233: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3377 - mae: 0.3377 - val_loss: 0.1670 - val_mae: 0.1670\nEpoch 1234/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1716 - mae: 0.1716\nEpoch 1234: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3133 - mae: 0.3133 - val_loss: 0.1139 - val_mae: 0.1139\nEpoch 1235/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1072 - mae: 0.1072\nEpoch 1235: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3279 - mae: 0.3279 - val_loss: 0.1099 - val_mae: 0.1099\nEpoch 1236/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1359 - mae: 0.1359\nEpoch 1236: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3234 - mae: 0.3234 - val_loss: 0.1664 - val_mae: 0.1664\nEpoch 1237/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2998 - mae: 0.2998\nEpoch 1237: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3177 - mae: 0.3177 - val_loss: 0.1387 - val_mae: 0.1387\nEpoch 1238/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1306 - mae: 0.1306\nEpoch 1238: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3407 - mae: 0.3407 - val_loss: 0.1989 - val_mae: 0.1989\nEpoch 1239/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4166 - mae: 0.4166\nEpoch 1239: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3651 - mae: 0.3651 - val_loss: 0.1781 - val_mae: 0.1781\nEpoch 1240/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2179 - mae: 0.2179\nEpoch 1240: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3416 - mae: 0.3416 - val_loss: 0.1825 - val_mae: 0.1825\nEpoch 1241/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3090 - mae: 0.3090\nEpoch 1241: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3187 - mae: 0.3187 - val_loss: 0.1015 - val_mae: 0.1015\nEpoch 1242/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1164 - mae: 0.1164\nEpoch 1242: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 8ms/step - loss: 0.3018 - mae: 0.3018 - val_loss: 0.1646 - val_mae: 0.1646\nEpoch 1243/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1308 - mae: 0.1308\nEpoch 1243: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3058 - mae: 0.3058 - val_loss: 0.1052 - val_mae: 0.1052\nEpoch 1244/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2530 - mae: 0.2530\nEpoch 1244: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3057 - mae: 0.3057 - val_loss: 0.1101 - val_mae: 0.1101\nEpoch 1245/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2114 - mae: 0.2114\nEpoch 1245: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3374 - mae: 0.3374 - val_loss: 0.1890 - val_mae: 0.1890\nEpoch 1246/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1659 - mae: 0.1659\nEpoch 1246: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3229 - mae: 0.3229 - val_loss: 0.1014 - val_mae: 0.1014\nEpoch 1247/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1209 - mae: 0.1209\nEpoch 1247: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3229 - mae: 0.3229 - val_loss: 0.1341 - val_mae: 0.1341\nEpoch 1248/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1530 - mae: 0.1530\nEpoch 1248: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 12ms/step - loss: 0.3291 - mae: 0.3291 - val_loss: 0.1029 - val_mae: 0.1029\nEpoch 1249/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2807 - mae: 0.2807\nEpoch 1249: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 10ms/step - loss: 0.3742 - mae: 0.3742 - val_loss: 0.2649 - val_mae: 0.2649\nEpoch 1250/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4746 - mae: 0.4746\nEpoch 1250: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3812 - mae: 0.3812 - val_loss: 0.2524 - val_mae: 0.2524\nEpoch 1251/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3513 - mae: 0.3513\nEpoch 1251: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3442 - mae: 0.3442 - val_loss: 0.1424 - val_mae: 0.1424\nEpoch 1252/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1597 - mae: 0.1597\nEpoch 1252: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3145 - mae: 0.3145 - val_loss: 0.1404 - val_mae: 0.1404\nEpoch 1253/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.0881 - mae: 0.0881\nEpoch 1253: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3125 - mae: 0.3125 - val_loss: 0.1045 - val_mae: 0.1045\nEpoch 1254/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1471 - mae: 0.1471\nEpoch 1254: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3296 - mae: 0.3296 - val_loss: 0.1789 - val_mae: 0.1789\nEpoch 1255/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1525 - mae: 0.1525\nEpoch 1255: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3337 - mae: 0.3337 - val_loss: 0.1364 - val_mae: 0.1364\nEpoch 1256/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1806 - mae: 0.1806\nEpoch 1256: val_loss did not improve from 0.10001\n7/7 [==============================] - 0s 7ms/step - loss: 0.3275 - mae: 0.3275 - val_loss: 0.1014 - val_mae: 0.1014\nEpoch 1257/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1195 - mae: 0.1195\nEpoch 1257: val_loss improved from 0.10001 to 0.09826, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.3201 - mae: 0.3201 - val_loss: 0.0983 - val_mae: 0.0983\nEpoch 1258/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9631 - mae: 0.9631\nEpoch 1258: val_loss did not improve from 0.09826\n7/7 [==============================] - 0s 7ms/step - loss: 0.3046 - mae: 0.3046 - val_loss: 0.1112 - val_mae: 0.1112\nEpoch 1259/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6509 - mae: 0.6509\nEpoch 1259: val_loss did not improve from 0.09826\n7/7 [==============================] - 0s 7ms/step - loss: 0.3106 - mae: 0.3106 - val_loss: 0.1180 - val_mae: 0.1180\nEpoch 1260/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1737 - mae: 0.1737\nEpoch 1260: val_loss did not improve from 0.09826\n7/7 [==============================] - 0s 7ms/step - loss: 0.3258 - mae: 0.3258 - val_loss: 0.1160 - val_mae: 0.1160\nEpoch 1261/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.0972 - mae: 0.0972\nEpoch 1261: val_loss did not improve from 0.09826\n7/7 [==============================] - 0s 7ms/step - loss: 0.3021 - mae: 0.3021 - val_loss: 0.2372 - val_mae: 0.2372\nEpoch 1262/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2175 - mae: 0.2175\nEpoch 1262: val_loss did not improve from 0.09826\n7/7 [==============================] - 0s 7ms/step - loss: 0.3608 - mae: 0.3608 - val_loss: 0.1733 - val_mae: 0.1733\nEpoch 1263/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1519 - mae: 0.1519\nEpoch 1263: val_loss did not improve from 0.09826\n7/7 [==============================] - 0s 7ms/step - loss: 0.3446 - mae: 0.3446 - val_loss: 0.1189 - val_mae: 0.1189\nEpoch 1264/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1975 - mae: 0.1975\nEpoch 1264: val_loss did not improve from 0.09826\n7/7 [==============================] - 0s 7ms/step - loss: 0.3174 - mae: 0.3174 - val_loss: 0.1286 - val_mae: 0.1286\nEpoch 1265/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1180 - mae: 0.1180\nEpoch 1265: val_loss did not improve from 0.09826\n7/7 [==============================] - 0s 7ms/step - loss: 0.3429 - mae: 0.3429 - val_loss: 0.2874 - val_mae: 0.2874\nEpoch 1266/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5062 - mae: 0.5062\nEpoch 1266: val_loss did not improve from 0.09826\n7/7 [==============================] - 0s 7ms/step - loss: 0.3523 - mae: 0.3523 - val_loss: 0.2229 - val_mae: 0.2229\nEpoch 1267/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1722 - mae: 0.1722\nEpoch 1267: val_loss did not improve from 0.09826\n7/7 [==============================] - 0s 7ms/step - loss: 0.3559 - mae: 0.3559 - val_loss: 0.1388 - val_mae: 0.1388\nEpoch 1268/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6610 - mae: 0.6610\nEpoch 1268: val_loss did not improve from 0.09826\n7/7 [==============================] - 0s 7ms/step - loss: 0.3341 - mae: 0.3341 - val_loss: 0.1038 - val_mae: 0.1038\nEpoch 1269/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5363 - mae: 0.5363\nEpoch 1269: val_loss did not improve from 0.09826\n7/7 [==============================] - 0s 7ms/step - loss: 0.3130 - mae: 0.3130 - val_loss: 0.1097 - val_mae: 0.1097\nEpoch 1270/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1198 - mae: 0.1198\nEpoch 1270: val_loss did not improve from 0.09826\n7/7 [==============================] - 0s 7ms/step - loss: 0.3056 - mae: 0.3056 - val_loss: 0.1135 - val_mae: 0.1135\nEpoch 1271/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2009 - mae: 0.2009\nEpoch 1271: val_loss did not improve from 0.09826\n7/7 [==============================] - 0s 7ms/step - loss: 0.3106 - mae: 0.3106 - val_loss: 0.1085 - val_mae: 0.1085\nEpoch 1272/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1437 - mae: 0.1437\nEpoch 1272: val_loss did not improve from 0.09826\n7/7 [==============================] - 0s 7ms/step - loss: 0.3070 - mae: 0.3070 - val_loss: 0.1007 - val_mae: 0.1007\nEpoch 1273/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2539 - mae: 0.2539\nEpoch 1273: val_loss improved from 0.09826 to 0.09801, saving model to best_model.h5\n7/7 [==============================] - 0s 10ms/step - loss: 0.3153 - mae: 0.3153 - val_loss: 0.0980 - val_mae: 0.0980\nEpoch 1274/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3956 - mae: 0.3956\nEpoch 1274: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3140 - mae: 0.3140 - val_loss: 0.2176 - val_mae: 0.2176\nEpoch 1275/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1614 - mae: 0.1614\nEpoch 1275: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3359 - mae: 0.3359 - val_loss: 0.3405 - val_mae: 0.3405\nEpoch 1276/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3948 - mae: 0.3948\nEpoch 1276: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3718 - mae: 0.3718 - val_loss: 0.1672 - val_mae: 0.1672\nEpoch 1277/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7174 - mae: 0.7174\nEpoch 1277: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3263 - mae: 0.3263 - val_loss: 0.1064 - val_mae: 0.1064\nEpoch 1278/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1129 - mae: 0.1129\nEpoch 1278: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 6ms/step - loss: 0.3075 - mae: 0.3075 - val_loss: 0.1153 - val_mae: 0.1153\nEpoch 1279/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1146 - mae: 0.1146\nEpoch 1279: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3024 - mae: 0.3024 - val_loss: 0.1464 - val_mae: 0.1464\nEpoch 1280/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1388 - mae: 0.1388\nEpoch 1280: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3008 - mae: 0.3008 - val_loss: 0.1325 - val_mae: 0.1325\nEpoch 1281/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1643 - mae: 0.1643\nEpoch 1281: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3096 - mae: 0.3096 - val_loss: 0.1256 - val_mae: 0.1256\nEpoch 1282/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3019 - mae: 0.3019\nEpoch 1282: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3239 - mae: 0.3239 - val_loss: 0.2245 - val_mae: 0.2245\nEpoch 1283/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5116 - mae: 0.5116\nEpoch 1283: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.4493 - mae: 0.4493 - val_loss: 0.2948 - val_mae: 0.2948\nEpoch 1284/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5306 - mae: 0.5306\nEpoch 1284: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.4506 - mae: 0.4506 - val_loss: 0.1160 - val_mae: 0.1160\nEpoch 1285/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1176 - mae: 0.1176\nEpoch 1285: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.4975 - mae: 0.4975 - val_loss: 0.2314 - val_mae: 0.2314\nEpoch 1286/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5088 - mae: 0.5088\nEpoch 1286: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.4104 - mae: 0.4104 - val_loss: 0.1796 - val_mae: 0.1796\nEpoch 1287/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5552 - mae: 0.5552\nEpoch 1287: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3493 - mae: 0.3493 - val_loss: 0.1180 - val_mae: 0.1180\nEpoch 1288/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2817 - mae: 0.2817\nEpoch 1288: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3369 - mae: 0.3369 - val_loss: 0.2222 - val_mae: 0.2222\nEpoch 1289/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3201 - mae: 0.3201\nEpoch 1289: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3976 - mae: 0.3976 - val_loss: 0.1290 - val_mae: 0.1290\nEpoch 1290/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3934 - mae: 0.3934\nEpoch 1290: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3416 - mae: 0.3416 - val_loss: 0.2036 - val_mae: 0.2036\nEpoch 1291/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6987 - mae: 0.6987\nEpoch 1291: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3286 - mae: 0.3286 - val_loss: 0.1111 - val_mae: 0.1111\nEpoch 1292/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2511 - mae: 0.2511\nEpoch 1292: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3133 - mae: 0.3133 - val_loss: 0.1359 - val_mae: 0.1359\nEpoch 1293/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6574 - mae: 0.6574\nEpoch 1293: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 10ms/step - loss: 0.3215 - mae: 0.3215 - val_loss: 0.1144 - val_mae: 0.1144\nEpoch 1294/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1241 - mae: 0.1241\nEpoch 1294: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3096 - mae: 0.3096 - val_loss: 0.1050 - val_mae: 0.1050\nEpoch 1295/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4288 - mae: 0.4288\nEpoch 1295: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 10ms/step - loss: 0.3092 - mae: 0.3092 - val_loss: 0.2484 - val_mae: 0.2484\nEpoch 1296/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7489 - mae: 0.7489\nEpoch 1296: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3302 - mae: 0.3302 - val_loss: 0.1292 - val_mae: 0.1292\nEpoch 1297/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3660 - mae: 0.3660\nEpoch 1297: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3028 - mae: 0.3028 - val_loss: 0.1156 - val_mae: 0.1156\nEpoch 1298/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3954 - mae: 0.3954\nEpoch 1298: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3249 - mae: 0.3249 - val_loss: 0.1462 - val_mae: 0.1462\nEpoch 1299/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6478 - mae: 0.6478\nEpoch 1299: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3298 - mae: 0.3298 - val_loss: 0.1458 - val_mae: 0.1458\nEpoch 1300/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1474 - mae: 0.1474\nEpoch 1300: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 8ms/step - loss: 0.3233 - mae: 0.3233 - val_loss: 0.1045 - val_mae: 0.1045\nEpoch 1301/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6862 - mae: 0.6862\nEpoch 1301: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3092 - mae: 0.3092 - val_loss: 0.1221 - val_mae: 0.1221\nEpoch 1302/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6911 - mae: 0.6911\nEpoch 1302: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3101 - mae: 0.3101 - val_loss: 0.1059 - val_mae: 0.1059\nEpoch 1303/1500\n1/7 [===>..........................] - ETA: 0s - loss: 1.0570 - mae: 1.0570\nEpoch 1303: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3446 - mae: 0.3446 - val_loss: 0.1785 - val_mae: 0.1785\nEpoch 1304/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2873 - mae: 0.2873\nEpoch 1304: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3494 - mae: 0.3494 - val_loss: 0.1066 - val_mae: 0.1066\nEpoch 1305/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1213 - mae: 0.1213\nEpoch 1305: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3133 - mae: 0.3133 - val_loss: 0.1313 - val_mae: 0.1313\nEpoch 1306/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1112 - mae: 0.1112\nEpoch 1306: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3113 - mae: 0.3113 - val_loss: 0.1015 - val_mae: 0.1015\nEpoch 1307/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1446 - mae: 0.1446\nEpoch 1307: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3039 - mae: 0.3039 - val_loss: 0.3316 - val_mae: 0.3316\nEpoch 1308/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9419 - mae: 0.9419\nEpoch 1308: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.4159 - mae: 0.4159 - val_loss: 0.2662 - val_mae: 0.2662\nEpoch 1309/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2198 - mae: 0.2198\nEpoch 1309: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 6ms/step - loss: 0.3638 - mae: 0.3638 - val_loss: 0.1849 - val_mae: 0.1849\nEpoch 1310/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3220 - mae: 0.3220\nEpoch 1310: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3389 - mae: 0.3389 - val_loss: 0.1165 - val_mae: 0.1165\nEpoch 1311/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1095 - mae: 0.1095\nEpoch 1311: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3149 - mae: 0.3149 - val_loss: 0.1031 - val_mae: 0.1031\nEpoch 1312/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1143 - mae: 0.1143\nEpoch 1312: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 9ms/step - loss: 0.3101 - mae: 0.3101 - val_loss: 0.1718 - val_mae: 0.1718\nEpoch 1313/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1608 - mae: 0.1608\nEpoch 1313: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3177 - mae: 0.3177 - val_loss: 0.1041 - val_mae: 0.1041\nEpoch 1314/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.8133 - mae: 0.8133\nEpoch 1314: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3102 - mae: 0.3102 - val_loss: 0.1233 - val_mae: 0.1233\nEpoch 1315/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1804 - mae: 0.1804\nEpoch 1315: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3390 - mae: 0.3390 - val_loss: 0.1145 - val_mae: 0.1145\nEpoch 1316/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1438 - mae: 0.1438\nEpoch 1316: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3252 - mae: 0.3252 - val_loss: 0.1669 - val_mae: 0.1669\nEpoch 1317/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4228 - mae: 0.4228\nEpoch 1317: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3217 - mae: 0.3217 - val_loss: 0.1132 - val_mae: 0.1132\nEpoch 1318/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1521 - mae: 0.1521\nEpoch 1318: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3102 - mae: 0.3102 - val_loss: 0.1119 - val_mae: 0.1119\nEpoch 1319/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1588 - mae: 0.1588\nEpoch 1319: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3211 - mae: 0.3211 - val_loss: 0.1262 - val_mae: 0.1262\nEpoch 1320/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2836 - mae: 0.2836\nEpoch 1320: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3312 - mae: 0.3312 - val_loss: 0.1058 - val_mae: 0.1058\nEpoch 1321/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2258 - mae: 0.2258\nEpoch 1321: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3296 - mae: 0.3296 - val_loss: 0.1973 - val_mae: 0.1973\nEpoch 1322/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1428 - mae: 0.1428\nEpoch 1322: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3150 - mae: 0.3150 - val_loss: 0.1746 - val_mae: 0.1746\nEpoch 1323/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1764 - mae: 0.1764\nEpoch 1323: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3137 - mae: 0.3137 - val_loss: 0.1174 - val_mae: 0.1174\nEpoch 1324/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1493 - mae: 0.1493\nEpoch 1324: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3367 - mae: 0.3367 - val_loss: 0.1013 - val_mae: 0.1013\nEpoch 1325/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1084 - mae: 0.1084\nEpoch 1325: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3021 - mae: 0.3021 - val_loss: 0.1881 - val_mae: 0.1881\nEpoch 1326/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4665 - mae: 0.4665\nEpoch 1326: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3370 - mae: 0.3370 - val_loss: 0.2043 - val_mae: 0.2043\nEpoch 1327/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1903 - mae: 0.1903\nEpoch 1327: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3239 - mae: 0.3239 - val_loss: 0.1044 - val_mae: 0.1044\nEpoch 1328/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6966 - mae: 0.6966\nEpoch 1328: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3085 - mae: 0.3085 - val_loss: 0.2761 - val_mae: 0.2761\nEpoch 1329/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4966 - mae: 0.4966\nEpoch 1329: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 10ms/step - loss: 0.3486 - mae: 0.3486 - val_loss: 0.2161 - val_mae: 0.2161\nEpoch 1330/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7937 - mae: 0.7937\nEpoch 1330: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3252 - mae: 0.3252 - val_loss: 0.1116 - val_mae: 0.1116\nEpoch 1331/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3182 - mae: 0.3182\nEpoch 1331: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 10ms/step - loss: 0.3195 - mae: 0.3195 - val_loss: 0.1270 - val_mae: 0.1270\nEpoch 1332/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2502 - mae: 0.2502\nEpoch 1332: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3164 - mae: 0.3164 - val_loss: 0.1056 - val_mae: 0.1056\nEpoch 1333/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2715 - mae: 0.2715\nEpoch 1333: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3043 - mae: 0.3043 - val_loss: 0.1034 - val_mae: 0.1034\nEpoch 1334/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4538 - mae: 0.4538\nEpoch 1334: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3209 - mae: 0.3209 - val_loss: 0.1696 - val_mae: 0.1696\nEpoch 1335/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1523 - mae: 0.1523\nEpoch 1335: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3153 - mae: 0.3153 - val_loss: 0.1341 - val_mae: 0.1341\nEpoch 1336/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1881 - mae: 0.1881\nEpoch 1336: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3476 - mae: 0.3476 - val_loss: 0.1555 - val_mae: 0.1555\nEpoch 1337/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1442 - mae: 0.1442\nEpoch 1337: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 8ms/step - loss: 0.3249 - mae: 0.3249 - val_loss: 0.1791 - val_mae: 0.1791\nEpoch 1338/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2799 - mae: 0.2799\nEpoch 1338: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3267 - mae: 0.3267 - val_loss: 0.1045 - val_mae: 0.1045\nEpoch 1339/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2327 - mae: 0.2327\nEpoch 1339: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3136 - mae: 0.3136 - val_loss: 0.1537 - val_mae: 0.1537\nEpoch 1340/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1501 - mae: 0.1501\nEpoch 1340: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3451 - mae: 0.3451 - val_loss: 0.2010 - val_mae: 0.2010\nEpoch 1341/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7374 - mae: 0.7374\nEpoch 1341: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3537 - mae: 0.3537 - val_loss: 0.1216 - val_mae: 0.1216\nEpoch 1342/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1487 - mae: 0.1487\nEpoch 1342: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 6ms/step - loss: 0.3083 - mae: 0.3083 - val_loss: 0.1051 - val_mae: 0.1051\nEpoch 1343/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6877 - mae: 0.6877\nEpoch 1343: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3397 - mae: 0.3397 - val_loss: 0.1274 - val_mae: 0.1274\nEpoch 1344/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.0984 - mae: 0.0984\nEpoch 1344: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3492 - mae: 0.3492 - val_loss: 0.1720 - val_mae: 0.1720\nEpoch 1345/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1698 - mae: 0.1698\nEpoch 1345: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3126 - mae: 0.3126 - val_loss: 0.1254 - val_mae: 0.1254\nEpoch 1346/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1309 - mae: 0.1309\nEpoch 1346: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3305 - mae: 0.3305 - val_loss: 0.1221 - val_mae: 0.1221\nEpoch 1347/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1223 - mae: 0.1223\nEpoch 1347: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3219 - mae: 0.3219 - val_loss: 0.1882 - val_mae: 0.1882\nEpoch 1348/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2309 - mae: 0.2309\nEpoch 1348: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3122 - mae: 0.3122 - val_loss: 0.1239 - val_mae: 0.1239\nEpoch 1349/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.0988 - mae: 0.0988\nEpoch 1349: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3040 - mae: 0.3040 - val_loss: 0.1370 - val_mae: 0.1370\nEpoch 1350/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7293 - mae: 0.7293\nEpoch 1350: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3102 - mae: 0.3102 - val_loss: 0.1311 - val_mae: 0.1311\nEpoch 1351/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1205 - mae: 0.1205\nEpoch 1351: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3194 - mae: 0.3194 - val_loss: 0.1307 - val_mae: 0.1307\nEpoch 1352/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3344 - mae: 0.3344\nEpoch 1352: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3132 - mae: 0.3132 - val_loss: 0.1040 - val_mae: 0.1040\nEpoch 1353/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2292 - mae: 0.2292\nEpoch 1353: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3149 - mae: 0.3149 - val_loss: 0.1032 - val_mae: 0.1032\nEpoch 1354/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3679 - mae: 0.3679\nEpoch 1354: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3133 - mae: 0.3133 - val_loss: 0.2333 - val_mae: 0.2333\nEpoch 1355/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1842 - mae: 0.1842\nEpoch 1355: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3302 - mae: 0.3302 - val_loss: 0.1144 - val_mae: 0.1144\nEpoch 1356/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5578 - mae: 0.5578\nEpoch 1356: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3475 - mae: 0.3475 - val_loss: 0.1269 - val_mae: 0.1269\nEpoch 1357/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2872 - mae: 0.2872\nEpoch 1357: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3314 - mae: 0.3314 - val_loss: 0.1670 - val_mae: 0.1670\nEpoch 1358/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1348 - mae: 0.1348\nEpoch 1358: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3194 - mae: 0.3194 - val_loss: 0.1032 - val_mae: 0.1032\nEpoch 1359/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7698 - mae: 0.7698\nEpoch 1359: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3043 - mae: 0.3043 - val_loss: 0.1765 - val_mae: 0.1765\nEpoch 1360/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1456 - mae: 0.1456\nEpoch 1360: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3235 - mae: 0.3235 - val_loss: 0.1219 - val_mae: 0.1219\nEpoch 1361/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1177 - mae: 0.1177\nEpoch 1361: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3395 - mae: 0.3395 - val_loss: 0.2536 - val_mae: 0.2536\nEpoch 1362/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3515 - mae: 0.3515\nEpoch 1362: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 6ms/step - loss: 0.3656 - mae: 0.3656 - val_loss: 0.1239 - val_mae: 0.1239\nEpoch 1363/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1623 - mae: 0.1623\nEpoch 1363: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3420 - mae: 0.3420 - val_loss: 0.2505 - val_mae: 0.2505\nEpoch 1364/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3438 - mae: 0.3438\nEpoch 1364: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3455 - mae: 0.3455 - val_loss: 0.1049 - val_mae: 0.1049\nEpoch 1365/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1546 - mae: 0.1546\nEpoch 1365: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3088 - mae: 0.3088 - val_loss: 0.1115 - val_mae: 0.1115\nEpoch 1366/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1766 - mae: 0.1766\nEpoch 1366: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3114 - mae: 0.3114 - val_loss: 0.1019 - val_mae: 0.1019\nEpoch 1367/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1384 - mae: 0.1384\nEpoch 1367: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3070 - mae: 0.3070 - val_loss: 0.2307 - val_mae: 0.2307\nEpoch 1368/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5347 - mae: 0.5347\nEpoch 1368: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3743 - mae: 0.3743 - val_loss: 0.1594 - val_mae: 0.1594\nEpoch 1369/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3895 - mae: 0.3895\nEpoch 1369: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3700 - mae: 0.3700 - val_loss: 0.1498 - val_mae: 0.1498\nEpoch 1370/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2894 - mae: 0.2894\nEpoch 1370: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3243 - mae: 0.3243 - val_loss: 0.1053 - val_mae: 0.1053\nEpoch 1371/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1185 - mae: 0.1185\nEpoch 1371: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3119 - mae: 0.3119 - val_loss: 0.1085 - val_mae: 0.1085\nEpoch 1372/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9568 - mae: 0.9568\nEpoch 1372: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 8ms/step - loss: 0.3151 - mae: 0.3151 - val_loss: 0.1461 - val_mae: 0.1461\nEpoch 1373/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1603 - mae: 0.1603\nEpoch 1373: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3289 - mae: 0.3289 - val_loss: 0.2032 - val_mae: 0.2032\nEpoch 1374/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7006 - mae: 0.7006\nEpoch 1374: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3316 - mae: 0.3316 - val_loss: 0.1151 - val_mae: 0.1151\nEpoch 1375/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1321 - mae: 0.1321\nEpoch 1375: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3238 - mae: 0.3238 - val_loss: 0.1054 - val_mae: 0.1054\nEpoch 1376/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1269 - mae: 0.1269\nEpoch 1376: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 8ms/step - loss: 0.3098 - mae: 0.3098 - val_loss: 0.1361 - val_mae: 0.1361\nEpoch 1377/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3814 - mae: 0.3814\nEpoch 1377: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 8ms/step - loss: 0.3236 - mae: 0.3236 - val_loss: 0.1633 - val_mae: 0.1633\nEpoch 1378/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1741 - mae: 0.1741\nEpoch 1378: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3165 - mae: 0.3165 - val_loss: 0.1422 - val_mae: 0.1422\nEpoch 1379/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5046 - mae: 0.5046\nEpoch 1379: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3218 - mae: 0.3218 - val_loss: 0.1342 - val_mae: 0.1342\nEpoch 1380/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1429 - mae: 0.1429\nEpoch 1380: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 6ms/step - loss: 0.3148 - mae: 0.3148 - val_loss: 0.1038 - val_mae: 0.1038\nEpoch 1381/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5514 - mae: 0.5514\nEpoch 1381: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3102 - mae: 0.3102 - val_loss: 0.1144 - val_mae: 0.1144\nEpoch 1382/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1146 - mae: 0.1146\nEpoch 1382: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3255 - mae: 0.3255 - val_loss: 0.1221 - val_mae: 0.1221\nEpoch 1383/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6579 - mae: 0.6579\nEpoch 1383: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3049 - mae: 0.3049 - val_loss: 0.1030 - val_mae: 0.1030\nEpoch 1384/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2726 - mae: 0.2726\nEpoch 1384: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 6ms/step - loss: 0.3039 - mae: 0.3039 - val_loss: 0.1042 - val_mae: 0.1042\nEpoch 1385/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1549 - mae: 0.1549\nEpoch 1385: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3196 - mae: 0.3196 - val_loss: 0.2062 - val_mae: 0.2062\nEpoch 1386/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1858 - mae: 0.1858\nEpoch 1386: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 6ms/step - loss: 0.3392 - mae: 0.3392 - val_loss: 0.1726 - val_mae: 0.1726\nEpoch 1387/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6888 - mae: 0.6888\nEpoch 1387: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 6ms/step - loss: 0.3438 - mae: 0.3438 - val_loss: 0.1000 - val_mae: 0.1000\nEpoch 1388/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2883 - mae: 0.2883\nEpoch 1388: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 6ms/step - loss: 0.3215 - mae: 0.3215 - val_loss: 0.1945 - val_mae: 0.1945\nEpoch 1389/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3226 - mae: 0.3226\nEpoch 1389: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3272 - mae: 0.3272 - val_loss: 0.1630 - val_mae: 0.1630\nEpoch 1390/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4706 - mae: 0.4706\nEpoch 1390: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3467 - mae: 0.3467 - val_loss: 0.1158 - val_mae: 0.1158\nEpoch 1391/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6384 - mae: 0.6384\nEpoch 1391: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 6ms/step - loss: 0.3435 - mae: 0.3435 - val_loss: 0.2472 - val_mae: 0.2472\nEpoch 1392/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2922 - mae: 0.2922\nEpoch 1392: val_loss did not improve from 0.09801\n7/7 [==============================] - 0s 7ms/step - loss: 0.3631 - mae: 0.3631 - val_loss: 0.1065 - val_mae: 0.1065\nEpoch 1393/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2608 - mae: 0.2608\nEpoch 1393: val_loss improved from 0.09801 to 0.09617, saving model to best_model.h5\n7/7 [==============================] - 0s 13ms/step - loss: 0.3354 - mae: 0.3354 - val_loss: 0.0962 - val_mae: 0.0962\nEpoch 1394/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1241 - mae: 0.1241\nEpoch 1394: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 6ms/step - loss: 0.3171 - mae: 0.3171 - val_loss: 0.1041 - val_mae: 0.1041\nEpoch 1395/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5266 - mae: 0.5266\nEpoch 1395: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3095 - mae: 0.3095 - val_loss: 0.2656 - val_mae: 0.2656\nEpoch 1396/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2341 - mae: 0.2341\nEpoch 1396: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3575 - mae: 0.3575 - val_loss: 0.1221 - val_mae: 0.1221\nEpoch 1397/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1066 - mae: 0.1066\nEpoch 1397: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3253 - mae: 0.3253 - val_loss: 0.1101 - val_mae: 0.1101\nEpoch 1398/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1675 - mae: 0.1675\nEpoch 1398: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3208 - mae: 0.3208 - val_loss: 0.1058 - val_mae: 0.1058\nEpoch 1399/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1255 - mae: 0.1255\nEpoch 1399: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3103 - mae: 0.3103 - val_loss: 0.1194 - val_mae: 0.1194\nEpoch 1400/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1018 - mae: 0.1018\nEpoch 1400: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3092 - mae: 0.3092 - val_loss: 0.1080 - val_mae: 0.1080\nEpoch 1401/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1013 - mae: 0.1013\nEpoch 1401: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 9ms/step - loss: 0.3228 - mae: 0.3228 - val_loss: 0.1581 - val_mae: 0.1581\nEpoch 1402/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1687 - mae: 0.1687\nEpoch 1402: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3305 - mae: 0.3305 - val_loss: 0.0984 - val_mae: 0.0984\nEpoch 1403/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1218 - mae: 0.1218\nEpoch 1403: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3135 - mae: 0.3135 - val_loss: 0.1483 - val_mae: 0.1483\nEpoch 1404/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2592 - mae: 0.2592\nEpoch 1404: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3133 - mae: 0.3133 - val_loss: 0.1180 - val_mae: 0.1180\nEpoch 1405/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2834 - mae: 0.2834\nEpoch 1405: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3222 - mae: 0.3222 - val_loss: 0.1605 - val_mae: 0.1605\nEpoch 1406/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3491 - mae: 0.3491\nEpoch 1406: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3525 - mae: 0.3525 - val_loss: 0.1377 - val_mae: 0.1377\nEpoch 1407/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1122 - mae: 0.1122\nEpoch 1407: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3354 - mae: 0.3354 - val_loss: 0.1414 - val_mae: 0.1414\nEpoch 1408/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1100 - mae: 0.1100\nEpoch 1408: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3417 - mae: 0.3417 - val_loss: 0.2141 - val_mae: 0.2141\nEpoch 1409/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2479 - mae: 0.2479\nEpoch 1409: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3607 - mae: 0.3607 - val_loss: 0.1086 - val_mae: 0.1086\nEpoch 1410/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5148 - mae: 0.5148\nEpoch 1410: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3274 - mae: 0.3274 - val_loss: 0.2100 - val_mae: 0.2100\nEpoch 1411/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2160 - mae: 0.2160\nEpoch 1411: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3609 - mae: 0.3609 - val_loss: 0.2910 - val_mae: 0.2910\nEpoch 1412/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2619 - mae: 0.2619\nEpoch 1412: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 10ms/step - loss: 0.3431 - mae: 0.3431 - val_loss: 0.1211 - val_mae: 0.1211\nEpoch 1413/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6920 - mae: 0.6920\nEpoch 1413: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3167 - mae: 0.3167 - val_loss: 0.1056 - val_mae: 0.1056\nEpoch 1414/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9008 - mae: 0.9008\nEpoch 1414: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3414 - mae: 0.3414 - val_loss: 0.1131 - val_mae: 0.1131\nEpoch 1415/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1037 - mae: 0.1037\nEpoch 1415: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3343 - mae: 0.3343 - val_loss: 0.1380 - val_mae: 0.1380\nEpoch 1416/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2309 - mae: 0.2309\nEpoch 1416: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3158 - mae: 0.3158 - val_loss: 0.1885 - val_mae: 0.1885\nEpoch 1417/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2827 - mae: 0.2827\nEpoch 1417: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3832 - mae: 0.3832 - val_loss: 0.1358 - val_mae: 0.1358\nEpoch 1418/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5897 - mae: 0.5897\nEpoch 1418: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 10ms/step - loss: 0.3502 - mae: 0.3502 - val_loss: 0.1376 - val_mae: 0.1376\nEpoch 1419/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4030 - mae: 0.4030\nEpoch 1419: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3207 - mae: 0.3207 - val_loss: 0.2682 - val_mae: 0.2682\nEpoch 1420/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2308 - mae: 0.2308\nEpoch 1420: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3692 - mae: 0.3692 - val_loss: 0.2212 - val_mae: 0.2212\nEpoch 1421/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2905 - mae: 0.2905\nEpoch 1421: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3227 - mae: 0.3227 - val_loss: 0.1587 - val_mae: 0.1587\nEpoch 1422/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3927 - mae: 0.3927\nEpoch 1422: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 10ms/step - loss: 0.3300 - mae: 0.3300 - val_loss: 0.1089 - val_mae: 0.1089\nEpoch 1423/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1227 - mae: 0.1227\nEpoch 1423: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3559 - mae: 0.3559 - val_loss: 0.1158 - val_mae: 0.1158\nEpoch 1424/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2335 - mae: 0.2335\nEpoch 1424: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3321 - mae: 0.3321 - val_loss: 0.1696 - val_mae: 0.1696\nEpoch 1425/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7920 - mae: 0.7920\nEpoch 1425: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3581 - mae: 0.3581 - val_loss: 0.2744 - val_mae: 0.2744\nEpoch 1426/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6363 - mae: 0.6363\nEpoch 1426: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3855 - mae: 0.3855 - val_loss: 0.2375 - val_mae: 0.2375\nEpoch 1427/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3278 - mae: 0.3278\nEpoch 1427: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3357 - mae: 0.3357 - val_loss: 0.1174 - val_mae: 0.1174\nEpoch 1428/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2862 - mae: 0.2862\nEpoch 1428: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3059 - mae: 0.3059 - val_loss: 0.1411 - val_mae: 0.1411\nEpoch 1429/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4005 - mae: 0.4005\nEpoch 1429: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3129 - mae: 0.3129 - val_loss: 0.1142 - val_mae: 0.1142\nEpoch 1430/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1171 - mae: 0.1171\nEpoch 1430: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 10ms/step - loss: 0.3100 - mae: 0.3100 - val_loss: 0.1006 - val_mae: 0.1006\nEpoch 1431/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1195 - mae: 0.1195\nEpoch 1431: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3051 - mae: 0.3051 - val_loss: 0.1416 - val_mae: 0.1416\nEpoch 1432/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2575 - mae: 0.2575\nEpoch 1432: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3104 - mae: 0.3104 - val_loss: 0.1114 - val_mae: 0.1114\nEpoch 1433/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1122 - mae: 0.1122\nEpoch 1433: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3070 - mae: 0.3070 - val_loss: 0.1056 - val_mae: 0.1056\nEpoch 1434/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1509 - mae: 0.1509\nEpoch 1434: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3221 - mae: 0.3221 - val_loss: 0.1236 - val_mae: 0.1236\nEpoch 1435/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3094 - mae: 0.3094\nEpoch 1435: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3233 - mae: 0.3233 - val_loss: 0.1504 - val_mae: 0.1504\nEpoch 1436/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3237 - mae: 0.3237\nEpoch 1436: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3199 - mae: 0.3199 - val_loss: 0.1812 - val_mae: 0.1812\nEpoch 1437/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4024 - mae: 0.4024\nEpoch 1437: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3240 - mae: 0.3240 - val_loss: 0.1286 - val_mae: 0.1286\nEpoch 1438/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7012 - mae: 0.7012\nEpoch 1438: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3289 - mae: 0.3289 - val_loss: 0.1063 - val_mae: 0.1063\nEpoch 1439/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3162 - mae: 0.3162\nEpoch 1439: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3104 - mae: 0.3104 - val_loss: 0.1428 - val_mae: 0.1428\nEpoch 1440/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9715 - mae: 0.9715\nEpoch 1440: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3109 - mae: 0.3109 - val_loss: 0.2061 - val_mae: 0.2061\nEpoch 1441/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3238 - mae: 0.3238\nEpoch 1441: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3246 - mae: 0.3246 - val_loss: 0.1128 - val_mae: 0.1128\nEpoch 1442/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1249 - mae: 0.1249\nEpoch 1442: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3139 - mae: 0.3139 - val_loss: 0.1066 - val_mae: 0.1066\nEpoch 1443/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6860 - mae: 0.6860\nEpoch 1443: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3293 - mae: 0.3293 - val_loss: 0.1076 - val_mae: 0.1076\nEpoch 1444/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.0817 - mae: 0.0817\nEpoch 1444: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3750 - mae: 0.3750 - val_loss: 0.2210 - val_mae: 0.2210\nEpoch 1445/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5591 - mae: 0.5591\nEpoch 1445: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3954 - mae: 0.3954 - val_loss: 0.2033 - val_mae: 0.2033\nEpoch 1446/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2052 - mae: 0.2052\nEpoch 1446: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 9ms/step - loss: 0.3719 - mae: 0.3719 - val_loss: 0.1193 - val_mae: 0.1193\nEpoch 1447/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1886 - mae: 0.1886\nEpoch 1447: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3908 - mae: 0.3908 - val_loss: 0.2666 - val_mae: 0.2666\nEpoch 1448/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2497 - mae: 0.2497\nEpoch 1448: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3638 - mae: 0.3638 - val_loss: 0.2116 - val_mae: 0.2116\nEpoch 1449/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5252 - mae: 0.5252\nEpoch 1449: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3363 - mae: 0.3363 - val_loss: 0.1113 - val_mae: 0.1113\nEpoch 1450/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2633 - mae: 0.2633\nEpoch 1450: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 8ms/step - loss: 0.3173 - mae: 0.3173 - val_loss: 0.1185 - val_mae: 0.1185\nEpoch 1451/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2377 - mae: 0.2377\nEpoch 1451: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 9ms/step - loss: 0.3170 - mae: 0.3170 - val_loss: 0.1495 - val_mae: 0.1495\nEpoch 1452/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3048 - mae: 0.3048\nEpoch 1452: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3144 - mae: 0.3144 - val_loss: 0.1210 - val_mae: 0.1210\nEpoch 1453/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6585 - mae: 0.6585\nEpoch 1453: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3130 - mae: 0.3130 - val_loss: 0.1256 - val_mae: 0.1256\nEpoch 1454/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1779 - mae: 0.1779\nEpoch 1454: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3048 - mae: 0.3048 - val_loss: 0.2037 - val_mae: 0.2037\nEpoch 1455/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7008 - mae: 0.7008\nEpoch 1455: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3246 - mae: 0.3246 - val_loss: 0.1148 - val_mae: 0.1148\nEpoch 1456/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1086 - mae: 0.1086\nEpoch 1456: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 8ms/step - loss: 0.3077 - mae: 0.3077 - val_loss: 0.1055 - val_mae: 0.1055\nEpoch 1457/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5510 - mae: 0.5510\nEpoch 1457: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3081 - mae: 0.3081 - val_loss: 0.1114 - val_mae: 0.1114\nEpoch 1458/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2380 - mae: 0.2380\nEpoch 1458: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3379 - mae: 0.3379 - val_loss: 0.2914 - val_mae: 0.2914\nEpoch 1459/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2746 - mae: 0.2746\nEpoch 1459: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3769 - mae: 0.3769 - val_loss: 0.2888 - val_mae: 0.2888\nEpoch 1460/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3900 - mae: 0.3900\nEpoch 1460: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3517 - mae: 0.3517 - val_loss: 0.1317 - val_mae: 0.1317\nEpoch 1461/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1331 - mae: 0.1331\nEpoch 1461: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 9ms/step - loss: 0.3263 - mae: 0.3263 - val_loss: 0.1051 - val_mae: 0.1051\nEpoch 1462/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1200 - mae: 0.1200\nEpoch 1462: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 9ms/step - loss: 0.3066 - mae: 0.3066 - val_loss: 0.1307 - val_mae: 0.1307\nEpoch 1463/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7031 - mae: 0.7031\nEpoch 1463: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3237 - mae: 0.3237 - val_loss: 0.1586 - val_mae: 0.1586\nEpoch 1464/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2781 - mae: 0.2781\nEpoch 1464: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3262 - mae: 0.3262 - val_loss: 0.1325 - val_mae: 0.1325\nEpoch 1465/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6718 - mae: 0.6718\nEpoch 1465: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3129 - mae: 0.3129 - val_loss: 0.1043 - val_mae: 0.1043\nEpoch 1466/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4602 - mae: 0.4602\nEpoch 1466: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3040 - mae: 0.3040 - val_loss: 0.1167 - val_mae: 0.1167\nEpoch 1467/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1069 - mae: 0.1069\nEpoch 1467: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 10ms/step - loss: 0.3085 - mae: 0.3085 - val_loss: 0.1048 - val_mae: 0.1048\nEpoch 1468/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1060 - mae: 0.1060\nEpoch 1468: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3092 - mae: 0.3092 - val_loss: 0.1003 - val_mae: 0.1003\nEpoch 1469/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1393 - mae: 0.1393\nEpoch 1469: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3077 - mae: 0.3077 - val_loss: 0.1476 - val_mae: 0.1476\nEpoch 1470/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.6681 - mae: 0.6681\nEpoch 1470: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3355 - mae: 0.3355 - val_loss: 0.1785 - val_mae: 0.1785\nEpoch 1471/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.9731 - mae: 0.9731\nEpoch 1471: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3709 - mae: 0.3709 - val_loss: 0.1089 - val_mae: 0.1089\nEpoch 1472/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4054 - mae: 0.4054\nEpoch 1472: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3540 - mae: 0.3540 - val_loss: 0.1887 - val_mae: 0.1887\nEpoch 1473/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2276 - mae: 0.2276\nEpoch 1473: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3654 - mae: 0.3654 - val_loss: 0.1863 - val_mae: 0.1863\nEpoch 1474/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2209 - mae: 0.2209\nEpoch 1474: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3526 - mae: 0.3526 - val_loss: 0.3022 - val_mae: 0.3022\nEpoch 1475/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3895 - mae: 0.3895\nEpoch 1475: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.4352 - mae: 0.4352 - val_loss: 0.2685 - val_mae: 0.2685\nEpoch 1476/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2190 - mae: 0.2190\nEpoch 1476: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3642 - mae: 0.3642 - val_loss: 0.2421 - val_mae: 0.2421\nEpoch 1477/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3461 - mae: 0.3461\nEpoch 1477: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3368 - mae: 0.3368 - val_loss: 0.1721 - val_mae: 0.1721\nEpoch 1478/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1979 - mae: 0.1979\nEpoch 1478: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 9ms/step - loss: 0.3408 - mae: 0.3408 - val_loss: 0.1263 - val_mae: 0.1263\nEpoch 1479/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1598 - mae: 0.1598\nEpoch 1479: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3176 - mae: 0.3176 - val_loss: 0.1330 - val_mae: 0.1330\nEpoch 1480/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4643 - mae: 0.4643\nEpoch 1480: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3059 - mae: 0.3059 - val_loss: 0.1035 - val_mae: 0.1035\nEpoch 1481/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3965 - mae: 0.3965\nEpoch 1481: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3198 - mae: 0.3198 - val_loss: 0.1288 - val_mae: 0.1288\nEpoch 1482/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5496 - mae: 0.5496\nEpoch 1482: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3147 - mae: 0.3147 - val_loss: 0.1956 - val_mae: 0.1956\nEpoch 1483/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.2078 - mae: 0.2078\nEpoch 1483: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3536 - mae: 0.3536 - val_loss: 0.1034 - val_mae: 0.1034\nEpoch 1484/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1150 - mae: 0.1150\nEpoch 1484: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3192 - mae: 0.3192 - val_loss: 0.1177 - val_mae: 0.1177\nEpoch 1485/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1499 - mae: 0.1499\nEpoch 1485: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3033 - mae: 0.3033 - val_loss: 0.1041 - val_mae: 0.1041\nEpoch 1486/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3997 - mae: 0.3997\nEpoch 1486: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3066 - mae: 0.3066 - val_loss: 0.1089 - val_mae: 0.1089\nEpoch 1487/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4010 - mae: 0.4010\nEpoch 1487: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3075 - mae: 0.3075 - val_loss: 0.1250 - val_mae: 0.1250\nEpoch 1488/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1257 - mae: 0.1257\nEpoch 1488: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 6ms/step - loss: 0.3093 - mae: 0.3093 - val_loss: 0.1183 - val_mae: 0.1183\nEpoch 1489/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1511 - mae: 0.1511\nEpoch 1489: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 6ms/step - loss: 0.3186 - mae: 0.3186 - val_loss: 0.1214 - val_mae: 0.1214\nEpoch 1490/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1119 - mae: 0.1119\nEpoch 1490: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3039 - mae: 0.3039 - val_loss: 0.1293 - val_mae: 0.1293\nEpoch 1491/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1464 - mae: 0.1464\nEpoch 1491: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3196 - mae: 0.3196 - val_loss: 0.1431 - val_mae: 0.1431\nEpoch 1492/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4440 - mae: 0.4440\nEpoch 1492: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 6ms/step - loss: 0.3169 - mae: 0.3169 - val_loss: 0.1107 - val_mae: 0.1107\nEpoch 1493/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.5325 - mae: 0.5325\nEpoch 1493: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3411 - mae: 0.3411 - val_loss: 0.2264 - val_mae: 0.2264\nEpoch 1494/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.4439 - mae: 0.4439\nEpoch 1494: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3376 - mae: 0.3376 - val_loss: 0.1338 - val_mae: 0.1338\nEpoch 1495/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1705 - mae: 0.1705\nEpoch 1495: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3256 - mae: 0.3256 - val_loss: 0.1032 - val_mae: 0.1032\nEpoch 1496/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1389 - mae: 0.1389\nEpoch 1496: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 10ms/step - loss: 0.3258 - mae: 0.3258 - val_loss: 0.2167 - val_mae: 0.2167\nEpoch 1497/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3126 - mae: 0.3126\nEpoch 1497: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3285 - mae: 0.3285 - val_loss: 0.1161 - val_mae: 0.1161\nEpoch 1498/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.7136 - mae: 0.7136\nEpoch 1498: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 8ms/step - loss: 0.3156 - mae: 0.3156 - val_loss: 0.1269 - val_mae: 0.1269\nEpoch 1499/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.1620 - mae: 0.1620\nEpoch 1499: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3066 - mae: 0.3066 - val_loss: 0.1068 - val_mae: 0.1068\nEpoch 1500/1500\n1/7 [===>..........................] - ETA: 0s - loss: 0.3449 - mae: 0.3449\nEpoch 1500: val_loss did not improve from 0.09617\n7/7 [==============================] - 0s 7ms/step - loss: 0.3036 - mae: 0.3036 - val_loss: 0.1017 - val_mae: 0.1017\n",
          "output_type": "stream"
        },
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7f82bc452440>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training, the code evaluates the model's performance on the test data, calculating the loss (model_dn_loss) and MAE (model_dn_mae)."
      ],
      "metadata": {
        "id": "kJRfYCxTJzvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaulate the model\n",
        "model_dn_loss, model_dn_mae = model_dn.evaluate(X_test_normal, y_test)"
      ],
      "metadata": {
        "id": "U65oJPceHU2p",
        "outputId": "0e4d3e43-24ae-474f-908e-f6912e2f8d6e",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:08:25.152760Z",
          "iopub.execute_input": "2023-07-10T10:08:25.153205Z",
          "iopub.status.idle": "2023-07-10T10:08:25.236490Z",
          "shell.execute_reply.started": "2023-07-10T10:08:25.153167Z",
          "shell.execute_reply": "2023-07-10T10:08:25.235657Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "2/2 [==============================] - 0s 5ms/step - loss: 0.1017 - mae: 0.1017\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next code block uses the trained model to make predictions on the test data (X_test_normal) and stores the predictions in test_pred."
      ],
      "metadata": {
        "id": "ntqxuGJvJ1ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = model_dn.predict(X_test_normal)\n",
        "test_pred"
      ],
      "metadata": {
        "id": "3NWsWKrvHU2p",
        "outputId": "f5849bb4-8909-4468-eafb-0b6b5184efaf",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:08:25.238118Z",
          "iopub.execute_input": "2023-07-10T10:08:25.239102Z",
          "iopub.status.idle": "2023-07-10T10:08:25.407365Z",
          "shell.execute_reply.started": "2023-07-10T10:08:25.239065Z",
          "shell.execute_reply": "2023-07-10T10:08:25.406216Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "2/2 [==============================] - 0s 4ms/step\n",
          "output_type": "stream"
        },
        {
          "execution_count": 17,
          "output_type": "execute_result",
          "data": {
            "text/plain": "array([[19.292742 ],\n       [19.300455 ],\n       [27.855587 ],\n       [20.52618  ],\n       [16.692194 ],\n       [12.005781 ],\n       [23.082802 ],\n       [18.69486  ],\n       [11.652883 ],\n       [12.030577 ],\n       [26.032667 ],\n       [24.40135  ],\n       [14.8353   ],\n       [22.425278 ],\n       [ 6.141244 ],\n       [ 4.9527087],\n       [21.95617  ],\n       [20.89751  ],\n       [20.371483 ],\n       [13.989289 ],\n       [14.917675 ],\n       [16.650465 ],\n       [13.882806 ],\n       [13.783384 ],\n       [21.409826 ],\n       [30.189026 ],\n       [23.5406   ],\n       [15.029769 ],\n       [ 6.867806 ],\n       [13.0579815],\n       [24.90806  ],\n       [ 9.524882 ],\n       [17.643507 ],\n       [18.449133 ],\n       [18.810421 ],\n       [ 3.29669  ],\n       [21.462135 ],\n       [16.072733 ],\n       [16.583387 ],\n       [11.511883 ],\n       [13.745917 ],\n       [23.576315 ],\n       [30.817448 ],\n       [ 9.23029  ],\n       [13.877988 ],\n       [22.46339  ],\n       [28.833933 ],\n       [21.439268 ],\n       [23.289568 ],\n       [ 9.858039 ],\n       [34.95683  ]], dtype=float32)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape, test_pred.shape, tf.squeeze(test_pred).shape"
      ],
      "metadata": {
        "id": "2dvLunp-HU2q",
        "outputId": "0749c6ad-60db-41bc-ba01-7ff516d94626",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:08:25.409147Z",
          "iopub.execute_input": "2023-07-10T10:08:25.409592Z",
          "iopub.status.idle": "2023-07-10T10:08:25.418116Z",
          "shell.execute_reply.started": "2023-07-10T10:08:25.409554Z",
          "shell.execute_reply": "2023-07-10T10:08:25.417016Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 18,
          "output_type": "execute_result",
          "data": {
            "text/plain": "((51,), (51, 1), TensorShape([51]))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code defines two utility functions: mean_absolute_scaled_error and evaluate_preds. The mean_absolute_scaled_error function calculates the mean absolute scaled error metric. The evaluate_preds function calculates various evaluation metrics, including MAE, MSE, RMSE, MAPE, MASE, and R-squared score."
      ],
      "metadata": {
        "id": "3hsk8AjoJ4Dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MASE implemented courtesy of sktime - https://github.com/alan-turing-institute/sktime/blob/ee7a06843a44f4aaec7582d847e36073a9ab0566/sktime/performance_metrics/forecasting/_functions.py#L16\n",
        "def mean_absolute_scaled_error(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Implement MASE (assuming no seasonality of data).\n",
        "  \"\"\"\n",
        "  mae = tf.reduce_mean(tf.abs(y_true - y_pred))\n",
        "\n",
        "  # Find MAE of naive forecast (no seasonality)\n",
        "  mae_naive_no_season = tf.reduce_mean(tf.abs(y_true[1:] - y_true[:-1])) # our seasonality is 1 day (hence the shifting of 1 day)\n",
        "\n",
        "  return mae / mae_naive_no_season\n",
        "\n",
        "def evaluate_preds(y_true, y_pred):\n",
        "    # Make sure float32 (for metric calculations)\n",
        "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    # Calculate various metrics\n",
        "    mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
        "    mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)\n",
        "    rmse = tf.sqrt(mse)\n",
        "    mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
        "    mase = mean_absolute_scaled_error(y_true, y_pred)\n",
        "\n",
        "    # Calculate R-squared score\n",
        "    ss_res = tf.reduce_sum((y_true - y_pred)**2)\n",
        "    ss_tot = tf.reduce_sum((y_true - tf.reduce_mean(y_true))**2)\n",
        "    r2 = 1 - (ss_res / ss_tot)\n",
        "\n",
        "    # Account for different sized metrics\n",
        "    if mae.ndim > 0:\n",
        "        mae = tf.reduce_mean(mae)\n",
        "        mse = tf.reduce_mean(mse)\n",
        "        rmse = tf.reduce_mean(rmse)\n",
        "        mape = tf.reduce_mean(mape)\n",
        "        mase = tf.reduce_mean(mase)\n",
        "        r2 = tf.reduce_mean(r2)\n",
        "\n",
        "    return {\"mae\": mae.numpy(),\n",
        "            \"mse\": mse.numpy(),\n",
        "            \"rmse\": rmse.numpy(),\n",
        "            \"mape\": str(mape.numpy()) + \"%\",\n",
        "            \"mase\": mase.numpy(),\n",
        "            \"r2\": r2.numpy()}"
      ],
      "metadata": {
        "id": "Kw-ZjmQlHU2q",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:08:25.420126Z",
          "iopub.execute_input": "2023-07-10T10:08:25.420558Z",
          "iopub.status.idle": "2023-07-10T10:08:25.436401Z",
          "shell.execute_reply.started": "2023-07-10T10:08:25.420519Z",
          "shell.execute_reply": "2023-07-10T10:08:25.434859Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape, test_pred.shape"
      ],
      "metadata": {
        "id": "2kFhGegDHU2q",
        "outputId": "68bcd3f7-99a8-4567-f5e7-bdde50d1517d",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:08:25.438418Z",
          "iopub.execute_input": "2023-07-10T10:08:25.438951Z",
          "iopub.status.idle": "2023-07-10T10:08:25.453837Z",
          "shell.execute_reply.started": "2023-07-10T10:08:25.438912Z",
          "shell.execute_reply": "2023-07-10T10:08:25.452530Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 20,
          "output_type": "execute_result",
          "data": {
            "text/plain": "((51,), (51, 1))"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure y_test and test_pred have the same shape"
      ],
      "metadata": {
        "id": "OA54bamjJ8D7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = tf.squeeze(y_test)\n",
        "test_pred = tf.squeeze(test_pred)"
      ],
      "metadata": {
        "id": "U3Ufy6wrHU2q",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:08:25.456010Z",
          "iopub.execute_input": "2023-07-10T10:08:25.456478Z",
          "iopub.status.idle": "2023-07-10T10:08:25.466332Z",
          "shell.execute_reply.started": "2023-07-10T10:08:25.456445Z",
          "shell.execute_reply": "2023-07-10T10:08:25.465451Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code applies these utility functions to the test data (y_test and test_pred), generating a dictionary of evaluation metrics."
      ],
      "metadata": {
        "id": "Jpq5M1XCJ7Lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_preds(y_test,test_pred)"
      ],
      "metadata": {
        "id": "JVO_abF3HU2r",
        "outputId": "52c8ec36-3415-4e4c-82b6-0715dcea8ff2",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:08:25.467482Z",
          "iopub.execute_input": "2023-07-10T10:08:25.468513Z",
          "iopub.status.idle": "2023-07-10T10:08:25.504105Z",
          "shell.execute_reply.started": "2023-07-10T10:08:25.468470Z",
          "shell.execute_reply": "2023-07-10T10:08:25.503195Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 22,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'mae': 0.101748876,\n 'mse': 0.022398995,\n 'rmse': 0.14966294,\n 'mape': '0.8678263%',\n 'mase': 0.013991868,\n 'r2': 0.9995185}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the subsequent code block, the actual and predicted values are formatted into a DataFrame, dr, for further analysis and comparison."
      ],
      "metadata": {
        "id": "LesJSW7LKB9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actual_ys = y_test\n",
        "predicted_ys = model_dn.predict(X_test_normal)\n",
        "predicted_ys = predicted_ys.flatten()\n",
        "\n",
        "data = {f'feature_{i}': X_test_normal[:, i] for i in range(X_test_normal.shape[1])}\n",
        "data['Actual Y'] = actual_ys\n",
        "data['Predicted Y'] = predicted_ys\n",
        "\n",
        "dr = pd.DataFrame(data)\n",
        "dr"
      ],
      "metadata": {
        "id": "sjTAMOnRHU2r",
        "outputId": "e1378b4f-ae36-456f-8599-410298974ba1",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:08:25.505148Z",
          "iopub.execute_input": "2023-07-10T10:08:25.505474Z",
          "iopub.status.idle": "2023-07-10T10:08:25.620643Z",
          "shell.execute_reply.started": "2023-07-10T10:08:25.505447Z",
          "shell.execute_reply": "2023-07-10T10:08:25.619549Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "2/2 [==============================] - 0s 3ms/step\n",
          "output_type": "stream"
        },
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "    feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n0    0.525900   0.220339   0.682842   0.917098   0.734375   0.575510   \n1    0.525900   0.067797   0.433276   0.834197   0.414062   0.526531   \n2    0.355575   0.355932   0.448873   0.839378   0.468750   0.585714   \n3    0.501317   0.220339   0.407279   0.860104   0.570312   0.432653   \n4    0.576822   0.305085   0.273830   0.823834   0.406250   0.361224   \n5    0.669886   0.305085   0.282496   0.834197   0.328125   0.265306   \n6    0.449517   0.355932   0.362218   0.787565   0.492188   0.497959   \n7    0.538191   0.677966   0.348354   0.787565   0.562500   0.518367   \n8    0.677788   0.016949   0.552860   0.911917   0.859375   0.414286   \n9    0.672520   0.169492   0.440208   0.917098   0.593750   0.432653   \n10   0.391572   0.677966   0.675910   0.906736   0.804688   0.673469   \n11   0.424056   0.508475   0.559792   0.875648   0.648438   0.561224   \n12   0.614574   0.559322   0.353553   0.803109   0.476563   0.457143   \n13   0.462687   0.152542   0.407279   0.870466   0.398438   0.444898   \n14   0.787533   0.542373   0.254766   0.823834   0.500000   0.204082   \n15   0.810360   0.050847   0.175043   0.891192   0.320313   0.265306   \n16   0.472344   0.338983   0.261698   0.818653   0.343750   0.377551   \n17   0.493415   0.220339   0.306759   0.756477   0.414062   0.404082   \n18   0.503951   0.440678   0.383016   0.880829   0.460938   0.404082   \n19   0.630378   0.101695   0.227036   0.792746   0.265625   0.222449   \n20   0.612818   0.847458   0.272097   0.782383   0.515625   0.371429   \n21   0.579456   0.186441   0.646447   0.911917   0.695312   0.548980   \n22   0.633011   0.491525   0.419411   0.880829   0.773438   0.406122   \n23   0.635645   0.474576   0.294627   0.766839   0.515625   0.400000   \n24   0.484636   0.322034   0.693241   0.860104   0.679688   0.661224   \n25   0.310799   0.745763   0.802426   0.880829   0.804688   0.824490   \n26   0.441615   0.322034   0.792028   0.927461   0.843750   0.779592   \n27   0.611062   0.525424   0.249567   0.823834   0.507812   0.297959   \n28   0.772608   0.067797   0.469671   0.932642   0.578125   0.453061   \n29   0.649693   0.186441   0.455806   0.813472   0.750000   0.400000   \n30   0.414399   0.305085   0.403813   0.860104   0.492188   0.393878   \n31   0.719930   0.271186   0.487002   0.906736   0.500000   0.404082   \n32   0.559263   0.406780   0.336222   0.777202   0.429688   0.442857   \n33   0.542581   0.711864   0.497400   0.896373   0.640625   0.485714   \n34   0.536435   0.474576   0.528596   0.854922   0.617188   0.497959   \n35   0.843723   0.084746   0.282496   0.870466   0.359375   0.210204   \n36   0.482880   0.305085   0.346620   0.823834   0.242188   0.377551   \n37   0.589991   0.101695   0.452340   0.792746   0.539062   0.561224   \n38   0.579456   0.372881   0.625650   0.901554   0.843750   0.536735   \n39   0.680421   0.305085   0.188908   0.782383   0.343750   0.330612   \n40   0.634767   0.559322   0.251300   0.870466   0.453125   0.328571   \n41   0.440737   0.423729   0.544194   0.906736   0.523438   0.495918   \n42   0.295874   0.101695   0.604853   0.818653   0.578125   0.536735   \n43   0.724320   0.016949   0.285962   0.886010   0.343750   0.261224   \n44   0.632133   0.355932   0.317158   0.906736   0.359375   0.353061   \n45   0.462687   0.271186   0.476603   0.823834   0.539062   0.477551   \n46   0.336260   0.203390   0.535529   0.860104   0.609375   0.461224   \n47   0.482002   0.542373   0.228769   0.854922   0.351562   0.218367   \n48   0.446883   0.508475   0.336222   0.792746   0.500000   0.477551   \n49   0.713784   0.254237   0.185442   0.823834   0.382812   0.357143   \n50   0.221247   0.406780   1.696014   0.886010   1.570313   1.161224   \n\n    feature_6  feature_7  feature_8  feature_9  feature_10  feature_11  \\\n0    0.452465   0.480296   0.647059   0.638462    0.445205    0.811189   \n1    0.375000   0.376847   0.411765   0.407692    0.260274    0.496503   \n2    0.626761   0.443350   0.606618   0.538462    0.308219    0.608392   \n3    0.367958   0.337438   0.389706   0.330769    0.226027    0.433566   \n4    0.302817   0.187192   0.319853   0.253846    0.205479    0.349650   \n5    0.306338   0.273399   0.397059   0.492308    0.246575    0.363636   \n6    0.357394   0.226601   0.415441   0.461538    0.342466    0.629371   \n7    0.461268   0.214286   0.264706   0.315385    0.246575    0.384615   \n8    0.338028   0.470443   0.584559   0.669231    0.404110    0.755245   \n9    0.339789   0.364532   0.378676   0.438462    1.013699    0.538462   \n10   0.623239   0.445813   0.529412   0.607692    0.273973    0.797203   \n11   0.538732   0.492611   0.613971   0.476923    0.260274    0.762238   \n12   0.382042   0.298030   0.345588   0.423077    0.239726    0.601399   \n13   0.404930   0.352217   0.448529   0.461538    0.376712    0.370629   \n14   0.158451   0.273399   0.323529   0.338462    0.226027    0.545455   \n15   0.125000   0.174877   0.172794   0.207692    0.198630    0.069930   \n16   0.292254   0.275862   0.386029   0.430769    0.335616    0.447552   \n17   0.411972   0.349754   0.584559   0.438462    0.178082    0.440559   \n18   0.397887   0.327586   0.444853   0.415385    0.226027    0.216783   \n19   0.121479   0.266010   0.411765   0.192308    0.260274    0.440559   \n20   0.329225   0.293103   0.367647   0.361538    0.184932    0.545455   \n21   0.547535   0.591133   0.683824   0.584615    0.335616    0.860140   \n22   0.362676   0.280788   0.352941   0.446154    0.287671    0.601399   \n23   0.258803   0.221675   0.415441   0.276923    0.301370    0.671329   \n24   0.547535   0.573892   0.731618   0.861538    0.417808    0.888112   \n25   0.697183   0.593596   0.606618   0.723077    0.376712    0.755245   \n26   0.702465   0.586207   0.753676   0.638462    0.383562    0.867133   \n27   0.339789   0.233990   0.238971   0.246154    0.198630    0.258741   \n28   0.250000   0.325123   0.459559   0.515385    0.417808    0.559441   \n29   0.399648   0.455665   0.617647   0.330769    0.301370    0.608392   \n30   0.417254   0.408867   0.437500   0.507692    0.171233    0.419580   \n31   0.390845   0.428571   0.492647   0.492308    0.260274    0.475524   \n32   0.360915   0.369458   0.496324   0.230769    0.191781    0.755245   \n33   0.500000   0.359606   0.430147   0.461538    0.267123    0.664336   \n34   0.496479   0.472906   0.470588   0.607692    0.438356    0.552448   \n35   0.181338   0.283251   0.286765   0.284615    0.232877    0.356643   \n36   0.403169   0.384236   0.378676   0.292308    0.253425    0.510490   \n37   0.355634   0.428571   0.625000   0.438462    0.260274    0.867133   \n38   0.473592   0.418719   0.591912   0.523077    0.342466    0.874126   \n39   0.250000   0.162562   0.253676   0.246154    0.184932    0.461538   \n40   0.302817   0.167488   0.261029   0.184615    0.164384    0.559441   \n41   0.535211   0.448276   0.514706   0.392308    0.239726    0.608392   \n42   0.626761   0.527094   0.786765   0.538462    0.417808    0.727273   \n43   0.135563   0.219212   0.327206   0.238462    0.246575    0.398601   \n44   0.213028   0.300493   0.411765   0.400000    0.294521    0.342657   \n45   0.410211   0.416256   0.643382   0.500000    0.383562    0.699301   \n46   0.475352   0.497537   0.647059   0.600000    0.273973    0.797203   \n47   0.255282   0.219212   0.286765   0.238462    0.178082    0.335664   \n48   0.380282   0.342365   0.363971   0.284615    0.219178    0.475524   \n49   0.176056   0.103448   0.113971   0.138462    0.212329    0.433566   \n50   1.385563   1.544335   1.474265   1.238462    0.719178    1.412587   \n\n    feature_12  feature_13  Actual Y  Predicted Y  \n0     0.913669    0.642857      19.2    19.292742  \n1     0.489209    0.339286      19.2    19.300455  \n2     0.489209    0.285714      28.0    27.855587  \n3     0.553957    0.339286      20.5    20.526180  \n4     0.381295    0.267857      16.7    16.692194  \n5     0.388489    0.285714      12.1    12.005781  \n6     0.561151    0.535714      23.6    23.082802  \n7     0.381295    0.446429      18.6    18.694860  \n8     0.647482    0.607143      11.7    11.652883  \n9     0.482014    0.464286      11.9    12.030577  \n10    0.776978    0.785714      26.1    26.032667  \n11    0.676259    0.607143      24.5    24.401350  \n12    0.597122    0.535714      14.8    14.835300  \n13    0.517986    0.428571      22.5    22.425278  \n14    0.510791    0.410714       6.3     6.141244  \n15    0.302158    0.196429       5.3     4.952709  \n16    0.453237    0.285714      22.0    21.956169  \n17    0.388489    0.196429      20.9    20.897511  \n18    0.374101    0.214286      20.4    20.371483  \n19    0.503597    0.321429      14.0    13.989289  \n20    0.503597    0.535714      14.9    14.917675  \n21    0.654676    0.428571      16.5    16.650465  \n22    0.633094    0.660714      13.9    13.882806  \n23    0.589928    0.392857      13.8    13.783384  \n24    0.755396    0.517857      21.3    21.409826  \n25    0.697842    0.660714      30.4    30.189026  \n26    0.776978    0.750000      23.6    23.540600  \n27    0.338129    0.232143      15.0    15.029769  \n28    0.604317    0.482143       7.1     6.867806  \n29    0.690647    0.696429      13.0    13.057981  \n30    0.496403    0.142857      24.9    24.908060  \n31    0.654676    0.482143       9.6     9.524882  \n32    0.661871    0.321429      17.5    17.643507  \n33    0.618705    0.571429      18.4    18.449133  \n34    0.647482    0.571429      18.7    18.810421  \n35    0.517986    0.339286       3.7     3.296690  \n36    0.359712    0.267857      21.4    21.462135  \n37    0.683453    0.482143      16.0    16.072733  \n38    0.151079    0.642857      16.6    16.583387  \n39    0.525180    0.250000      11.5    11.511883  \n40    0.460432    0.517857      13.8    13.745917  \n41    0.546763    0.375000      23.6    23.576315  \n42    0.697842    0.589286      31.2    30.817448  \n43    0.446043    0.428571       9.4     9.230290  \n44    0.460432    0.446429      13.9    13.877988  \n45    0.669065    0.410714      22.5    22.463390  \n46    0.705036    0.267857      29.0    28.833933  \n47    0.460432    0.285714      21.5    21.439268  \n48    0.467626    0.375000      23.3    23.289568  \n49    0.424460    0.196429       9.9     9.858039  \n50    0.575540    1.000000      35.2    34.956829  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0</th>\n      <th>feature_1</th>\n      <th>feature_2</th>\n      <th>feature_3</th>\n      <th>feature_4</th>\n      <th>feature_5</th>\n      <th>feature_6</th>\n      <th>feature_7</th>\n      <th>feature_8</th>\n      <th>feature_9</th>\n      <th>feature_10</th>\n      <th>feature_11</th>\n      <th>feature_12</th>\n      <th>feature_13</th>\n      <th>Actual Y</th>\n      <th>Predicted Y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.525900</td>\n      <td>0.220339</td>\n      <td>0.682842</td>\n      <td>0.917098</td>\n      <td>0.734375</td>\n      <td>0.575510</td>\n      <td>0.452465</td>\n      <td>0.480296</td>\n      <td>0.647059</td>\n      <td>0.638462</td>\n      <td>0.445205</td>\n      <td>0.811189</td>\n      <td>0.913669</td>\n      <td>0.642857</td>\n      <td>19.2</td>\n      <td>19.292742</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.525900</td>\n      <td>0.067797</td>\n      <td>0.433276</td>\n      <td>0.834197</td>\n      <td>0.414062</td>\n      <td>0.526531</td>\n      <td>0.375000</td>\n      <td>0.376847</td>\n      <td>0.411765</td>\n      <td>0.407692</td>\n      <td>0.260274</td>\n      <td>0.496503</td>\n      <td>0.489209</td>\n      <td>0.339286</td>\n      <td>19.2</td>\n      <td>19.300455</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.355575</td>\n      <td>0.355932</td>\n      <td>0.448873</td>\n      <td>0.839378</td>\n      <td>0.468750</td>\n      <td>0.585714</td>\n      <td>0.626761</td>\n      <td>0.443350</td>\n      <td>0.606618</td>\n      <td>0.538462</td>\n      <td>0.308219</td>\n      <td>0.608392</td>\n      <td>0.489209</td>\n      <td>0.285714</td>\n      <td>28.0</td>\n      <td>27.855587</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.501317</td>\n      <td>0.220339</td>\n      <td>0.407279</td>\n      <td>0.860104</td>\n      <td>0.570312</td>\n      <td>0.432653</td>\n      <td>0.367958</td>\n      <td>0.337438</td>\n      <td>0.389706</td>\n      <td>0.330769</td>\n      <td>0.226027</td>\n      <td>0.433566</td>\n      <td>0.553957</td>\n      <td>0.339286</td>\n      <td>20.5</td>\n      <td>20.526180</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.576822</td>\n      <td>0.305085</td>\n      <td>0.273830</td>\n      <td>0.823834</td>\n      <td>0.406250</td>\n      <td>0.361224</td>\n      <td>0.302817</td>\n      <td>0.187192</td>\n      <td>0.319853</td>\n      <td>0.253846</td>\n      <td>0.205479</td>\n      <td>0.349650</td>\n      <td>0.381295</td>\n      <td>0.267857</td>\n      <td>16.7</td>\n      <td>16.692194</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.669886</td>\n      <td>0.305085</td>\n      <td>0.282496</td>\n      <td>0.834197</td>\n      <td>0.328125</td>\n      <td>0.265306</td>\n      <td>0.306338</td>\n      <td>0.273399</td>\n      <td>0.397059</td>\n      <td>0.492308</td>\n      <td>0.246575</td>\n      <td>0.363636</td>\n      <td>0.388489</td>\n      <td>0.285714</td>\n      <td>12.1</td>\n      <td>12.005781</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.449517</td>\n      <td>0.355932</td>\n      <td>0.362218</td>\n      <td>0.787565</td>\n      <td>0.492188</td>\n      <td>0.497959</td>\n      <td>0.357394</td>\n      <td>0.226601</td>\n      <td>0.415441</td>\n      <td>0.461538</td>\n      <td>0.342466</td>\n      <td>0.629371</td>\n      <td>0.561151</td>\n      <td>0.535714</td>\n      <td>23.6</td>\n      <td>23.082802</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.538191</td>\n      <td>0.677966</td>\n      <td>0.348354</td>\n      <td>0.787565</td>\n      <td>0.562500</td>\n      <td>0.518367</td>\n      <td>0.461268</td>\n      <td>0.214286</td>\n      <td>0.264706</td>\n      <td>0.315385</td>\n      <td>0.246575</td>\n      <td>0.384615</td>\n      <td>0.381295</td>\n      <td>0.446429</td>\n      <td>18.6</td>\n      <td>18.694860</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.677788</td>\n      <td>0.016949</td>\n      <td>0.552860</td>\n      <td>0.911917</td>\n      <td>0.859375</td>\n      <td>0.414286</td>\n      <td>0.338028</td>\n      <td>0.470443</td>\n      <td>0.584559</td>\n      <td>0.669231</td>\n      <td>0.404110</td>\n      <td>0.755245</td>\n      <td>0.647482</td>\n      <td>0.607143</td>\n      <td>11.7</td>\n      <td>11.652883</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.672520</td>\n      <td>0.169492</td>\n      <td>0.440208</td>\n      <td>0.917098</td>\n      <td>0.593750</td>\n      <td>0.432653</td>\n      <td>0.339789</td>\n      <td>0.364532</td>\n      <td>0.378676</td>\n      <td>0.438462</td>\n      <td>1.013699</td>\n      <td>0.538462</td>\n      <td>0.482014</td>\n      <td>0.464286</td>\n      <td>11.9</td>\n      <td>12.030577</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.391572</td>\n      <td>0.677966</td>\n      <td>0.675910</td>\n      <td>0.906736</td>\n      <td>0.804688</td>\n      <td>0.673469</td>\n      <td>0.623239</td>\n      <td>0.445813</td>\n      <td>0.529412</td>\n      <td>0.607692</td>\n      <td>0.273973</td>\n      <td>0.797203</td>\n      <td>0.776978</td>\n      <td>0.785714</td>\n      <td>26.1</td>\n      <td>26.032667</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.424056</td>\n      <td>0.508475</td>\n      <td>0.559792</td>\n      <td>0.875648</td>\n      <td>0.648438</td>\n      <td>0.561224</td>\n      <td>0.538732</td>\n      <td>0.492611</td>\n      <td>0.613971</td>\n      <td>0.476923</td>\n      <td>0.260274</td>\n      <td>0.762238</td>\n      <td>0.676259</td>\n      <td>0.607143</td>\n      <td>24.5</td>\n      <td>24.401350</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.614574</td>\n      <td>0.559322</td>\n      <td>0.353553</td>\n      <td>0.803109</td>\n      <td>0.476563</td>\n      <td>0.457143</td>\n      <td>0.382042</td>\n      <td>0.298030</td>\n      <td>0.345588</td>\n      <td>0.423077</td>\n      <td>0.239726</td>\n      <td>0.601399</td>\n      <td>0.597122</td>\n      <td>0.535714</td>\n      <td>14.8</td>\n      <td>14.835300</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.462687</td>\n      <td>0.152542</td>\n      <td>0.407279</td>\n      <td>0.870466</td>\n      <td>0.398438</td>\n      <td>0.444898</td>\n      <td>0.404930</td>\n      <td>0.352217</td>\n      <td>0.448529</td>\n      <td>0.461538</td>\n      <td>0.376712</td>\n      <td>0.370629</td>\n      <td>0.517986</td>\n      <td>0.428571</td>\n      <td>22.5</td>\n      <td>22.425278</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.787533</td>\n      <td>0.542373</td>\n      <td>0.254766</td>\n      <td>0.823834</td>\n      <td>0.500000</td>\n      <td>0.204082</td>\n      <td>0.158451</td>\n      <td>0.273399</td>\n      <td>0.323529</td>\n      <td>0.338462</td>\n      <td>0.226027</td>\n      <td>0.545455</td>\n      <td>0.510791</td>\n      <td>0.410714</td>\n      <td>6.3</td>\n      <td>6.141244</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.810360</td>\n      <td>0.050847</td>\n      <td>0.175043</td>\n      <td>0.891192</td>\n      <td>0.320313</td>\n      <td>0.265306</td>\n      <td>0.125000</td>\n      <td>0.174877</td>\n      <td>0.172794</td>\n      <td>0.207692</td>\n      <td>0.198630</td>\n      <td>0.069930</td>\n      <td>0.302158</td>\n      <td>0.196429</td>\n      <td>5.3</td>\n      <td>4.952709</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.472344</td>\n      <td>0.338983</td>\n      <td>0.261698</td>\n      <td>0.818653</td>\n      <td>0.343750</td>\n      <td>0.377551</td>\n      <td>0.292254</td>\n      <td>0.275862</td>\n      <td>0.386029</td>\n      <td>0.430769</td>\n      <td>0.335616</td>\n      <td>0.447552</td>\n      <td>0.453237</td>\n      <td>0.285714</td>\n      <td>22.0</td>\n      <td>21.956169</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.493415</td>\n      <td>0.220339</td>\n      <td>0.306759</td>\n      <td>0.756477</td>\n      <td>0.414062</td>\n      <td>0.404082</td>\n      <td>0.411972</td>\n      <td>0.349754</td>\n      <td>0.584559</td>\n      <td>0.438462</td>\n      <td>0.178082</td>\n      <td>0.440559</td>\n      <td>0.388489</td>\n      <td>0.196429</td>\n      <td>20.9</td>\n      <td>20.897511</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.503951</td>\n      <td>0.440678</td>\n      <td>0.383016</td>\n      <td>0.880829</td>\n      <td>0.460938</td>\n      <td>0.404082</td>\n      <td>0.397887</td>\n      <td>0.327586</td>\n      <td>0.444853</td>\n      <td>0.415385</td>\n      <td>0.226027</td>\n      <td>0.216783</td>\n      <td>0.374101</td>\n      <td>0.214286</td>\n      <td>20.4</td>\n      <td>20.371483</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.630378</td>\n      <td>0.101695</td>\n      <td>0.227036</td>\n      <td>0.792746</td>\n      <td>0.265625</td>\n      <td>0.222449</td>\n      <td>0.121479</td>\n      <td>0.266010</td>\n      <td>0.411765</td>\n      <td>0.192308</td>\n      <td>0.260274</td>\n      <td>0.440559</td>\n      <td>0.503597</td>\n      <td>0.321429</td>\n      <td>14.0</td>\n      <td>13.989289</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.612818</td>\n      <td>0.847458</td>\n      <td>0.272097</td>\n      <td>0.782383</td>\n      <td>0.515625</td>\n      <td>0.371429</td>\n      <td>0.329225</td>\n      <td>0.293103</td>\n      <td>0.367647</td>\n      <td>0.361538</td>\n      <td>0.184932</td>\n      <td>0.545455</td>\n      <td>0.503597</td>\n      <td>0.535714</td>\n      <td>14.9</td>\n      <td>14.917675</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.579456</td>\n      <td>0.186441</td>\n      <td>0.646447</td>\n      <td>0.911917</td>\n      <td>0.695312</td>\n      <td>0.548980</td>\n      <td>0.547535</td>\n      <td>0.591133</td>\n      <td>0.683824</td>\n      <td>0.584615</td>\n      <td>0.335616</td>\n      <td>0.860140</td>\n      <td>0.654676</td>\n      <td>0.428571</td>\n      <td>16.5</td>\n      <td>16.650465</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.633011</td>\n      <td>0.491525</td>\n      <td>0.419411</td>\n      <td>0.880829</td>\n      <td>0.773438</td>\n      <td>0.406122</td>\n      <td>0.362676</td>\n      <td>0.280788</td>\n      <td>0.352941</td>\n      <td>0.446154</td>\n      <td>0.287671</td>\n      <td>0.601399</td>\n      <td>0.633094</td>\n      <td>0.660714</td>\n      <td>13.9</td>\n      <td>13.882806</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.635645</td>\n      <td>0.474576</td>\n      <td>0.294627</td>\n      <td>0.766839</td>\n      <td>0.515625</td>\n      <td>0.400000</td>\n      <td>0.258803</td>\n      <td>0.221675</td>\n      <td>0.415441</td>\n      <td>0.276923</td>\n      <td>0.301370</td>\n      <td>0.671329</td>\n      <td>0.589928</td>\n      <td>0.392857</td>\n      <td>13.8</td>\n      <td>13.783384</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.484636</td>\n      <td>0.322034</td>\n      <td>0.693241</td>\n      <td>0.860104</td>\n      <td>0.679688</td>\n      <td>0.661224</td>\n      <td>0.547535</td>\n      <td>0.573892</td>\n      <td>0.731618</td>\n      <td>0.861538</td>\n      <td>0.417808</td>\n      <td>0.888112</td>\n      <td>0.755396</td>\n      <td>0.517857</td>\n      <td>21.3</td>\n      <td>21.409826</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.310799</td>\n      <td>0.745763</td>\n      <td>0.802426</td>\n      <td>0.880829</td>\n      <td>0.804688</td>\n      <td>0.824490</td>\n      <td>0.697183</td>\n      <td>0.593596</td>\n      <td>0.606618</td>\n      <td>0.723077</td>\n      <td>0.376712</td>\n      <td>0.755245</td>\n      <td>0.697842</td>\n      <td>0.660714</td>\n      <td>30.4</td>\n      <td>30.189026</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.441615</td>\n      <td>0.322034</td>\n      <td>0.792028</td>\n      <td>0.927461</td>\n      <td>0.843750</td>\n      <td>0.779592</td>\n      <td>0.702465</td>\n      <td>0.586207</td>\n      <td>0.753676</td>\n      <td>0.638462</td>\n      <td>0.383562</td>\n      <td>0.867133</td>\n      <td>0.776978</td>\n      <td>0.750000</td>\n      <td>23.6</td>\n      <td>23.540600</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.611062</td>\n      <td>0.525424</td>\n      <td>0.249567</td>\n      <td>0.823834</td>\n      <td>0.507812</td>\n      <td>0.297959</td>\n      <td>0.339789</td>\n      <td>0.233990</td>\n      <td>0.238971</td>\n      <td>0.246154</td>\n      <td>0.198630</td>\n      <td>0.258741</td>\n      <td>0.338129</td>\n      <td>0.232143</td>\n      <td>15.0</td>\n      <td>15.029769</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.772608</td>\n      <td>0.067797</td>\n      <td>0.469671</td>\n      <td>0.932642</td>\n      <td>0.578125</td>\n      <td>0.453061</td>\n      <td>0.250000</td>\n      <td>0.325123</td>\n      <td>0.459559</td>\n      <td>0.515385</td>\n      <td>0.417808</td>\n      <td>0.559441</td>\n      <td>0.604317</td>\n      <td>0.482143</td>\n      <td>7.1</td>\n      <td>6.867806</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.649693</td>\n      <td>0.186441</td>\n      <td>0.455806</td>\n      <td>0.813472</td>\n      <td>0.750000</td>\n      <td>0.400000</td>\n      <td>0.399648</td>\n      <td>0.455665</td>\n      <td>0.617647</td>\n      <td>0.330769</td>\n      <td>0.301370</td>\n      <td>0.608392</td>\n      <td>0.690647</td>\n      <td>0.696429</td>\n      <td>13.0</td>\n      <td>13.057981</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.414399</td>\n      <td>0.305085</td>\n      <td>0.403813</td>\n      <td>0.860104</td>\n      <td>0.492188</td>\n      <td>0.393878</td>\n      <td>0.417254</td>\n      <td>0.408867</td>\n      <td>0.437500</td>\n      <td>0.507692</td>\n      <td>0.171233</td>\n      <td>0.419580</td>\n      <td>0.496403</td>\n      <td>0.142857</td>\n      <td>24.9</td>\n      <td>24.908060</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.719930</td>\n      <td>0.271186</td>\n      <td>0.487002</td>\n      <td>0.906736</td>\n      <td>0.500000</td>\n      <td>0.404082</td>\n      <td>0.390845</td>\n      <td>0.428571</td>\n      <td>0.492647</td>\n      <td>0.492308</td>\n      <td>0.260274</td>\n      <td>0.475524</td>\n      <td>0.654676</td>\n      <td>0.482143</td>\n      <td>9.6</td>\n      <td>9.524882</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.559263</td>\n      <td>0.406780</td>\n      <td>0.336222</td>\n      <td>0.777202</td>\n      <td>0.429688</td>\n      <td>0.442857</td>\n      <td>0.360915</td>\n      <td>0.369458</td>\n      <td>0.496324</td>\n      <td>0.230769</td>\n      <td>0.191781</td>\n      <td>0.755245</td>\n      <td>0.661871</td>\n      <td>0.321429</td>\n      <td>17.5</td>\n      <td>17.643507</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.542581</td>\n      <td>0.711864</td>\n      <td>0.497400</td>\n      <td>0.896373</td>\n      <td>0.640625</td>\n      <td>0.485714</td>\n      <td>0.500000</td>\n      <td>0.359606</td>\n      <td>0.430147</td>\n      <td>0.461538</td>\n      <td>0.267123</td>\n      <td>0.664336</td>\n      <td>0.618705</td>\n      <td>0.571429</td>\n      <td>18.4</td>\n      <td>18.449133</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.536435</td>\n      <td>0.474576</td>\n      <td>0.528596</td>\n      <td>0.854922</td>\n      <td>0.617188</td>\n      <td>0.497959</td>\n      <td>0.496479</td>\n      <td>0.472906</td>\n      <td>0.470588</td>\n      <td>0.607692</td>\n      <td>0.438356</td>\n      <td>0.552448</td>\n      <td>0.647482</td>\n      <td>0.571429</td>\n      <td>18.7</td>\n      <td>18.810421</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.843723</td>\n      <td>0.084746</td>\n      <td>0.282496</td>\n      <td>0.870466</td>\n      <td>0.359375</td>\n      <td>0.210204</td>\n      <td>0.181338</td>\n      <td>0.283251</td>\n      <td>0.286765</td>\n      <td>0.284615</td>\n      <td>0.232877</td>\n      <td>0.356643</td>\n      <td>0.517986</td>\n      <td>0.339286</td>\n      <td>3.7</td>\n      <td>3.296690</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.482880</td>\n      <td>0.305085</td>\n      <td>0.346620</td>\n      <td>0.823834</td>\n      <td>0.242188</td>\n      <td>0.377551</td>\n      <td>0.403169</td>\n      <td>0.384236</td>\n      <td>0.378676</td>\n      <td>0.292308</td>\n      <td>0.253425</td>\n      <td>0.510490</td>\n      <td>0.359712</td>\n      <td>0.267857</td>\n      <td>21.4</td>\n      <td>21.462135</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.589991</td>\n      <td>0.101695</td>\n      <td>0.452340</td>\n      <td>0.792746</td>\n      <td>0.539062</td>\n      <td>0.561224</td>\n      <td>0.355634</td>\n      <td>0.428571</td>\n      <td>0.625000</td>\n      <td>0.438462</td>\n      <td>0.260274</td>\n      <td>0.867133</td>\n      <td>0.683453</td>\n      <td>0.482143</td>\n      <td>16.0</td>\n      <td>16.072733</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.579456</td>\n      <td>0.372881</td>\n      <td>0.625650</td>\n      <td>0.901554</td>\n      <td>0.843750</td>\n      <td>0.536735</td>\n      <td>0.473592</td>\n      <td>0.418719</td>\n      <td>0.591912</td>\n      <td>0.523077</td>\n      <td>0.342466</td>\n      <td>0.874126</td>\n      <td>0.151079</td>\n      <td>0.642857</td>\n      <td>16.6</td>\n      <td>16.583387</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0.680421</td>\n      <td>0.305085</td>\n      <td>0.188908</td>\n      <td>0.782383</td>\n      <td>0.343750</td>\n      <td>0.330612</td>\n      <td>0.250000</td>\n      <td>0.162562</td>\n      <td>0.253676</td>\n      <td>0.246154</td>\n      <td>0.184932</td>\n      <td>0.461538</td>\n      <td>0.525180</td>\n      <td>0.250000</td>\n      <td>11.5</td>\n      <td>11.511883</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.634767</td>\n      <td>0.559322</td>\n      <td>0.251300</td>\n      <td>0.870466</td>\n      <td>0.453125</td>\n      <td>0.328571</td>\n      <td>0.302817</td>\n      <td>0.167488</td>\n      <td>0.261029</td>\n      <td>0.184615</td>\n      <td>0.164384</td>\n      <td>0.559441</td>\n      <td>0.460432</td>\n      <td>0.517857</td>\n      <td>13.8</td>\n      <td>13.745917</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>0.440737</td>\n      <td>0.423729</td>\n      <td>0.544194</td>\n      <td>0.906736</td>\n      <td>0.523438</td>\n      <td>0.495918</td>\n      <td>0.535211</td>\n      <td>0.448276</td>\n      <td>0.514706</td>\n      <td>0.392308</td>\n      <td>0.239726</td>\n      <td>0.608392</td>\n      <td>0.546763</td>\n      <td>0.375000</td>\n      <td>23.6</td>\n      <td>23.576315</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>0.295874</td>\n      <td>0.101695</td>\n      <td>0.604853</td>\n      <td>0.818653</td>\n      <td>0.578125</td>\n      <td>0.536735</td>\n      <td>0.626761</td>\n      <td>0.527094</td>\n      <td>0.786765</td>\n      <td>0.538462</td>\n      <td>0.417808</td>\n      <td>0.727273</td>\n      <td>0.697842</td>\n      <td>0.589286</td>\n      <td>31.2</td>\n      <td>30.817448</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>0.724320</td>\n      <td>0.016949</td>\n      <td>0.285962</td>\n      <td>0.886010</td>\n      <td>0.343750</td>\n      <td>0.261224</td>\n      <td>0.135563</td>\n      <td>0.219212</td>\n      <td>0.327206</td>\n      <td>0.238462</td>\n      <td>0.246575</td>\n      <td>0.398601</td>\n      <td>0.446043</td>\n      <td>0.428571</td>\n      <td>9.4</td>\n      <td>9.230290</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>0.632133</td>\n      <td>0.355932</td>\n      <td>0.317158</td>\n      <td>0.906736</td>\n      <td>0.359375</td>\n      <td>0.353061</td>\n      <td>0.213028</td>\n      <td>0.300493</td>\n      <td>0.411765</td>\n      <td>0.400000</td>\n      <td>0.294521</td>\n      <td>0.342657</td>\n      <td>0.460432</td>\n      <td>0.446429</td>\n      <td>13.9</td>\n      <td>13.877988</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>0.462687</td>\n      <td>0.271186</td>\n      <td>0.476603</td>\n      <td>0.823834</td>\n      <td>0.539062</td>\n      <td>0.477551</td>\n      <td>0.410211</td>\n      <td>0.416256</td>\n      <td>0.643382</td>\n      <td>0.500000</td>\n      <td>0.383562</td>\n      <td>0.699301</td>\n      <td>0.669065</td>\n      <td>0.410714</td>\n      <td>22.5</td>\n      <td>22.463390</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>0.336260</td>\n      <td>0.203390</td>\n      <td>0.535529</td>\n      <td>0.860104</td>\n      <td>0.609375</td>\n      <td>0.461224</td>\n      <td>0.475352</td>\n      <td>0.497537</td>\n      <td>0.647059</td>\n      <td>0.600000</td>\n      <td>0.273973</td>\n      <td>0.797203</td>\n      <td>0.705036</td>\n      <td>0.267857</td>\n      <td>29.0</td>\n      <td>28.833933</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>0.482002</td>\n      <td>0.542373</td>\n      <td>0.228769</td>\n      <td>0.854922</td>\n      <td>0.351562</td>\n      <td>0.218367</td>\n      <td>0.255282</td>\n      <td>0.219212</td>\n      <td>0.286765</td>\n      <td>0.238462</td>\n      <td>0.178082</td>\n      <td>0.335664</td>\n      <td>0.460432</td>\n      <td>0.285714</td>\n      <td>21.5</td>\n      <td>21.439268</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>0.446883</td>\n      <td>0.508475</td>\n      <td>0.336222</td>\n      <td>0.792746</td>\n      <td>0.500000</td>\n      <td>0.477551</td>\n      <td>0.380282</td>\n      <td>0.342365</td>\n      <td>0.363971</td>\n      <td>0.284615</td>\n      <td>0.219178</td>\n      <td>0.475524</td>\n      <td>0.467626</td>\n      <td>0.375000</td>\n      <td>23.3</td>\n      <td>23.289568</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>0.713784</td>\n      <td>0.254237</td>\n      <td>0.185442</td>\n      <td>0.823834</td>\n      <td>0.382812</td>\n      <td>0.357143</td>\n      <td>0.176056</td>\n      <td>0.103448</td>\n      <td>0.113971</td>\n      <td>0.138462</td>\n      <td>0.212329</td>\n      <td>0.433566</td>\n      <td>0.424460</td>\n      <td>0.196429</td>\n      <td>9.9</td>\n      <td>9.858039</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>0.221247</td>\n      <td>0.406780</td>\n      <td>1.696014</td>\n      <td>0.886010</td>\n      <td>1.570313</td>\n      <td>1.161224</td>\n      <td>1.385563</td>\n      <td>1.544335</td>\n      <td>1.474265</td>\n      <td>1.238462</td>\n      <td>0.719178</td>\n      <td>1.412587</td>\n      <td>0.575540</td>\n      <td>1.000000</td>\n      <td>35.2</td>\n      <td>34.956829</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next code block ensures that the y_test and test_pred arrays are 1D and then creates a DataFrame, dw, containing the actual and predicted values, as well as the difference and percentage difference between them."
      ],
      "metadata": {
        "id": "ZCxs2jl9KD50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure 'ytest' and 'test_pred' are 1D\n",
        "ytest_1D = tf.squeeze(y_test)\n",
        "test_pred_1D = tf.squeeze(test_pred)\n",
        "\n",
        "# Now create the DataFrame\n",
        "dw = pd.DataFrame({'Actual': ytest_1D, 'Predicted': test_pred_1D})\n",
        "\n",
        "# Calculate the difference\n",
        "dw['Difference'] = dw['Predicted'] - dw['Actual']\n",
        "\n",
        "# Calculate the percentage difference\n",
        "dw['Percentage Difference'] = (dw['Difference'] / dw['Actual']) * 100\n",
        "\n",
        "dw = dw.round(2)\n",
        "dw.head(30)"
      ],
      "metadata": {
        "id": "pCZhA-w-HU2r",
        "outputId": "cb145760-663a-42dc-8317-26c65c3e7240",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:08:25.622267Z",
          "iopub.execute_input": "2023-07-10T10:08:25.622974Z",
          "iopub.status.idle": "2023-07-10T10:08:25.648058Z",
          "shell.execute_reply.started": "2023-07-10T10:08:25.622933Z",
          "shell.execute_reply": "2023-07-10T10:08:25.647179Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 24,
          "output_type": "execute_result",
          "data": {
            "text/plain": "    Actual  Predicted  Difference  Percentage Difference\n0     19.2  19.290001        0.09                   0.48\n1     19.2  19.299999        0.10                   0.52\n2     28.0  27.860001       -0.14                  -0.52\n3     20.5  20.530001        0.03                   0.13\n4     16.7  16.690001       -0.01                  -0.05\n5     12.1  12.010000       -0.09                  -0.78\n6     23.6  23.080000       -0.52                  -2.19\n7     18.6  18.690001        0.09                   0.51\n8     11.7  11.650000       -0.05                  -0.40\n9     11.9  12.030000        0.13                   1.10\n10    26.1  26.030001       -0.07                  -0.26\n11    24.5  24.400000       -0.10                  -0.40\n12    14.8  14.840000        0.04                   0.24\n13    22.5  22.430000       -0.07                  -0.33\n14     6.3   6.140000       -0.16                  -2.52\n15     5.3   4.950000       -0.35                  -6.55\n16    22.0  21.959999       -0.04                  -0.20\n17    20.9  20.900000       -0.00                  -0.01\n18    20.4  20.370001       -0.03                  -0.14\n19    14.0  13.990000       -0.01                  -0.08\n20    14.9  14.920000        0.02                   0.12\n21    16.5  16.650000        0.15                   0.91\n22    13.9  13.880000       -0.02                  -0.12\n23    13.8  13.780000       -0.02                  -0.12\n24    21.3  21.410000        0.11                   0.52\n25    30.4  30.190001       -0.21                  -0.69\n26    23.6  23.540001       -0.06                  -0.25\n27    15.0  15.030000        0.03                   0.20\n28     7.1   6.870000       -0.23                  -3.27\n29    13.0  13.060000        0.06                   0.45",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Actual</th>\n      <th>Predicted</th>\n      <th>Difference</th>\n      <th>Percentage Difference</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>19.2</td>\n      <td>19.290001</td>\n      <td>0.09</td>\n      <td>0.48</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>19.2</td>\n      <td>19.299999</td>\n      <td>0.10</td>\n      <td>0.52</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>28.0</td>\n      <td>27.860001</td>\n      <td>-0.14</td>\n      <td>-0.52</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20.5</td>\n      <td>20.530001</td>\n      <td>0.03</td>\n      <td>0.13</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>16.7</td>\n      <td>16.690001</td>\n      <td>-0.01</td>\n      <td>-0.05</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>12.1</td>\n      <td>12.010000</td>\n      <td>-0.09</td>\n      <td>-0.78</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>23.6</td>\n      <td>23.080000</td>\n      <td>-0.52</td>\n      <td>-2.19</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>18.6</td>\n      <td>18.690001</td>\n      <td>0.09</td>\n      <td>0.51</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>11.7</td>\n      <td>11.650000</td>\n      <td>-0.05</td>\n      <td>-0.40</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>11.9</td>\n      <td>12.030000</td>\n      <td>0.13</td>\n      <td>1.10</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>26.1</td>\n      <td>26.030001</td>\n      <td>-0.07</td>\n      <td>-0.26</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>24.5</td>\n      <td>24.400000</td>\n      <td>-0.10</td>\n      <td>-0.40</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>14.8</td>\n      <td>14.840000</td>\n      <td>0.04</td>\n      <td>0.24</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>22.5</td>\n      <td>22.430000</td>\n      <td>-0.07</td>\n      <td>-0.33</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>6.3</td>\n      <td>6.140000</td>\n      <td>-0.16</td>\n      <td>-2.52</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>5.3</td>\n      <td>4.950000</td>\n      <td>-0.35</td>\n      <td>-6.55</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>22.0</td>\n      <td>21.959999</td>\n      <td>-0.04</td>\n      <td>-0.20</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>20.9</td>\n      <td>20.900000</td>\n      <td>-0.00</td>\n      <td>-0.01</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>20.4</td>\n      <td>20.370001</td>\n      <td>-0.03</td>\n      <td>-0.14</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>14.0</td>\n      <td>13.990000</td>\n      <td>-0.01</td>\n      <td>-0.08</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>14.9</td>\n      <td>14.920000</td>\n      <td>0.02</td>\n      <td>0.12</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>16.5</td>\n      <td>16.650000</td>\n      <td>0.15</td>\n      <td>0.91</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>13.9</td>\n      <td>13.880000</td>\n      <td>-0.02</td>\n      <td>-0.12</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>13.8</td>\n      <td>13.780000</td>\n      <td>-0.02</td>\n      <td>-0.12</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>21.3</td>\n      <td>21.410000</td>\n      <td>0.11</td>\n      <td>0.52</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>30.4</td>\n      <td>30.190001</td>\n      <td>-0.21</td>\n      <td>-0.69</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>23.6</td>\n      <td>23.540001</td>\n      <td>-0.06</td>\n      <td>-0.25</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>15.0</td>\n      <td>15.030000</td>\n      <td>0.03</td>\n      <td>0.20</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>7.1</td>\n      <td>6.870000</td>\n      <td>-0.23</td>\n      <td>-3.27</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>13.0</td>\n      <td>13.060000</td>\n      <td>0.06</td>\n      <td>0.45</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following that, the code plots the last 200 records of the predicted and actual values to visualize their comparison.\n"
      ],
      "metadata": {
        "id": "icswRk5zKIiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get last 200 records from your data\n",
        "y_pred_sub = test_pred[-200:]\n",
        "y_test_sub = y_test[-200:]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(len(y_pred_sub)), y_pred_sub, label='Predicted')\n",
        "plt.plot(range(len(y_test_sub)), y_test_sub, label='Actual')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Values')\n",
        "plt.legend()\n",
        "plt.title(\"Actual vs Predicted\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bSCs0XidHU2r",
        "outputId": "f2cf195a-d43e-4f92-e6d3-7241251e9165",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:08:25.649319Z",
          "iopub.execute_input": "2023-07-10T10:08:25.649692Z",
          "iopub.status.idle": "2023-07-10T10:08:26.000342Z",
          "shell.execute_reply.started": "2023-07-10T10:08:25.649662Z",
          "shell.execute_reply": "2023-07-10T10:08:25.999384Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 1000x500 with 1 Axes>",
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, two scatter plots are created to check for linearity between the actual and predicted values. The plots help assess how well the model's predictions align with the actual values."
      ],
      "metadata": {
        "id": "eyQ5PHyXKOB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "plt.scatter(y_test, test_pred, color='r')\n",
        "plt.title('Check for Linearity:\\n Actual vs Predicted values')\n",
        "plt.xlabel('Actual values')\n",
        "plt.ylabel('Predicted values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "To09AL5mHU2s",
        "outputId": "5b8a7406-3c64-4a48-d770-5a3941e17e44",
        "execution": {
          "iopub.status.busy": "2023-07-10T10:08:26.001256Z",
          "iopub.execute_input": "2023-07-10T10:08:26.001595Z",
          "iopub.status.idle": "2023-07-10T10:08:26.308853Z",
          "shell.execute_reply.started": "2023-07-10T10:08:26.001560Z",
          "shell.execute_reply": "2023-07-10T10:08:26.307780Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 1400x500 with 1 Axes>",
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    }
  ]
}